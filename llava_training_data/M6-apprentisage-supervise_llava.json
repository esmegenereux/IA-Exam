[
  {
    "id": "fa9a056b-3c42-4edc-8cec-af63e6413d49",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_1.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 6: Apprentissage supervisÃ©"
      }
    ]
  },
  {
    "id": "18791ead-b745-438b-bb32-7541fa8201e4",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_2.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "ac36e6f2-f4d5-4382-9769-879a98188508",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_3.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n3Apprentissage supervisÃ©1. Motivation et intÃ©rÃªt de l'apprentissage automatique 2. Classiï¬cation des principaux types d'apprentissage 3. DÃ©ï¬nition de l'apprentissage supervisÃ© 4. MÃ©thode de la rÃ©gression linÃ©aire simple 5. Apprentissage par descente de gradient 6. MÃ©thode de la rÃ©gression linÃ©aire multiple 7. MÃ©thode de la rÃ©gression logistique 8. Graphe de dÃ©pendance, forward pass, et backward pass\nÏƒ(z)=11+eâˆ’z"
      }
    ]
  },
  {
    "id": "9cd20cc6-89d7-4025-a002-65f8324a2084",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_4.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApprentissage automatique\n4\nPouvez-vous donner quelques exemples d'applications usuelles ?\nNous utilisons rÃ©guliÃ¨rement des outils basÃ©s sur de l'apprentissage automatique dans notre vie quotidienne\nhttps://support.apple.com/en-ca/102381\nApplication: reconnaissance faciale pour vos appareils personnels"
      }
    ]
  },
  {
    "id": "43a49699-bd20-497f-a184-5f82682e712e",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_5.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartReconnaissance de caractÃ¨res (OCR - optical character recognition)\n5Application 3: dispatching automatique de lettres postalesApplication 2: reconnaissance de rÃ©ponses dans un formulaire administratif\nhttps://viso.ai/computer-vision/optical-character-recognition-ocr/\nObjectif simple, mais il est trÃ¨s diï¬ƒcile de construire manuellement un algorithme pour cette tÃ¢cheApplication 1: reconnaissance de caractÃ¨res dans une facture"
      }
    ]
  },
  {
    "id": "d5715072-2563-4636-895c-60b2a640ff43",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_6.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartConduite autonome\n6\nhttps://www.tesla.com/AI"
      }
    ]
  },
  {
    "id": "4243b7fb-7663-4e62-87cf-806437a19e03",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_7.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTraduction automatique du langage\n7\nEn 2020: la traduction n'est pas parfaite, mais donne nÃ©anmoins lâ€™idÃ©e gÃ©nÃ©raleIntÃ©rÃªt: peut servir de base Ã  de meilleures traductions\n2020\n2022\n2023\nEn 2021, 2022 et 2023: la traduction sâ€™est continuellement amÃ©liorÃ©e"
      }
    ]
  },
  {
    "id": "9b058edf-87f1-46b6-93e7-0e85ea377c07",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_8.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartMoteur de recherche\n8\nTÃ¢che 2: donner les pages Web les plus probablesTÃ¢che 1: lâ€™auto-complÃ©tion de ma rechercheSpÃ©ciï¬citÃ©: pour cela, on souhaite tenir compte du proï¬l de la personne (p.ex., sa localisation)"
      }
    ]
  },
  {
    "id": "3773133a-db06-4ed0-952c-467997dbf5a0",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_9.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartChatbots vocaux\n9\nTÃ¢che 1: reconstruction d'une phrase Ã  partir d'une onde acoustiqueTÃ¢che 2: analyse de la requÃªteTÃ¢che 3: rÃ©ponse Ã  donner ou prise de dÃ©cision\nhttps://powervirtualagents.microsoft.com/fr-ca/voicebots-conversational-ai/Applications: support pour des clients, utilisation domestique, etc."
      }
    ]
  },
  {
    "id": "d6983721-df9b-4e03-aeac-c012a35d1dc9",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_10.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartChatbots textuels\n10\nhttps://play.aidungeon.io/Application 2: support clientÃ¨le sur un site webApplication 3: gÃ©nÃ©rateur dâ€™histoires Ã  des ï¬ns rÃ©crÃ©atives\nApplication 1: support dâ€™aide gÃ©nÃ©rale personnalisÃ©e"
      }
    ]
  },
  {
    "id": "6486005b-478a-42bd-adf2-ec8442292b9d",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_11.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApplication en robotique\n11\nhttps://openai.com/blog/learning-dexterity/"
      }
    ]
  },
  {
    "id": "e6db5217-c324-40f7-a41c-8ef0ad17c502",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_12.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApplications dans le domaine mÃ©dical\n12\nhttps://spectrum.ieee.org/consumer-electronics/audiovideo/deep-learning-reinvents-the-hearing-aid\nDeep Q-learning for simultaneous beam orientation and trajectory optimization for Cyberknife [Kafaei et al., 2021]Application 1: prothÃ¨se auditive pour mal-entendantsApplication 2: amÃ©lioration du processus de traitement pour une radiothÃ©rapie"
      }
    ]
  },
  {
    "id": "4098a60a-7bb6-4506-8e7d-ed60348715eb",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_13.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApprentissage pour les jeux\n13\nAlphaStar: Mastering the real-time strategy game StarCraft II [DeepMind, 2019]\nPlaying Atari with Deep Reinforcement Learning [DeepMind, 2013]Mastering the game of Go with deep neural networks and tree search [DeepMind, 2016]\nObjectif: utiliser les jeux comme environnement pour dÃ©velopper et tester des IAs (module 2)"
      }
    ]
  },
  {
    "id": "0e3ef04e-29d0-4880-a157-127d74dbe58f",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_14.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartGÃ©nÃ©rateur dâ€™images - crÃ©ations artistiques\n14Image style transfer using convolutional neural networks [Gatys et al. 2016]\nhttps://deepart.io (la plateforme nâ€™est plus disponible)\nStyle de Turner\nStyle de Van GoghStyle de Munch"
      }
    ]
  },
  {
    "id": "a9c681c4-0cbd-479c-990d-a2a9713d5279",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_15.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartGÃ©nÃ©rateur dâ€™images - crÃ©ations artistiques\n15\nGÃ©nÃ©rateurs dâ€™imagesPopularitÃ©: gÃ©nÃ©ration dâ€™images rigolotes, plausibles et liÃ©es Ã  la requÃªteObjectif: gÃ©nÃ©rer des images Ã  partir dâ€™une description textuelle\nLogiciels: DALL-E, Midjourney, Stable diï¬€usion, etc.\nGÃ©nÃ©ration: via la version payante de DALL-EHierarchical Text-Conditional image Generation with CLIP Latents [OpenAI, 2022 - Dall-E 2]"
      }
    ]
  },
  {
    "id": "56d9d723-fbf4-4cc4-9515-acb28236e233",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_16.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartGÃ©nÃ©ration automatique\n16\nUne seule de ces photos provient d'une vraie personne, laquelle ?\nhttps://thispersondoesnotexist.com/Et je nâ€™en suis pas sÃ»r!Danger: ouvre la porte Ã  de nombreuses dÃ©rives Ã©thiques (dernier module)ABCDEF"
      }
    ]
  },
  {
    "id": "ec27b24e-f2f7-494b-be9d-12c1e1fb1f05",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_17.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAutres applications et ressources\n17\n[N-BEATS, Oreshkin et al. 2020]\nObjectif: rendre disponible diï¬€Ã©rentes applications et leurs donnÃ©es relativesParticularitÃ©: tÃ¢ches prÃ©sentÃ©es sous la forme de compÃ©titionsObservation: de plus en plus d'entreprises passent par Kaggle pour la rÃ©solution de leurs problÃ¨meshttps://www.youtube.com/watch?v=Bui3DWs02h4\nAutre application: dÃ©tection de spamAutre application: prÃ©diction de ventes\nEt encore un grand nombre dâ€™applications, avec Ã©normÃ©ment dâ€™exemples sur le Web"
      }
    ]
  },
  {
    "id": "54cbea82-d3f7-4b17-baf6-7b073416e119",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_18.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDÃ©ï¬nition de l'apprentissage automatique\n18\nDÃ©ï¬nition moderne de lâ€™apprentissage automatique (parmi d'autres)\"A computer program is said to learn from experience E  with respect to some task T and some performance measure P,  if its performance on T, as measured by P, improves with experience E\"http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html\nTom Mitchell (1997)(2) L'apprentissage a besoin d'expÃ©riences passÃ©es (E - donnÃ©es)(3) Besoin d'avoir un moyen d'Ã©valuer les performances d'un modÃ¨le (P)(1) L'apprentissage est dÃ©diÃ© pour une tÃ¢che prÃ©cise (T)Trois Ã©lÃ©ments fondamentaux apparaissent:\nConfusion avec l'intelligence artiï¬cielle\"Most of what is labeled AI today, particularly in the public sphere,  is actually machine learning (ML), a term in use for the past several decades\"https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/8Michael Jordan (2019)\nLâ€™apprentissage automatique nâ€™est quâ€™un sous-domaine de l'intelligence artiï¬cielleConfusion malheureusement trÃ¨s frÃ©quente (et vÃ©hiculÃ©e par les mÃ©dias)"
      }
    ]
  },
  {
    "id": "c47f6f4b-e736-4f50-ba41-2281571f0068",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_19.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartContenu du cours\n19ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "a3eddf8d-4f8d-44d1-8f31-2304fde46b1d",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_20.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartObjectif de l'apprentissage automatique\n20\nVentes prÃ©cÃ©dentes\"This movie was far better  than the trailer made it look.\"Review\nDashcam\nFreinerPrÃ©diction\nScorePrÃ©diction\nVentes futuresPrÃ©diction\nLâ€™objectif est de construire une fonction capable d'eï¬€ectuer une prÃ©diction, pour une situation spÃ©ciï¬que\nNotez la grande variÃ©tÃ© des situations pouvant Ãªtre considÃ©rÃ©es (utile dans Ã©normÃ©ment de contextes)"
      }
    ]
  },
  {
    "id": "fc71aa09-7b8b-4790-9992-c13b6da10885",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_21.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartIntÃ©rÃªt de l'apprentissage automatique\n21\nPourquoi ne pas rÃ©soudre ces problÃ¨mes avec les techniques dÃ©jÃ  vues ?Raison 1: certaines tÃ¢ches sont trÃ¨s diï¬ƒciles Ã  rÃ©soudre avec les techniques prÃ©cÃ©dentes\nExemple: eï¬€ectuer une reconnaissance d'imagesApproche traditionnelle: dÃ©celer manuellement des caractÃ©ristiquesApprentissage: exploiter les similaritÃ©s avec d'autres photos dÃ©jÃ  vuesApproche naÃ¯ve: mÃ©moriser tous les chiï¬€res vus dans une base de donnÃ©es\nRÃ©alitÃ©: plusieurs mÃ©thodes d'apprentissage utilisent des algorithmes connusExemple: un clustering peut se faire avec de la recherche localeCette deuxiÃ¨me option est plus facile (et eï¬ƒcace) Ã  mettre en oeuvre\nConclusion: la frontiÃ¨re entre les diï¬€Ã©rents modules n'est pas si stricte...Raison 2: on souhaite avoir une approche applicable Ã  des situations qui ne sont pas encore rencontrÃ©es\nExemple: eï¬€ectuer une reconnaissance de chiï¬€res Ã©crits Ã  la mainDiï¬ƒcultÃ©: incapable de gÃ©nÃ©raliser Ã  des nouveaux chiï¬€res, similaires, mais non-vusRaison 3: en quelques sortes, on va utiliser des algorithmes dÃ©jÃ  vus"
      }
    ]
  },
  {
    "id": "697f43db-76af-4603-a1a7-24552405c866",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_22.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartPrincipaux types d'apprentissage\n22Apprentissage supervisÃ©CaractÃ©ristique: apprentissage qui utilise des donnÃ©es labellisÃ©es (la vraie valeur est connue - ground truth)Analogie de lâ€™apprentissage: un professeur qui enseigne des notions Diï¬ƒcultÃ©: on doit avoir ces donnÃ©es Ã  disposition en suï¬ƒsance\nAnalogie de lâ€™exÃ©cution: lâ€™Ã©lÃ¨ve reproduit dans de nouveaux contextesApprentissage non-supervisÃ©Exemple: dÃ©tecter une anomalie (malware parmi des programmes sains)CaractÃ©ristique: apprentissage qui nâ€™utilise aucune donnÃ©e labellisÃ©e\nObjectif: apprendre des associations de similaritÃ© entre les donnÃ©esExemple: regrouper les donnÃ©es semblables (clustering)Diï¬ƒcultÃ©: on a pas de label de rÃ©fÃ©rence pour aider lâ€™apprentissageApprentissage par renforcement\nCaractÃ©ristique: apprendre par expÃ©riences et essais-erreurs via un signal de rÃ©compenseSignal de rÃ©compense: positif en cas de bonne action, nÃ©gatif sinonDiï¬ƒcultÃ©: important de savoir comment rÃ©compenser les actions faitesObjectif: obtenir le plus de rÃ©compenses lors de ces actionsFonctionnement: lâ€™agent rÃ©alise toute une sÃ©rie dâ€™actions"
      }
    ]
  },
  {
    "id": "ec1d3e0d-cec6-4c70-bfaf-8197dd287856",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_23.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartLimitations de l'apprentissage automatique\n23\nAvez-vous des exemples ?Limitations: certaines tÃ¢ches ne sont pas propices Ã  l'apprentissage automatiqueCritÃ¨res Ã  considÃ©rer: eï¬ƒcacitÃ©, Ã©thique, donnÃ©es disponibles, etc.(1) TÃ¢ches trÃ¨s bien rÃ©solues avec des algorithmes traditionnelsExemple: trier une listeExemple: trouver le plus court chemin entre deux noeuds\nApproches liÃ©es: neural reasoning, neural combinatorial optimizationExemple: rÃ©soudre eï¬ƒcacement un problÃ¨me combinatoire(2) TÃ¢ches nÃ©cessitant des garantiesNon-conseillÃ© pour des systÃ¨mes critiques oÃ¹ une erreur est coÃ»teuse Ã  rÃ©parer\nDiï¬ƒcultÃ©: lâ€™apprentissage automatique ne rÃ©alise que des prÃ©dictionsConsÃ©quence: il y a un risque que les prÃ©dictions soient erronÃ©esApproches liÃ©es: machine learning with guarantees, safe machine learning"
      }
    ]
  },
  {
    "id": "3284c807-738f-480f-97f0-e74396ed08b9",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_24.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartLimitations de l'apprentissage automatique\n24\n(3) TÃ¢ches trop coÃ»teuse pour utiliser un modÃ¨leExemple: systÃ¨mes temps-rÃ©els\n(4) TÃ¢ches avec peu ou aucune donnÃ©e disponible(5) Situations Ã©thiques dÃ©licatesDiï¬ƒcultÃ©: entraÃ®ner un modÃ¨le peut demander des ressources consÃ©quencesRessources: temps dâ€™exÃ©cution, temps dâ€™entraÃ®nement, mÃ©moire spatiale, etc.\nExemple: prÃ©diction d'une maladie rareRÃ©alitÃ©: entraÃ®ner un modÃ¨le demande des donnÃ©es (idÃ©alement labellisÃ©es)Diï¬ƒcultÃ©: ces donnÃ©es ne sont pas toujours disponiblesExemple: prÃ©diction si un Ã©tudiant doit Ãªtre admis Ã  une universitÃ© ou nonExemple: prÃ©diction si une personne a droit Ã  un prÃªt hypothÃ©caireDiï¬ƒcultÃ©: on doit faire attention Ã  ce genre de situation et empÃªcher la discriminationApproches liÃ©es: few-shot learning, zero-shot learning, dÃ©tection dâ€™anomaliesApproches liÃ©es: Fairness-aware AI, Trustworthy machine learningApproches liÃ©es: deep learning on FPGA\nPour un grand nombre de situations, lâ€™apprentissage automatique nâ€™est pas lâ€™approche Ã  privilÃ©gier"
      }
    ]
  },
  {
    "id": "4d5901d4-087f-4010-8fe2-8599e0a4cdb8",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_25.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n25Apprentissage supervisÃ©1. Motivation et intÃ©rÃªt de l'apprentissage automatique 2. Classiï¬cation des principaux types d'apprentissage 3. DÃ©ï¬nition de l'apprentissage supervisÃ© 4. MÃ©thode de la rÃ©gression linÃ©aire simple 5. Apprentissage par descente de gradient 6. MÃ©thode de la rÃ©gression linÃ©aire multiple 7. MÃ©thode de la rÃ©gression logistique 8. Graphe de dÃ©pendance, forward pass, et backward pass\nÏƒ(z)=11+eâˆ’z"
      }
    ]
  },
  {
    "id": "58837930-f875-49ad-94f2-738713007d9a",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_26.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartMise en situation: le problÃ¨me de l'Ã©tudiant\n26ProblÃ¨me de l'Ã©tudiantMise en situation: suite Ã  ces notes de l'intra, un Ã©tudiant se demande combien il obtiendra Ã  son examen(1) Un Ã©tudiant est caractÃ©risÃ© par sa note Ã  l'intra\nPremiÃ¨re Ã©tape: construire une formalisation du problÃ¨meSon idÃ©e: il se base sur les rÃ©sultats des Ã©tudiants de l'annÃ©e prÃ©cÃ©dentexâˆˆ[0..10](2) Le rÃ©sultat est la note Ã  l'examen ï¬nalyâˆˆ[0..10](3) On souhaite construire une fonction de prÃ©dictionf:[0..10]â†’[0..10]ProblÃ¨me visant Ã  construire une fonction de prÃ©diction retournant une valeur continueProblÃ¨me de rÃ©gression\nf:Xâ†’â„\nNote: on peut Ã©galement restreindre la prÃ©diction Ã  un intervalle de valeurs continues"
      }
    ]
  },
  {
    "id": "39d0dc95-e890-4c37-b69d-3a13b6d45be8",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_27.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFormalisation du problÃ¨me\n27(2) Les donnÃ©es sont utilisÃ©es pour construire une fonction de prÃ©dictionPrincipes de l'apprentissage supervisÃ©\nNotations et vocabulaire gÃ©nÃ©ral(3) Utilisation de la fonction pour prÃ©dire la valeur de nouvelles situations(1) PrÃ©sence de donnÃ©es historiques, dont on connait la vraie valeurEnsemble dâ€™entraÃ®nement (training set): les donnÃ©es utilisÃ©es pour construire la fonctionD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}m:tailleÂ deÂ l'ensembleDonnÃ©e (data): une association entre en ensemble de caractÃ©ristiques (features) et un label (x(i),y(i)),avecÂ xâˆˆ[0..10]etyâˆˆ[0..10]x(i):caractÃ©ristiqueÂ deÂ laÂ donnÃ©eÂ iÂ (onÂ veraÂ parÂ laÂ suiteÂ qu'onÂ peutÂ avoirÂ plusieursÂ caractÃ©ristiques)y(i):labelÂ deÂ laÂ donnÃ©eÂ i(laÂ vraieÂ valeur,Â ouÂ encoreÂ l'Ã©tiquette)Fonction de prÃ©diction: fonction donnant une valeur Ã  partir des caractÃ©ristiques en entrÃ©e f:[0..10]â†’[0..10]Valeur prÃ©dite: valeur de sortie de la fonction pour de nouvelles donnÃ©es Ì‚y=f(x)Â (leÂ chapeauÂ indiqueÂ qu'onÂ aÂ uneÂ valeurÂ estimÃ©e)m=20"
      }
    ]
  },
  {
    "id": "cd3443b8-07b2-43c0-81fc-d8b34a12bbd5",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_28.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction de prÃ©diction\n28Principe: prÃ©conception que l'on fait sur notre fonctionHypothÃ¨se: concept fondamental en apprentissage automatique\nNote: hypothÃ¨se trÃ¨s simple mais nÃ©anmoins trÃ¨s utile\nNotre objectif est de construire une fonction de prÃ©diction. Quelle forme de fonction utiliser ? (linÃ©aire, polynomiale, exponentielle, etc.)Exemple: fonction linÃ©aireFonction de prÃ©diction ayant la forme suivante:Approximation par une fonction linÃ©aire\nÌ‚y=f(x)=w.x+bx:caractÃ©ristiqueÂ (feature)w,b:paramÃ¨tresÂ deÂ laÂ fonctionÂ (parameters)w:poidsÂ deÂ laÂ caractÃ©ristiqueÂ xÂ (weight)b:biaisÂ (biasÂ orÂ intercept)ParamÃ¨tres: dÃ©ï¬nissent le type de la droiteObjectif: trouver des valeurs adÃ©quates pour ces paramÃ¨tresVotre tÃ¢che: vous devez dÃ©ï¬nir lâ€™hypothÃ¨se qui vous semble pertinente"
      }
    ]
  },
  {
    "id": "49a70c00-6f94-4187-a34e-28e85db1c6c0",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_29.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartRÃ©gression linÃ©aire simple\n29f(x)=0.76x+0.96\nf(x)=âˆ’0.5x+9Intuition: on souhaite obtenir la droite qui donne une bonne approximation de nos donnÃ©es\nObservation: la fonction de droite semble Ãªtre de meilleure qualitÃ©\nComment dÃ©terminer ces paramÃ¨tres (w et b) ?\nÌ‚y=f(x)=w.x+bApproximation linÃ©aireRappel: cela revient Ã  dÃ©terminer la droite (pente et intercept)\nTÃ¢che consistant Ã  dÃ©terminer la meilleure approximation linÃ©aire entre une variable (x) et son label (y)RÃ©gression linÃ©aire"
      }
    ]
  },
  {
    "id": "80119801-3fb4-4265-9074-bd8a89c08e73",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_30.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction dâ€™Ã©cart\n30Intuition: on veut construire la fonction (linÃ©aire) qui approxime le mieux nos donnÃ©esMÃ©trique possible: considÃ©rer lâ€™Ã©cart entre la fonction et les points connus\nxy(x(1),y(1))(x(2),y(2))(x(3),y(3))(x(4),y(4))Ì‚y=w.x+bÌ‚y(2)âˆ’y(2)f(x(2))âˆ’y(2)(w.x(2)+b)âˆ’y(2)\nComment formaliser la qualitÃ© d'une fonction par rapport Ã  nos donnÃ©es ?Diï¬ƒcultÃ©: la notion dâ€™Ã©cart nâ€™est pas bien dÃ©ï¬nieEn pratique, il existe plusieurs faÃ§ons de calculer cet Ã©cart\nExemple:Â l'Ã©cartÂ quadratiqueâ†’L(Ì‚y(i),y(i))=(Ì‚y(i)âˆ’y(i))2Fonction dÃ©ï¬nissant lâ€™Ã©cart quâ€™un modÃ¨le de  prÃ©diction a avec un certain point de lâ€™espaceFonction dâ€™Ã©cart (loss function)\nL(Ì‚y(i),y(i))â†’Ã©cartIntÃ©rÃªt: Ã©vite que les Ã©carts positifs et nÃ©gatifs se compensentIntÃ©rÃªt: donne une forte pÃ©nalitÃ© pour les grands Ã©cartsIntÃ©rÃªt: cet Ã©cart est une fonction diï¬€Ã©rentiable\nUn objectif possible est de minimiser la somme de tous ces Ã©carts"
      }
    ]
  },
  {
    "id": "0203caca-de10-4e66-a170-81a398faf806",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_31.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFormalisation de l'apprentissage supervisÃ©\n31Fonction combinant les Ã©carts avec tous les points de lâ€™ensemble considÃ©rÃ©Fonction de coÃ»t (cost function)\nJ(w,b):mâ¨i=1L(Ì‚y(i),y(i))â†’Ã©cartÂ totalIntuition: il sâ€™agit dâ€™une mesure d'Ã  quel point une prÃ©diction est loin de lâ€™ensemble des donnÃ©esmâ¨i=1:opÃ©rateurÂ combinantÂ lesÂ diï¬€Ã©rentsÂ Ã©cartsÂ obtenuesÂ viaÂ laÂ fonctionÂ d'Ã©cartw,b:paramÃ¨tresÂ deÂ laÂ fonctionÂ (parameters)Note: par cette formulation, on remarque que lâ€™Ã©valuation du coÃ»t dÃ©pend des valeurs des paramÃ¨tres\nNote: la fonction de coÃ»t (et dâ€™Ã©cart) dÃ©pendent du type de prÃ©diction Ã  rÃ©aliserTÃ¢che de rÃ©gression: erreur quadratique moyenneTÃ¢che de classiï¬cation: entropie croisÃ©eTÃ¢che consistant Ã  dÃ©terminer les paramÃ¨tres (w et b) qui minimisent une fonction de coÃ»tApprentissage supervisÃ©\nminw,bJ(w,b)"
      }
    ]
  },
  {
    "id": "4e054460-339a-4e11-8f91-a150198f88a3",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_32.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartRetour au problÃ¨me de lâ€™Ã©tudiant\n32\nFonction de coÃ»t communÃ©ment utilisÃ©e pour des tÃ¢ches de rÃ©gressionErreur quadratique moyenne (mean squared error)\n=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2Â avecÂ Ì‚y(i)=w.x(i)+bJ(w,b)=12mmâˆ‘i=1L(Ì‚y(i),y(i))Note: le facteur 1/2 ne change rien aux Ã©carts, mais va simpliï¬er les futurs calculsNotre tÃ¢che: minimiser cette fonction et ainsi obtenir notre Ã©quation de droite(Ì‚y(2)âˆ’y(2))2"
      }
    ]
  },
  {
    "id": "75ec47ba-ae56-4442-952d-903f2a818ec1",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_33.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDescente de gradient\n33Descente de gradient: mÃ©thode (parmi d'autres) pour minimiser une fonctionFonctionnement: amÃ©liore les paramÃ¨tres Ã©tape par Ã©tape en suivant la direction donnÃ©e par le gradientHypothÃ¨se: nÃ©cessite d'avoir une fonction de coÃ»t principalement diï¬€Ã©rentiable \n(2) La direction du mouvement Ã  prendre(3) Lâ€™intensitÃ© du mouvement Ã  prendre\nComment minimiser cette fonction ?\nBasÃ© sur trois ingrÃ©dients essentiels:\nAnalogie: cette mÃ©thode peut Ãªtre vue comme un hill climbing (module 3) dans un espace continu(1) Le choix d'une position initiale\nJ(w,b)=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2Â avecÂ Ì‚y(i)=w.x(i)+bErreur quadratique moyenne\nDiï¬€Ã©rence: on va exploiter le fait que la fonction soit diï¬€Ã©rentiable pour dÃ©terminer la direction Ã  prendreOn itÃ¨re jusquâ€™Ã  ce quâ€™un critÃ¨re dâ€™arrÃªt soit atteint"
      }
    ]
  },
  {
    "id": "1ff3a36b-928e-4dbc-8f31-ca67f705270d",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_34.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nJ(w)=w2Exemple dâ€™une descente de gradient (fonction Ã  1 paramÃ¨tre)\n34Cas 1: w est positifConsÃ©quence: la mise Ã  jour va diminuer w\nw>0â†’dJ(w)dw>0Cas 2: w est nÃ©gatifEtape dâ€™une descente de gradient: mise Ã  jour des paramÃ¨tres\nFonctionÂ Ã Â minimiser:Â J(w)=w2,wâˆˆâ„(2) Direction: obtenue avec la dÃ©rivÃ©e de J(w)(3) IntensitÃ©: obtenue par une valeur positive(1) Initialisation: valeur alÃ©atoire pour wIl sâ€™agit de la direction de la pente la plus forte\ndJ(w)dw=2w\nÎ±âˆˆ]0,1]Il sâ€™agit du taux dâ€™apprentissage (learning rate)\nwâ†wâˆ’Î±dJ(w)dwDans notre exemple, il y a trois situations possiblesConsÃ©quence: la mise Ã  jour va augmenter wCas 3: w est nulConsÃ©quence: la descente de gradient a convergÃ©\nw<0â†’dJ(w)dw<0\nw=0â†’dJ(w)dw=>0\nAppliquons cette idÃ©e sur des fonctions ayant plus de paramÃ¨tres"
      }
    ]
  },
  {
    "id": "8a3cdcb3-ffeb-421b-81ea-3e864eb7c5b3",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_35.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDescente de gradient dans le cas gÃ©nÃ©ral\n35Une seule variable: la direction de la plus forte pente est donnÃ©e par la dÃ©rivÃ©ePlusieurs variables: la direction de la pente la plus forte est donnÃ©e par le gradientGradient d'une fonction: vecteur constituÃ© des dÃ©rivÃ©es partielles de chaque variable\nJ(w,b)=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2Â avecÂ Ì‚y(i)=w.x(i)+bExemple: erreur quadratique moyenne \nâˆ‡J(w,b)=(âˆ‚Jâˆ‚w,âˆ‚Jâˆ‚b)Etape dâ€™une descente de gradient: mise Ã  jour de tous les paramÃ¨tres suivant sa dÃ©rivÃ©e partielle\nw=wâˆ’Î±âˆ‚J(w,b)âˆ‚w\nb=bâˆ’Î±âˆ‚J(w,b)âˆ‚bUtilisation du gradient: la fonction de coÃ»t doit Ãªtre diï¬€Ã©rentiable (en grande partie)\nGradient:Â âˆ‡f(x1,â€¦,xn)=(âˆ‚fâˆ‚x1,â€¦,âˆ‚fâˆ‚xn)"
      }
    ]
  },
  {
    "id": "9763a898-9a46-4262-bd4d-d21060b7dd99",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_36.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartConvexitÃ© et gradient de la fonction de coÃ»t\n36\nA t-on une convergence vers un minimum global ?Bonne nouvelle: oui, dans ce cas prÃ©cisMauvaise nouvelle: non, dans le cas gÃ©nÃ©ralLa diï¬€Ã©rence vient de la convexitÃ© de la fonctionUne fonction est convexe si, pour deux positions quelconques de la fonction, la droite liant les deux points est toujours au dessus de la fonctionConvexitÃ©\nCondition: pour cela, il faut que le taux dâ€™apprentissage ne soit pas trop grandPropriÃ©tÃ©: une fonction convexe garantit que la descente de gradient converge vers le minimum global\nJ(w,b)=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2Erreur quadratique moyenneÂ avecÂ Ì‚y(i)=w.x(i)+b\nRÃ©gression linÃ©aire avec une erreur quadratique moyenne: la fonction de coÃ»t est convexe (parabole)"
      }
    ]
  },
  {
    "id": "1c2443ba-5d3e-4c7c-8afe-5be5a1932b7f",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_37.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\n(3)Â ApplicationÂ desÂ rÃ¨glesÂ d'analyse:Â âˆ‚J(w,b)âˆ‚w=12m(âˆ‚((w.x(1)+bâˆ’y(1))2)âˆ‚w+â€¦+âˆ‚((w.x(m)+bâˆ’y(m))2)âˆ‚w)Gradient de la fonction de coÃ»t\n37\n(1)Â Gradient:Â âˆ‡J(w,b)=(âˆ‚Jâˆ‚w,âˆ‚Jâˆ‚b)\n(2)Â DÃ©rivÃ©eÂ partielleÂ selonÂ w:Â âˆ‚J(w,b)âˆ‚w=âˆ‚(12mâˆ‘mi=1(w.x(i)+bâˆ’y(i))2)âˆ‚wRÃ¨gles d'analyse\nd(ax+bx)dx=d(ax)dx+d(bx)dxd(ax)dx=ad(x)dx\nFonctionÂ Ã Â dÃ©river:Â J(w,b)=12mmâˆ‘i=1(w.x(i)+bâˆ’y(i))2\n(4)Â DÃ©rivÃ©eÂ partielleÂ pourÂ uneÂ donnÃ©eÂ spÃ©ciï¬que:Â âˆ‚((w.x(i)+bâˆ’y(i))2)âˆ‚w=2x(i)(w.x(i)+bâˆ’y(i))=2x(i)(Ì‚y(i)âˆ’y(i))ProcÃ©dure: calculs basÃ©s sur des rÃ¨gles d'analyse\nContribution de la donnÃ©e i  pour le paramÃ¨tre w\n(5)Â TermeÂ ï¬nal:Â âˆ‚J(w,b)âˆ‚w=12m(2x(1)(Ì‚y(1)âˆ’y(1))+â€¦+2x(m)(Ì‚y(m)âˆ’y(m)))=1mmâˆ‘i=1x(i)(Ì‚y(i)âˆ’y(i))Moyenne de la  contribution  de chaque donnÃ©e\nQuel est le gradient de notre fonction de coÃ»t ?\nOn sort le terme constant, et on applique la dÃ©rivÃ©e partielle Ã  chaque Ã©lÃ©ment de la somme"
      }
    ]
  },
  {
    "id": "3db1744e-1f1d-4fdf-8983-159dfda49097",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_38.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDescente de gradient: algorithme\n38ModÃ¨leÂ uneÂ foisÂ entraÃ®nÃ©:Â Ì‚y=0.76x+0.96\nOn calcule la prÃ©diction du modÃ¨le actuel On Ã©value le gradientItÃ©ration de descente de gradient selon w\nğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:w,b=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾Ì‚y(i)âˆ€iâˆˆ{1,â€¦,m}ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾âˆ‚Jâˆ‚w,âˆ‚Jâˆ‚bğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾J(w,b)w=wâˆ’Î±âˆ‚Jâˆ‚wb=bâˆ’Î±âˆ‚Jâˆ‚bğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡w,bğ—€ğ—‹ğ–ºğ–½ğ—‚ğ–¾ğ—‡ğ—ğ–£ğ–¾ğ—Œğ–¼ğ–¾ğ—‡ğ—(D,Î±):EntrÃ©e: lâ€™ensemble dâ€™entraÃ®nement et le taux dâ€™apprentissageOn donne une valeur initiale alÃ©atoire aux paramÃ¨tresOn itÃ¨re tant quâ€™on est pas arrivÃ© Ã  convergenceOn Ã©value la fonction de coÃ»t\nJ(w,b)=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2\nÌ‚y=f(x)=w.x+b\nâˆ‚J(w,b)âˆ‚w=1mmâˆ‘i=1x(i)(Ì‚y(i)âˆ’y(i))ItÃ©ration de descente de gradient selon b\nâˆ‚J(w,b)âˆ‚b=1mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))DÃ©rivation laissÃ©e en exerciceNote: il ne sâ€™agit pas de la meilleure approche pour une rÃ©gression linÃ©aireAvantage: processus gÃ©nÃ©ral quâ€™on va adapter Ã  des cas plus complexesExemples: rÃ©gression multiple, logistique, et rÃ©seaux de neurones"
      }
    ]
  },
  {
    "id": "be2f6ce0-5def-4fd1-b371-efff9c94b880",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_39.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n39Apprentissage supervisÃ©1. Motivation et intÃ©rÃªt de l'apprentissage automatique 2. Classiï¬cation des principaux types d'apprentissage 3. DÃ©ï¬nition de l'apprentissage supervisÃ© 4. MÃ©thode de la rÃ©gression linÃ©aire simple 5. Apprentissage par descente de gradient 6. MÃ©thode de la rÃ©gression linÃ©aire multiple 7. MÃ©thode de la rÃ©gression logistique 8. Graphe de dÃ©pendance, forward pass, et backward pass\nÏƒ(z)=11+eâˆ’z"
      }
    ]
  },
  {
    "id": "4cd5cc48-c195-4085-aa1c-f7ad51f8ce40",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_40.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartRÃ©gression linÃ©aire multiple\n40ProblÃ¨me de l'Ã©tudiantMise en situation: lâ€™Ã©tudiant souhaite maintenant utiliser plusieurs caractÃ©ristiques (x) pour sa prÃ©diction\nDonnÃ©eÂ d'entrainement:(x(i),y(i))x(i):vecteurÂ desÂ caractÃ©risquesÂ deÂ laÂ donnÃ©eÂ ix(i)j:caractÃ©ristiqueÂ jdeÂ laÂ donnÃ©eÂ iIntuition: l'idÃ©e est d'avoir une reprÃ©sentation plus ï¬dÃ¨le de la rÃ©alitÃ©\nPrincipe: identique Ã  la rÃ©gression linÃ©aireDiï¬ƒcultÃ©: plus diï¬ƒcile Ã  visualiser (espace Ã  n dimension)Diï¬€Ã©rence: une donnÃ©e est maintenant reprÃ©sentÃ©e par un vecteur de caractÃ©ristiques\nFonction de prÃ©diction ayant la forme suivante:Fonction de prÃ©diction linÃ©aire gÃ©nÃ©ralisÃ©e\nÌ‚y=w1x1+w2x2+â€¦+wnxn+b=wTx+bx1,â€¦,xn:caractÃ©ristiquesÂ d'uneÂ donnÃ©eÂ duÂ problÃ¨meb,w1,â€¦,wn:paramÃ¨tresÂ devantÂ ÃªtreÂ apprisy(i):labelÂ deÂ laÂ donnÃ©eÂ i(inchangÃ©)\nÌ‚y=(w1â€¦wn)Ã—(x1â€¦xn)+b=wTx+b"
      }
    ]
  },
  {
    "id": "b819db2e-bc32-4063-88b3-ee239aaf989f",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_41.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:w,b=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾Ì‚y(i)âˆ€iâˆˆ{1,â€¦,m}ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾âˆ‚Jâˆ‚w1,â€¦,âˆ‚Jâˆ‚wn,âˆ‚Jâˆ‚bğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾J(w1,â€¦,wn,b)w1=w1âˆ’Î±âˆ‚Jâˆ‚w1b=bâˆ’Î±âˆ‚Jâˆ‚bğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡w1,â€¦,wn,bwn=wnâˆ’Î±âˆ‚Jâˆ‚wnâ€¦ğ—€ğ—‹ğ–ºğ–½ğ—‚ğ–¾ğ—‡ğ—ğ–£ğ–¾ğ—Œğ–¼ğ–¾ğ—‡ğ—(D,Î±):RÃ©gression linÃ©aire multiple\n41\nCombien de paramÃ¨tres a t-on avec n caractÃ©ristiques ?Cas gÃ©nÃ©ral: pour n caractÃ©ristiques, on a n+1 paramÃ¨tresx et w sont maintenant des vecteurs\nxâˆˆâ„n\nwâˆˆâ„n\nbâˆˆâ„\nJ(w1,â€¦,wn,b)=12mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))2\nÌ‚y=f(x)=wTx+b\nâˆ‚J(w1,â€¦,wn,b)âˆ‚wj=1mmâˆ‘i=1x(i)j(Ì‚y(i)âˆ’y(i))âˆ€jâˆˆ{1,â€¦,n}\nâˆ‚J(w,b)âˆ‚b=1mmâˆ‘i=1(Ì‚y(i)âˆ’y(i))le gradient propre Ã  un paramÃ¨tre considÃ¨re uniquement la caractÃ©ristique qui lui y est liÃ©e\nLe fonctionnement suit exactement le mÃªme principe que prÃ©cÃ©demment"
      }
    ]
  },
  {
    "id": "48670043-29e6-415e-9b3d-3211527396c3",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_42.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n42Apprentissage supervisÃ©1. Motivation et intÃ©rÃªt de l'apprentissage automatique 2. Classiï¬cation des principaux types d'apprentissage 3. DÃ©ï¬nition de l'apprentissage supervisÃ© 4. MÃ©thode de la rÃ©gression linÃ©aire simple 5. Apprentissage par descente de gradient 6. MÃ©thode de la rÃ©gression linÃ©aire multiple 7. MÃ©thode de la rÃ©gression logistique 8. Graphe de dÃ©pendance, forward pass, et backward pass\nÏƒ(z)=11+eâˆ’z"
      }
    ]
  },
  {
    "id": "bfdf3561-cdba-479d-8fc9-b50ec9c662e6",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_43.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nhttp://wiki.fast.aiMise en situation: prÃ©diction d'une rÃ©ussite\n43PrÃ©diction d'une rÃ©ussite\n(1) Lâ€™Ã©tudiant est reprÃ©sentÃ© par deux caractÃ©ristiquesx1âˆˆâ„:Â leÂ nombreÂ d'heuresÂ passÃ©esÂ Ã Â Ã©tudier\n(4) La prÃ©diction peut ensuite se faire suivant un seuilf(x)=Ì‚yâ‰¥0.5â†’RÃ©ussitef(x)=Ì‚y<0.5â†’EchecMise en situation: lâ€™Ã©tudiant souhaite maintenant savoir s'il va rÃ©ussir ou non son examenCaractÃ©ristiques: Il se base sur deux critÃ¨res, le nombre d'heures d'Ã©tude, et de repos\nSon idÃ©e: se baser sur les informations des Ã©tudiants prÃ©cÃ©dentsx2âˆˆâ„:Â leÂ nombreÂ d'heuresÂ passÃ©esÂ Ã Â dormirÂ laÂ veilleÂ deÂ l'examen(2) Le rÃ©sultat est une valeur binaireyâˆˆ{0,1}:Â l'Ã©tudiantÂ aÂ t-ilÂ rÃ©ussiÂ ouÂ nonÂ ?(3) On souhaite construire une fonction donnant une probabilitÃ© de succÃ¨sf:â„2â†’[0,1]:Â fonctionÂ quiÂ retourneÂ uneÂ probabilitÃ©Â deÂ succÃ¨s"
      }
    ]
  },
  {
    "id": "48021961-6294-4822-a128-d5110f0d875c",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_44.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartIntroduction Ã  la rÃ©gression logistique\n44\nQue pensez-vous de l'utilisation d'une rÃ©gression linÃ©aire ?Diï¬ƒcultÃ©: des valeurs trÃ¨s grandes peuvent Ãªtre prÃ©dites (nÃ©gatives ou positives)Notre situation: on souhaite obtenir une probabilitÃ© de succÃ¨s (valeur entre 0 et 1)ConsÃ©quence: la rÃ©gression linÃ©aire nâ€™est pas le modÃ¨le le plus adaptÃ©RÃ©gression logistique: Ã©tant donnÃ© une donnÃ©e x, on veut construire une fonction donnant une probabilitÃ©Ì‚yâ‰ˆ1:probableÂ deÂ rÃ©ussirÌ‚yâ‰ˆ0:probableÂ deÂ rater\nÌ‚yâ‰ˆ0.5:dÃ©cisionÂ incertaineExigence: on doit obtenir une valeur entre 0 et 1IdÃ©e de conception: construire une fonction compressant n'importe quelle valeur rÃ©elle entre 0 et 1Fonction compressant n'importe quelle valeur rÃ©elle entre 0 et 1Fonction sigmoÃ¯de\nÏƒ(z):â„â†’[0,1]=11+eâˆ’z\nÌ‚y=â„™(y=1|x)Â avecÂ Ì‚yÂ laÂ probabilitÃ©Â deÂ rÃ©ussiteÂ Ã Â l'examen"
      }
    ]
  },
  {
    "id": "7e785162-e13c-4e0c-83cb-cae3c8741b00",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_45.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction sigmoide et rÃ©gression logistique\n45Fonction sigmoide: compresse une valeur rÃ©elle entre 0 et 1(1)Â CasÂ oÃ¹Â zÂ estÂ positivementÂ grand:Â Ïƒ(z)=11+0=1InterprÃ©tation: on est conï¬ant sur la rÃ©ussite(2)Â CasÂ oÃ¹Â zÂ estÂ nÃ©gativementÂ grand:Â Ïƒ(z)=11+âˆ=0InterprÃ©tation: on est conï¬ant sur lâ€™Ã©chec(3)Â CasÂ oÃ¹Â zÂ estÂ nul:Â Ïƒ(z)=11+1=0.5InterprÃ©tation: on est incertain sur le choixAvantage supplÃ©mentaire: la fonction est diï¬€Ã©rentiable\nf(x)=Ì‚yâ‰¥0.5â†’RÃ©ussitef(x)=Ì‚y<0.5â†’Echec\nIdÃ©e: rÃ©aliser une combinaison linÃ©aire (comme avant) mais suivie ensuite dâ€™une fonction sigmoÃ¯deFonction de prÃ©diction ayant la forme suivante:RÃ©gression logistique\nÌ‚y=Ïƒ(wTx+b)Â avecÂ Ì‚yâˆˆ[0,1]x:vecteurÂ desÂ caractÃ©ristiquesÂ d'uneÂ donnÃ©eÂ duÂ problÃ¨meb,w:paramÃ¨tresÂ devantÂ ÃªtreÂ apprisÂ (wÂ estÂ unÂ vecteur)Note: on peut ensuite dÃ©ï¬nir un seuil de dÃ©cision pour la classiï¬cation ï¬nale"
      }
    ]
  },
  {
    "id": "580f2a83-89df-43ef-bbac-1afdec6f2490",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_46.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction dâ€™Ã©cart communÃ©ment utilisÃ©e pour des tÃ¢ches de de classiï¬cation binaireErreur de lâ€™entropie croisÃ©e binaire (binary cross entropy loss)\nL(Ì‚y,y)=âˆ’(ylog(Ì‚y)+(1âˆ’y)log(1âˆ’Ì‚y))Â avecÂ Ì‚y=Ïƒ(wTx+b)Fonction d'Ã©cart pour la rÃ©gression logistique\n46\nComment caractÃ©riser la qualitÃ© d'une rÃ©gression logistique ?Objectif: mesurer l'Ã©cart entre valeur prÃ©dite et la vraie valeur via une fonction dâ€™Ã©cartEcart nul: indique une prÃ©diction parfaite sur nos donnÃ©es\nCasÂ 1:Â situationÂ ouÂ y=0Â (valeurÂ d'Ã©checÂ -Â l'Ã©tudiantÂ aÂ ratÃ©Â l'examen)Intuition: cette fonction pÃ©nalise lâ€™Ã©cart entre la prÃ©diction et une vraie valeur de 1 ou de 0ConsÃ©quence:Â pousseÂ Ì‚yÂ Ã Â ÃªtreÂ petitÂ (Ì‚yâ†’0)ImpactÂ surÂ laÂ fonction:Â L(Ì‚y,0)=âˆ’log(1âˆ’Ì‚y)\nCasÂ 2:Â situationÂ ouÂ y=1Â (valeurÂ deÂ succÃ¨sÂ -Â l'Ã©tudiantÂ aÂ rÃ©ussiÂ l'examen)ConsÃ©quence:Â pousseÂ Ì‚yÂ Ã Â ÃªtreÂ grandÂ (Ì‚yâ†’1)ImpactÂ surÂ laÂ fonction:Â L(Ì‚y,1)=âˆ’log(Ì‚y)"
      }
    ]
  },
  {
    "id": "e0ebb8df-6260-41d7-bccb-f503281b8830",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_47.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction de coÃ»t pour la rÃ©gression logistique\n47\nJ(w,b)=1mmâˆ‘i=1L(Ì‚y(i),y(i))=âˆ’1mmâˆ‘i=1(y(i)log(Ì‚y(i))+(1âˆ’y(i))log(1âˆ’Ì‚y(i)))AvecÂ Ì‚y(i)=Ïƒ(wTx(i)+b)Â etÂ Ïƒ(z)=11+eâˆ’zFonction de coÃ»t dâ€™une rÃ©gression logistique\nEtape 2: prendre l'Ã©cart moyen de toutes ces prÃ©dictions\nPrincipe: exactement pareil que prÃ©cÃ©demment, mais avec notre nouvelle fonction de perte\n{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}â†’Ì‚y(i)â‰ˆy(i)âˆ€iâˆˆ{1,â€¦,m}â†’Ïƒ(wTx(i)+b)â‰ˆy(i)Etape 1: eï¬€ectuer la rÃ©gression logistique sur toutes les donnÃ©es dâ€™entraÃ®nement\nPreuve: https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convexPropriÃ©tÃ©: cette fonction est diï¬€Ã©rentiable et peut donc Ãªtre minimisÃ©e par une descente de gradient PropriÃ©tÃ©: cette fonction est Ã©galement convexe, on a ainsi une convergence vers un minimum global"
      }
    ]
  },
  {
    "id": "2e460921-57ef-45a1-ad66-aae62fdac6c5",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_48.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartCalcul du gradient\n48J(w,b)=1mmâˆ‘i=1L(Ì‚y(i),y(i))=âˆ’1mmâˆ‘i=1(y(i)log(Ì‚y(i))+(1âˆ’y(i))log(1âˆ’Ì‚y(i)))AvecÂ Ì‚y(i)=Ïƒ(wTx(i)+b)Â etÂ Ïƒ(z)=11+eâˆ’z\nFonction de coÃ»t\nBonne nouvelle: malgrÃ© l'apparence complexe, cette tÃ¢che n'est pas si diï¬ƒcile !Astuce: procÃ©der par une dÃ©composition en trois Ã©tapesEtape 1: exprimer la fonction en un graphe de dÃ©pendances (computation graph)Etape 2: Ã©valuer la fonction avec la valeur actuelle des paramÃ¨tres (forward pass)Etape 3: utiliser la rÃ¨gle du chaÃ®nage (chain rule) pour calculer les dÃ©rivÃ©es partielles (backward pass)\nJ(w1,w2,w3)=5(3w1+w2w23)Exemple: on veut faire une descente de gradient sur cette fonctionGÃ©nÃ©ralitÃ© de cette approche: il sâ€™agit Ã©galement du fonctionnement dâ€™un rÃ©seau de neurone"
      }
    ]
  },
  {
    "id": "48000bf8-811f-4e8c-836a-19a7f0416920",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_49.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartGraphe de dÃ©pendances et passe en avant (forward pass)\n49a=3w1w1w2w3J=5db=w23c=w2bd=a+cReprÃ©sentation dâ€™une expression numÃ©rique en une sÃ©rie dâ€™opÃ©rations Ã©lÃ©mentairesGraphe de dÃ©pendances (computation graph)\nIntuition: il sâ€™agit de mettre en Ã©vidence toutes les opÃ©rations Ã©lÃ©mentaires ainsi que leurs dÃ©pendancesExemple: on veut faire une descente de gradient sur cette fonction\nCalcul du coÃ»t de la fonction de prÃ©diction, selon la valeur actuelle des paramÃ¨tres (w et b)Passe en avant (forward pass)\nExemple: supposons quâ€™on ait les valeurs suivantes pour les 3 paramÃ¨tres9321551211\nJ(w1,w2,w3)=5(3w1+w2w23)\nw1=3,w2=2,w3=1"
      }
    ]
  },
  {
    "id": "a7f73ca8-6728-468c-b4a1-4800df9c2f79",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_50.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartPasse en arriÃ¨re (backward pass)\n50RÃ¨gle du chaÃ®nage\ndzdx=dzdyÃ—dydxsi z dÃ©pend de y,  et si y dÃ©pend de x.a=3w1w1w2w3J=5db=w23c=w2bd=a+cCalcul de la valeur des dÃ©rivÃ©es partielles de chaque paramÃ¨tre pour obtenir le gradientPasse en arriÃ¨re (backward pass or backpropagation)\nNote: pour ce calcul, on commence Ã  la derniÃ¨re valeur calculÃ©e, et on remonte jusquâ€™aux paramÃ¨tresPrincipe: on utilise la rÃ¨gle du chaÃ®nage pour Ã©valuer les dÃ©rivÃ©es partiellesâˆ‚Jâˆ‚w1=âˆ‚Jâˆ‚dÃ—âˆ‚dâˆ‚aÃ—âˆ‚aâˆ‚w1=5Ã—1Ã—3=15âˆ‚Jâˆ‚w2=âˆ‚Jâˆ‚dÃ—âˆ‚dâˆ‚cÃ—âˆ‚câˆ‚w2=5Ã—1Ã—b=5b=5w23=5âˆ‚Jâˆ‚w3=âˆ‚Jâˆ‚dÃ—âˆ‚dâˆ‚cÃ—âˆ‚câˆ‚bÃ—âˆ‚bâˆ‚w3=5Ã—1Ã—w2Ã—2w3=10w2w3=20\nJ(w1,w2,w3)=5(3w1+w2w23)\nâˆ‡J(w1,w2,w3)=(âˆ‚Jâˆ‚w1,âˆ‚Jâˆ‚w2,âˆ‚Jâˆ‚w3)"
      }
    ]
  },
  {
    "id": "7095c0bf-3751-4ae0-bf10-16a48c9515db",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_51.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDerniÃ¨re Ã©tape: descente de gradient\n51a=3w1w1w2w3J=5db=w23c=w2bd=a+c\nSituation actuelle: on a lâ€™Ã©valuation de notre gradient pour les valeurs actuels de nos paramÃ¨tresProchaine Ã©tape: utiliser ces valeurs pour eï¬€ectuer la descente de gradientw3â†w3âˆ’Î±Ã—20=1âˆ’0.1Ã—20=âˆ’1w1â†w1âˆ’Î±Ã—15=3âˆ’0.1Ã—15=1.5w2â†w2âˆ’Î±Ã—5=2âˆ’0.1Ã—5=1.5TauxÂ d'apprentissage:Â Î±=0.1NouvellesÂ valeurs:Â w1=1.5,w2=1.5,w3=âˆ’1ValeursÂ actuelles:Â w1=3,w2=2,w3=1Par la suite: refaire la passe en avant et en arriÃ¨re avec ces nouvelles valeurs\nJ(w1,w2,w3)=5(3w1+w2w23)\nw1=3,w2=2,w3=1\nâˆ‡J(w1,w2,w3)=(âˆ‚Jâˆ‚w1,âˆ‚Jâˆ‚w2,âˆ‚Jâˆ‚w3)=(15,5,20)"
      }
    ]
  },
  {
    "id": "7e7afa7c-f195-477f-8bcf-eb9c432170dd",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_52.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartGraphe de dÃ©pendance en pratiqueâ€¦\n52\nCe principe peut Ãªtre utilisÃ© pour diï¬€Ã©rentier des fonctions trÃ¨s complexes\nyoutube.com/watch?v=VMj-3S1tku0"
      }
    ]
  },
  {
    "id": "a5bbb56c-59f3-42a7-bac4-b132d8700c6e",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_53.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nGraphe de dÃ©pendances sur notre fonction d'Ã©cart\n53\nAvecÂ Ì‚y=Ïƒ(wTx+b)Â etÂ Ïƒ(z)=11+eâˆ’zL(Ì‚y,y)=âˆ’(ylog(Ì‚y)+(1âˆ’y)log(1âˆ’Ì‚y))Rappel\ndlog(x)dx=1x\nObjectif: calculer le gradientPrincipe: construire le graphe de dÃ©pendancez=w1x1+â€¦+wnxn+bx1bwnw1xn...L(Ì‚y,y)=âˆ’(ylog(Ì‚y)+(1âˆ’y)log(1âˆ’Ì‚y))Ì‚y=Ïƒ(z)âˆ‚L(Ì‚y,y)âˆ‚Ì‚y=âˆ’yÌ‚y+1âˆ’y1âˆ’Ì‚y=âˆ’y(1âˆ’Ì‚y)+Ì‚y(1âˆ’y)Ì‚y(1âˆ’Ì‚y)=âˆ’y+Ì‚yy+Ì‚yâˆ’Ì‚yyÌ‚y(1âˆ’Ì‚y)=Ì‚yâˆ’yÌ‚y(1âˆ’Ì‚y)DÃ©rivation complÃ¨te: https://math.stackexchange.com/questions/78575/ derivative-of-sigmoid-function-sigma-x-frac11e-x\nâˆ‚zâˆ‚wi=xiâˆ‚zâˆ‚b=1\nâˆ‚Ì‚yâˆ‚z=Ïƒ(z)(1âˆ’Ïƒ(z))=Ì‚y(1âˆ’Ì‚y)\nProchaine Ã©tape: passe en arriÃ¨re avec la rÃ¨gle du chainageâˆ‚Lâˆ‚wi=âˆ‚Lâˆ‚Ì‚yÃ—âˆ‚Ì‚yâˆ‚zÃ—âˆ‚zâˆ‚wi=Ì‚yâˆ’yÌ‚y(1âˆ’Ì‚y)Ã—Ì‚y(1âˆ’Ì‚y)Ã—xi=xi(Ì‚yâˆ’y)âˆ‚Lâˆ‚b=âˆ‚Lâˆ‚Ì‚yÃ—âˆ‚Ì‚yâˆ‚zÃ—âˆ‚zâˆ‚b=Ì‚yâˆ’yÌ‚y(1âˆ’Ì‚y)Ã—Ì‚y(1âˆ’Ì‚y)Ã—1=Ì‚yâˆ’y\nâˆ‡L(Ì‚y,y)=(âˆ‚Lâˆ‚w1,â€¦,âˆ‚Lâˆ‚wn,âˆ‚Lâˆ‚b)\nOn a ainsi nos dÃ©rivÃ©es partielle, et donc notre gradient !"
      }
    ]
  },
  {
    "id": "2860667f-0c31-45f1-a74c-adee3cac6421",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_54.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartIntÃ©gration de la fonction de coÃ»t\n54\nJ(w,b)=1mmâˆ‘i=1L(Ì‚y(i),y(i))=1m(L(Ì‚y(1),y(1))+â€¦+L(Ì‚y(m),y(m)))DerniÃ¨re Ã©tape: intÃ©grer la fonction de coÃ»tPrincipe: identique Ã  une rÃ©gression linÃ©aire, mais avec une autre fonction d'Ã©cartEtat actuel: le gradient de chaque terme de la fonction de perte est connu\nâˆ‚L(Ì‚y(i),y(i))âˆ‚b=(Ì‚y(i)âˆ’y(i))Â pourÂ toutesÂ lesÂ donnÃ©esÂ i\nâˆ‚J(w,b)âˆ‚wj=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚wj+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚wj)âˆ€jâˆˆ{1,â€¦,n}Gradient de la fonction de coÃ»t: simple application des rÃ¨gles dâ€™analyse\nâˆ‚J(w,b)âˆ‚b=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b)\nâˆ‚L(Ì‚y(i),y(i))âˆ‚wj=x(i)j(Ì‚y(i)âˆ’y(i))pourÂ toutesÂ lesÂ donnÃ©esÂ iÂ etÂ paramÃ¨tresÂ wjRÃ¨gles d'analyse\nd(ax+bx)dx=d(ax)dx+d(bx)dxd(ax)dx=ad(x)dx"
      }
    ]
  },
  {
    "id": "ab891e9e-3466-4123-a4d2-a7ef82cd9179",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_55.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartToutes les briques assemblÃ©es - problÃ¨me de lâ€™Ã©tudiant\n55âˆ‚J(w,b)âˆ‚b=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b)âˆ‚J(w,b)âˆ‚w1=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚w1+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚w1)âˆ‚J(w,b)âˆ‚w2=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚w2+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚w2)Passe en arriÃ¨re (backward pass) Passe en avant (forward pass)J(w,b)=1mmâˆ‘i=1L(Ì‚y(i),y(i))Â Â avecÂ Â Ì‚y(i)=Ïƒ(wTx(i)+b)\navecÂ âˆ‚L(Ì‚y(i),y(i))âˆ‚wi=x(i)i(Ì‚y(i)âˆ’y(i))Â etÂ âˆ‚L(Ì‚y(i),y(i))âˆ‚b=(Ì‚y(i)âˆ’y(i))\nCode disponible sur Moodle"
      }
    ]
  },
  {
    "id": "0f7b9426-411d-4e01-bc48-bfe5f947fc69",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_56.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n56Apprentissage supervisÃ©1. Motivation et intÃ©rÃªt de l'apprentissage automatique 2. Classiï¬cation des principaux types d'apprentissage 3. DÃ©ï¬nition de l'apprentissage supervisÃ© 4. MÃ©thode de la rÃ©gression linÃ©aire simple 5. Apprentissage par descente de gradient 6. MÃ©thode de la rÃ©gression linÃ©aire multiple 7. MÃ©thode de la rÃ©gression logistique 8. Graphe de dÃ©pendance, forward pass, et backward pass\nÏƒ(z)=11+eâˆ’z"
      }
    ]
  },
  {
    "id": "64a01da3-55f2-4c6c-9edb-ef669234eb3e",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_57.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartSynthÃ¨se des notions vues\n57\nApprentissage automatique\"A computer program is said to learn from experience E  with respect to some task T and some performance measure P,  if its performance on T, as measured by P, improves with experience E\"http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html\n(2) L'apprentissage a besoin d'expÃ©riences passÃ©es (E - donnÃ©es)(3) Besoin d'avoir un moyen d'Ã©valuer les performances d'un modÃ¨le (P)(1) L'apprentissage est dÃ©diÃ© pour une tÃ¢che prÃ©cise (T)Apprentissage supervisÃ©: minimiser d'un coÃ»t entre une prÃ©diction et les donnÃ©es connues(1) RÃ©gression linÃ©aire simple(2) RÃ©gression linÃ©aire multiple(3) RÃ©gression logistiquePlusieurs modÃ¨les possibles:\nAlgorithme dâ€™apprentissage: par descente de gradient via un graphe de dÃ©pendance(4) Et dâ€™autres !"
      }
    ]
  },
  {
    "id": "71ba1412-2192-450f-a6a3-9c599b551e88",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_58.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartLimitation de nos mÃ©thodes de rÃ©gression\n58\nQuelles sont les limitations de nos mÃ©thodes de rÃ©gression actuelles ?\nLimitation: notre prÃ©dictions sont une combinaison linÃ©aire des variables (Ã©ventuelle sigmoidisÃ©e)En pratique: ce nâ€™est pas souvent le cas !Figure de droite: donnÃ©es non linÃ©airement sÃ©parablesConclusion: il nous faut une fonction de prÃ©diction plus expressive !Figure de gauche: sÃ©paration linÃ©aire acceptableHypothÃ¨se sous-jacente: les donnÃ©es sont linÃ©airement sÃ©parables (ou presque)"
      }
    ]
  },
  {
    "id": "d9aff8e2-3542-4a9e-921b-0c7234383ce7",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_59.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartExemples de questions d'examen\n591. Eï¬€ectuer une Ã©tape de descente de gradient pour une fonction simple tout en dÃ©taillant les diï¬€Ã©rentes Ã©tapes (graphe de dÃ©pendance, passe en avant, et passe en arriÃ¨re)ThÃ©oriePratique1. Donner et expliquer une fonction d'Ã©cart ou de coÃ»t pour une rÃ©gression donnÃ©e 2. DÃ©crire la fonction sigmoide, ses intÃ©rÃªts, et donner un cas d'utilisation 3. Expliquer les diï¬€Ã©rences entre features et paramÃ¨tres 4. Expliquer le principe de la descente de gradient"
      }
    ]
  },
  {
    "id": "8f9e7de2-705a-4821-91c1-dd74217caaec",
    "image": "extracted_content\\M6-apprentisage-supervise\\images\\page_60.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesApprentissage supervisÃ©: FIN\nDALLE: A student calculating his chance to succeed at the exam, oil pastel drawing"
      }
    ]
  }
]