[
  {
    "id": "7f6f26ee-f342-42ed-8224-29e4891d4040",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_1.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 8: Apprentissage non-supervisÃ©"
      }
    ]
  },
  {
    "id": "3fa48459-2485-4753-b034-2657234fabe6",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_2.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "6c2d5d1f-0b8d-4bd3-aea7-c97eeb47f05d",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_3.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n3Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e"
      }
    ]
  },
  {
    "id": "4c6dce04-e979-4ac9-bcd0-243a43ad7395",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_4.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApprentissage non-supervisÃ©\n4Apprentissage supervisÃ©Principe: apprentissage qui se fait Ã  l'aide de donnÃ©es dont on connaÃ®t la vraie valeur\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}Ce label est indispensable pour entraÃ®ner un modÃ¨le (minimiser la fonction de coÃ»t)\nÂ VraieÂ valeurÂ (groundÂ truth):Â PourÂ chaqueÂ donnÃ©eÂ x(i),Â onÂ connaÃ®tÂ leÂ labelÂ y(i)\nQue se passe t-il si on a pas accÃ¨s Ã  ces labels ? \nType dâ€™apprentissage qui se fait sÃ»r base de donnÃ©es non-labellisÃ©esApprentissage non-supervisÃ©\nNote: on a quand mÃªme besoin de donnÃ©es, mais sans la prÃ©sence dâ€™un labelIntuition: la tÃ¢che est plus compliquÃ©e car on a moins dâ€™information Ã  disposition\nD:{x(1),x(2),â€¦,x(m)}\nOn nâ€™a plus aucun moyen dâ€™entraÃ®ner le modÃ¨leâ€¦"
      }
    ]
  },
  {
    "id": "f71b2282-824a-4464-9a88-0f5da824a955",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_5.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDiï¬€Ã©rences avec l'apprentissage supervisÃ©\n5Apprentissage supervisÃ©: apprentissage dâ€™une fonction de sÃ©paration\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}\nD:{x(1),x(2),â€¦,x(m)}Point commun: cela revient Ã  la minimisation dâ€™une fonction coÃ»t (ou la maximisation dâ€™une prÃ©cision)Apprentissage non-supervisÃ©: apprentissage dâ€™une fonction de regroupement ou dâ€™exclusion"
      }
    ]
  },
  {
    "id": "b4674a0c-88ca-4e6d-8083-0715119e222c",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_6.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApplication: segmentation du marchÃ©\n6Segmentation du marchÃ©OpÃ©ration eï¬€ectuÃ©e en marketing stratÃ©gique pour diviser la demande en plusieurs groupes cohÃ©rents\nObjectif: permet de dÃ©ï¬nir des oï¬€res ciblÃ©es Ã  chaque groupeEtape 1: Ã©tablir les caractÃ©ristiques des diï¬€Ã©rents clients (critÃ¨res gÃ©ographiques, civils, individuels, etc.)Etape 2: regrouper les clients similaires en un groupe cohÃ©rent (cluster)Etape 3: Ã©tablir des oï¬€res ou des stratÃ©gies ciblÃ©es pour chacun des groupes\nApplications: problÃ¨me prÃ©sent dans Ã©normÃ©ment de contextes impliquant une clientÃ¨le Ã  toucher"
      }
    ]
  },
  {
    "id": "dfce6a53-3dd7-47fc-a0dd-e167d45cbbc5",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_7.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nApplication: cybersÃ©curitÃ©\n7DÃ©tection de comportements frauduleux\nObjectif: permet de contrÃ´ler automatiquement les comportements suspects dans un rÃ©seauOpÃ©ration eï¬€ectuÃ©e en cybersÃ©curitÃ© pour surveiller le comportement d'utilisateurs dans un rÃ©seauEtape 1: Ã©tablir les caractÃ©ristiques des diï¬€Ã©rents utilisateurs (ressources utilisÃ©es, position actuelle, etc.)Etape 2: Ã©tablir un proï¬l d'utilisation normale en fonction des utilisateurs prÃ©sentsEtape 3: dÃ©tecter les comportements inhabituels\nAn interactive visual analytics approach for network anomaly detection through smart labeling [Fan et al., 2019]"
      }
    ]
  },
  {
    "id": "1ae8a57b-45b9-4360-89a9-6ebd15faff7a",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_8.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApplication: systÃ¨mes de recommandation\n8SystÃ¨mes de recommandationOpÃ©ration consistant Ã  recommander un ensemble d'entitÃ©s Ã  un utilisateurEtape 1: Ã©tablir les caractÃ©ristiques des utilisateurs (prÃ©fÃ©rences, achats prÃ©cÃ©dents, etc.)Etape 2: dÃ©celer les entitÃ©s qu'un utilisateur serait enclins Ã  vouloir \nApplications: Ã©normÃ©ment dâ€™utilisations dans le commerce en ligne"
      }
    ]
  },
  {
    "id": "ac5e1539-48fb-4a58-8306-80526306e429",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_9.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartApplication: visualisation de donnÃ©es\n9Visualisation de donnÃ©esObjectif: oï¬€rir une reprÃ©sentation visuelles des donnÃ©es Ã  des ï¬ns d'analyse par un humain\nMise en Ã©vidence de migrations importantes inattenduesCluster des pays ex-URSS avec l'EthiopieVisualizing Data using t-SNE [van der Maaten and Hinton, 2008]RÃ©duction de donnÃ©es de 100 dimensions Ã  2Les mots semblables sont projetÃ©s Ã  proximitÃ©The World Migration Network: rankings, groups and gravity models [Cappart and Thonet, 2015]\nVersion lisible: https://lvdmaaten.github.io/tsne/ (Words in 2D)"
      }
    ]
  },
  {
    "id": "ac2c5664-c98a-4bb4-9ca8-b20f95300a91",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_10.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n10Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e"
      }
    ]
  },
  {
    "id": "4287db43-1689-41db-a01e-6c247f30e706",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_11.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartProblÃ¨me du regroupement (clustering)\n11Construction du cluster: besoin de dÃ©ï¬nir un algorithme dÃ©diÃ©ProblÃ¨me consistant Ã  regrouper les donnÃ©es similaires en groupes (clusters) cohÃ©rentsProblÃ¨me du regroupement (clustering)\nNote: il sâ€™agit dâ€™une des grandes familles d'application de l'apprentissage non-supervisÃ©\nEvaluation dâ€™un clustering: besoin de dÃ©ï¬nir une fonction de coÃ»t"
      }
    ]
  },
  {
    "id": "520b2502-477b-4292-8b52-b49250a35a70",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_12.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme K-means\n12Initialisation: choisir k valeurs alÃ©atoirement, correspondant au centre de k clustersAlgorithme K-meansEtape 1: regrouper les donnÃ©es au centre (centroÃ¯des) du cluster le plus procheEtape 2: mettre Ã  jour les centres des clusters, en fonction des donnÃ©es qui y ont Ã©tÃ© ajoutÃ©es/retirÃ©esEtape 3: rÃ©pÃ©ter les Ã©tapes (1) Ã  (3) jusquâ€™Ã  convergencePrincipe: dÃ©ï¬nir le centre de k groupes, et associer les donnÃ©es au groupe ayant le centre le plus proche\nEtape dâ€™initialisation\nEtape 1: assignation au cluster le plus proche\nEtape 2: mise Ã  jour des centresDÃ©ï¬nition des centres: de maniÃ¨re itÃ©rative, en 3 Ã©tapes successives Ã  partir dâ€™une valeur initiale\nStratÃ©gie de regroupement: les nouvelles donnÃ©es sont assignÃ©es au cluster ayant le centre le plus proche Convergence: lâ€™itÃ©ration nâ€™a pas changÃ© les centres des clusters"
      }
    ]
  },
  {
    "id": "5cbf010c-31d5-41ff-b649-e27542a5a9ea",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_13.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme K-means - visualisation\n13Situation initiale\nAssignation au cluster le plus proche\nMise Ã  jour des centres\nAssignation au cluster le plus proche\nMise Ã  jour des centres\nAssignation au cluster le plus proche\nLe processus est rÃ©pÃ©tÃ© jusqu'Ã  ce que les centres ne bougent plus (convergence)"
      }
    ]
  },
  {
    "id": "acf49e81-2f09-429b-a15a-45e8aa65bbde",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_14.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme K-means - Formalisation\n14Notationsk:leÂ nombreÂ deÂ clusters{x(1),x(2),â€¦,x(m)}:lesÂ donnÃ©esÂ d'entraÃ®nementx(i)âˆˆâ„n:dimensionÂ d'uneÂ donnÃ©eÂ (nombreÂ deÂ caractÃ©ristiques)Î¼j:leÂ centreÂ duÂ clusterÂ j:âˆ€jâˆˆ{1,â€¦,k}c(i):indiceÂ duÂ clusterÂ oÃ¹Â laÂ donnÃ©eÂ iÂ aÂ Ã©tÃ©Â assignÃ©eÎ¼jâˆˆâ„n:dimensionÂ duÂ centreÂ duÂ clusterÎ¼c(i):centreÂ duÂ clusterÂ oÃ¹Â laÂ donnÃ©eÂ iÂ aÂ Ã©tÃ©Â assignÃ©eObjectif: minimiser la distance moyenne entre les donnÃ©es et le centre du cluster associÃ©Fonction de coÃ»t dâ€™un regroupement\nJ(c(1),â€¦,c(m),Î¼1,â€¦,Î¼j)=1mmâˆ‘i=1x(i)âˆ’Î¼c(i)2Â avecÂ x(i)âˆ’Î¼c(i)2=(x(i)1âˆ’Î¼1,c(i))2+â€¦+(x(i)nâˆ’Î¼n,c(i))2Â (pourÂ nÂ caractÃ©ristiques)Intuition: cela correspond Ã  la moyenne des distances euclidiennes entre chaque donnÃ©e et le centre associÃ©\nVisualisation pour des donnÃ©es ayant deux caractÃ©ristiques numÃ©riques (rÃ©elles)x(42)Î¼rougeÎ¼vertk=2Â (clusterÂ rougeÂ etÂ vert)x(i)âˆ’Î¼c(i)2=x(42)âˆ’Î¼vert2c(42)=vert"
      }
    ]
  },
  {
    "id": "5b0d0230-25a3-4bec-9722-0b64e52c935d",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_15.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme K-means\n15\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}On donne en entrÃ©e le jeu de donnÃ©es et le nombre de clusters souhaitÃ©sOn place les centres initiaux alÃ©atoirementEtape 1: on assigne les donnÃ©es au cluster le plus procheOn ne considÃ¨re que les donnÃ©es associÃ©es au cluster actuel dans lâ€™itÃ©rationOn retourne les assignations de chaque donnÃ©e, et les centres des clustersSituation initiale\nAssignation au cluster le plus proche\nMise Ã  jour des centroÃ¯desEtape 2: on calcule le nouveau centre du cluster, Ã©tant donnÃ© les points liÃ©sPrincipe: chaque itÃ©ration va rapprocher les centres dâ€™un groupe de donnÃ©es et ainsi rÃ©duire le coÃ»t actuel"
      }
    ]
  },
  {
    "id": "3d31df7c-3e99-4934-b3bb-fb2e86e2ba87",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_16.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAnalyse de l'algorithme\n16\nEst-ce que cet algorithme va converger ? Bonne nouvelle: chaque Ã©tape rapproche les centres des donnÃ©esConvergence: assurÃ©e car le coÃ»t est bornÃ© (jamais nÃ©gatif)ConsÃ©quence: chaque Ã©tape rÃ©duit ainsi le coÃ»t J\nEst-ce que la solution obtenue est optimale  ?  Mauvaise nouvelle: on risque de tomber dans un minimum localAugmentation du risque: lorsquâ€™on augmente le nombre de clusters ou le nombre de caractÃ©ristiques\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}"
      }
    ]
  },
  {
    "id": "4dd06a99-d89e-4286-8389-8ce07603fd64",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_17.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAmÃ©lioration de l'algorithme\n17\nAvez-vous une idÃ©e pour pallier cette diï¬ƒcultÃ© ? Module 3: dÃ©terminer les meilleurs clusters est un COPCadre gÃ©nÃ©ral: minimiser cette fonction de coÃ»t est NP-diï¬ƒcileModules 3 et 4: mÃ©thodes pour rÃ©soudre ce genre de problÃ¨mesProgrammation par contraintes: diï¬ƒcile Ã  mettre en oeuvreDiï¬ƒcultÃ©: le problÃ¨me implique Ã©normÃ©ment de variables\nAlgorithme K-means: algorithme de recherche localeEtapes 1 et 2: mouvements locaux pour minimiser le coÃ»tInitialisation: points de dÃ©part de la rechercheStratÃ©gie actuelle: de type Hill climbing sans diversiï¬cationExemples: redÃ©marrages, initialisation intelligente, ou utilisation de mÃ©taheuristiquesDiï¬ƒcultÃ©: forte chance de rester coincÃ© dans un minimum localNP-hardness of Euclidean sum-of-squares clustering [Aloise et al., 2009]Bonne nouvelle: on a vu plusieurs mÃ©thodes pour mitiger ce problÃ¨me"
      }
    ]
  },
  {
    "id": "9c7dabc1-b3ae-4078-8766-23417b8716ec",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_18.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartK-means avec des redÃ©marrages\n18MÃ©thode de redÃ©marrages\nğ–¿ğ—ˆğ—‹iâˆˆ1ğ—ğ—ˆÎ“:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œğ–¶ğ—‚ğ—ğ—ğ–±ğ–¾ğ—Œğ—ğ–ºğ—‹ğ—(D,k,Î“):s=ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k)ğ—‚ğ–¿J(s)<J(sâ‹†):sâ‹†=ssâ‹†=âŠ¥Observation: la solution oÃ¹ lâ€™algorithme converge dÃ©pend des positions initiales des clustersAmÃ©lioration simple: redÃ©marrer plusieurs fois lâ€™algorithme et prendre la meilleure solution trouvÃ©e\nNote: montre les connexions entre les diï¬€Ã©rentes modes de lâ€™intelligence artiï¬cielleAutres amÃ©liorations: utilisation de mÃ©taheuristiques (algorithmes gÃ©nÃ©tiques)\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}\nHG-means: A scalable hybrid genetic algorithm for minimum sum-of-squares clustering [Gribel and Vidal, 2019]Avantage: trÃ¨s facile Ã  mettre en oeuvre"
      }
    ]
  },
  {
    "id": "a21f063c-78ea-4bf9-bb99-663dd442e75a",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_19.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme K-means: initialisation des centres des clusters\n19\nComment initialiser alÃ©atoirement les centres ?\nk-means++: The advantages of careful seeding [Arthur and Vassilvitskii, 2006]Option 1: Initialisation totalement alÃ©atoirePrincipe: les valeurs sont initialisÃ©es alÃ©atoirement dans une zone dÃ©limitÃ©eDiï¬ƒcultÃ©: les centres peuvent se retrouver loin des donnÃ©esConclusion: convergence plus lente et mÃ©thode peu utilisÃ©ePrincipe: les valeurs initiales sont prises alÃ©atoirement sur des donnÃ©esAvantage: mÃ©thode simple avec de bons rÃ©sultats en pratiqueDiï¬ƒcultÃ©: certains placements sont moins pertinents que dâ€™autresOption 2: initialisation alÃ©atoire sur des donnÃ©esExemple: 2 centres initialisÃ©s trÃ¨s proches lâ€™un de lâ€™autre\nAlgorithme k-means++:  algorithme implÃ©mentant ce principePrincipe: choisir des centres initialement bien distants des autresOption 3: initialisation alÃ©atoire contrÃ´lÃ©e sur les donnÃ©es"
      }
    ]
  },
  {
    "id": "edd9c11f-6d9f-4d4d-9625-6b8cd446e740",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_20.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartChoix du nombre de clusters\n20Diï¬ƒcultÃ© de la tÃ¢che: choix pas toujours clair, et qui peut Ãªtre subjectifAlternatives: plusieurs critÃ¨res de sÃ©lection sont possibles\nOption 2: dÃ©cision en fonction de l'objectif subsÃ©quentExemple: on souhaite que le marchÃ© soit divisÃ© en exactement 3 segmentsPrincipe: le choix est fait sur base de l'objectif de la tÃ¢che de clusteringOption 1: dÃ©cision itÃ©rativeObservation: augmenter le nombre de cluster va rÃ©duire le coÃ»tPrincipe: augmenter ce nombre jusqu'Ã  ce que la rÃ©duction soit minimeExemple de gauche: prendre 4 ou 5 cluster semble appropriÃ©\nOption 3: utilisation dâ€™un autre algorithme de regroupementAï¬ƒnity propagation: le choix de ce nombre est intÃ©grÃ© dans l'algorithme\nCombien de clusters doit-on dÃ©ï¬nir pour ces donnÃ©es ?"
      }
    ]
  },
  {
    "id": "248db088-9b99-4aee-95e5-0bac16fea00c",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_21.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nTÃ¢che de regroupement: autres alternatives\n21\nhttps://scikit-learn.org/stable/modules/clustering.htmlIl existe Ã©normÃ©ment de variantes et dâ€™autres algorithmes pour rÃ©aliser une tÃ¢che de regroupementAlgorithme K-modes: dÃ©diÃ© aux donnÃ©es avec des catÃ©gories en caractÃ©ristiquesAlgorithme K-prototypes: dÃ©diÃ© aux donnÃ©es avec caractÃ©ristiques quelconques"
      }
    ]
  },
  {
    "id": "18071c41-9d42-4afc-ab16-7ce10137fded",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_22.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n22Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e"
      }
    ]
  },
  {
    "id": "2e469264-1404-4a03-8d7e-73e649067218",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_23.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDÃ©tection d'anomalies\n23Objectif: prÃ©dire la probabilitÃ© qu'une nouvelle donnÃ©e soit habituelleDÃ©cision: si la probabilitÃ© est infÃ©rieure Ã  un seuil, alors c'est une anomalieProblÃ¨me consistant Ã  repÃ©rer les donnÃ©es hors-normes  par rapport Ã  ensemble de donnÃ©es dans la normeDÃ©tection dâ€™anomalies (anomaly detection)\nPrincipe de la dÃ©tection d'anomaliesEtape 2: construire un modÃ¨le reï¬‚Ã©tant une utilisation normale du systÃ¨me, sur base de ces donnÃ©esEtape 3: utiliser ce modÃ¨le aï¬n de prÃ©dire si une nouvelle utilisation est normale ou suspecteEtape 1: collecter un ensemble de donnÃ©es d'utilisation normale d'un systÃ¨me"
      }
    ]
  },
  {
    "id": "febeddb9-b9a8-4f0b-be14-22e093ad1367",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_24.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDÃ©tection d'anomalies - Formalisation\n24{x(1),x(2),â€¦,x(m)}:lesÂ donnÃ©esÂ d'entraÃ®nementx(i)âˆˆâ„n:dimensionÂ deÂ chaqueÂ donnÃ©eÂ (nombreÂ deÂ caractÃ©ristiques)Ì‚y=f(x):â„™(xÂ estÂ uneÂ utilisationÂ normaleÂ duÂ systÃ¨me)Ì‚yâ‰¥Ïµ:laÂ donnÃ©eÂ xÂ estÂ dansÂ laÂ normeÌ‚y<Ïµ:laÂ donnÃ©eÂ xÂ estÂ uneÂ anomaliex(ğ—ğ–¾ğ—Œğ—):donnÃ©eÂ deÂ test\nUtilisation CPUUtilisation RAM\nHypothÃ¨se que lâ€™on va faire: chaque caractÃ©ristique est rÃ©gie par une loi normale qui lui est propreEn dessous du seuil: considÃ©rÃ© comme une anomalieÏµ=0.75Ïµ=0.5Ïµ=0.1Le gestionnaire pourra ensuite analyser cette utilisation en dÃ©tail\nComment crÃ©er une fonction pour cette tÃ¢che ?Ïµâˆˆ[0,1]:seuilÂ deÂ dÃ©cisionÂ pourÂ laÂ considÃ©rationÂ d'uneÂ anomalieExemple: ensemble de donnÃ©es Ã  deux caractÃ©ristiquesCela va passer par la dÃ©ï¬nition dâ€™une hypothÃ¨se sur la fonction\nLâ€™objectif est dâ€™apprendre une fonction de prÃ©diction"
      }
    ]
  },
  {
    "id": "d7ef2466-5093-4119-89db-53ff156d6f0a",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_25.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDistribution normale (gausienne)\n25\nLoiÂ normale:Â p(x|Î¼,Ïƒ2)=1Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼)22Ïƒ2)Ïƒ2:varianceÂ (paramÃ¨treÂ deÂ laÂ loi)xâˆ¼ğ’©(Î¼,Ïƒ2):xÂ prendÂ uneÂ valeurÂ tirÃ©eÂ d'uneÂ distibutionÂ normaleÂ deÂ paramÃ¨tresÂ Î¼,Ïƒ2Î¼:espÃ©ranceÂ (paramÃ¨treÂ deÂ laÂ loi)HypothÃ¨se: chaque caractÃ©ristique est tirÃ©e d'une loi normale spÃ©ciï¬quex1âˆ¼ğ’©(Î¼1,Ïƒ21)x2âˆ¼ğ’©(Î¼2,Ïƒ22)xnâˆ¼ğ’©(Î¼n,Ïƒ2n)â€¦ConsÃ©quence: chaque caractÃ©ristique a ses propres paramÃ¨tres, relatifs Ã  une loi normale\nCombien de paramÃ¨tres doit-on dÃ©terminer pour des donnÃ©es Ã  n caractÃ©ristiques ?Loi normale: comporte 2 paramÃ¨tres (espÃ©rance, et variance)\nIdÃ©e: les paramÃ¨tres des ces lois normales sont appris Ã  lâ€™aide des donnÃ©es disponiblesTotal: 2n paramÃ¨tres car on a n lois normales dans notre modÃ¨le"
      }
    ]
  },
  {
    "id": "fbc15b17-6dce-4b9a-9581-f50f789a35dc",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_26.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartFonction de vraisemblance\n26\nComment dÃ©terminer ces paramÃ¨tres ?Objectif: obtenir la loi normale qui est la plus plausible (ou vraisemblable) par rapport Ã  nos donnÃ©es\nBesoin: formaliser mathÃ©matiquement cette notion de vraisemblanceMesure de la vraisemblance d'une distribution statistiques pour les donnÃ©es considÃ©rÃ©esFonction de vraisemblance (likelihood function)\nL(Î¼,Ïƒ2|x(1),â€¦,x(m))=p(x(1)|Î¼,Ïƒ2)Ã—â€¦Ã—p(x(m)|Î¼,Ïƒ2)\nNote: cette fonction donne un score dâ€™Ã  quel point le modÃ¨le (la loi normale) ï¬tte les donnÃ©esPrincipe du calcul: on prend le produit des probabilitÃ©s de chaque donnÃ©e Ã©tant donnÃ© la loiCondition: suppose que les donnÃ©es sont indÃ©pendantes, et identiquement distribuÃ©es\nAttention: confusion courante entre la notion de probabilitÃ© et de vraisemblanceProbabilitÃ©: plausibilitÃ© d'obtenir un Ã©vÃ¨nement alÃ©atoire selon un certain modÃ¨leVraisemblance: plausibilitÃ© d'un modÃ¨le, Ã©tant donnÃ© l'observation de rÃ©alisations d'une variable alÃ©atoire"
      }
    ]
  },
  {
    "id": "d3d84c14-6f37-473d-bcd6-7f48ed80c6ed",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_27.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartEstimation du maximum de vraisemblance\n27ProblÃ¨me consistant Ã  trouver les paramÃ¨tres d'une loi normale aï¬n de maximiser la fonction de vraisemblance par rapport aux donnÃ©es observÃ©es Maximum de vraisemblance (maximum likelihood estimation - MLE)\nmaxÎ¼,Ïƒ2L(Î¼,Ïƒ2|x(1),â€¦,x(m))Note: cette dÃ©ï¬nition peut Ã©galement Ãªtre Ã©tendue Ã  des modÃ¨les qui ne sont pas des lois normales\nQuelles sont ces valeurs ?Intuition: on souhaite construire la loi normale la plus plausible par rapport aux donnÃ©es Objectif: trouver les valeurs des deux paramÃ¨tres maximisant la fonction de vraisemblance\nPreuve: https://www.youtube.com/watch?v=Dn6b9fCIUpMAttention: notez bien qu'on a une loi normale par caractÃ©ristique des donnÃ©esRÃ©sultat: ces paramÃ¨tres donnent la loi normale la plus vraisemblable pour nos donnÃ©es\nBonne nouvelle: il sâ€™agit dâ€™un rÃ©sultat statistique connu\nNote: la dÃ©ï¬nition des paramÃ¨tres peut Ãªtre beaucoup plus diï¬ƒcile avec dâ€™autres modÃ¨les\nÎ¼=1mmâˆ‘i=1x(i)\nÏƒ2=1mmâˆ‘i=1(x(i)âˆ’Î¼)2"
      }
    ]
  },
  {
    "id": "440c7241-64d8-4661-815d-90e014b5f7ac",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_28.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartEstimations des paramÃ¨tres pour la dÃ©tection d'anomalies\n28HypothÃ¨se 1: chaque caractÃ©ristique vient d'une loi normale spÃ©ciï¬queâ€¦Objectif: trouver les meilleurs paramÃ¨tres relatifs Ã  la distribution normale de chaque paramÃ¨treHypothÃ¨se 2: les caractÃ©ristiques sont considÃ©rÃ©es indÃ©pendantes des autres \nDonnÃ©es d'entrainement\nDistribution de l'utilisation CPU\nDistribution de l'utilisation RAMModÃ¨le ï¬nal: probabilitÃ© jointe des distributions de chaque caractÃ©ristique (leur multiplication) \nÌ‚y=p(x1|Î¼1,Ïƒ21)Ã—â€¦Ã—p(xn|Î¼n,Ïƒ2n)=nâˆj=1p(xj|Î¼j,Ïƒ2j)Suppose l'indÃ©pendance entre chaque caractÃ©ristique\nx1âˆ¼ğ’©(Î¼1,Ïƒ21)\nxnâˆ¼ğ’©(Î¼n,Ïƒ2n)"
      }
    ]
  },
  {
    "id": "ef2c4e73-3d25-4cc8-a196-ed1b6d7dcce1",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_29.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme de dÃ©tection d'anomalies\n29Phase d'entraÃ®nement\nPhase d'Ã©valuationObjectif: dÃ©terminer les paramÃ¨tres des lois normales\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡Î¼1,Ïƒ21,â€¦,Î¼n,Ïƒ2nğ–¿ğ—ˆğ—‹jâˆˆ{1,â€¦,n}:Î¼j=1mmâˆ‘i=1x(i)jÏƒ2j=1mmâˆ‘i=1(x(i)jâˆ’Î¼j)2ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–³ğ—‹ğ–ºğ—‚ğ—‡ğ—‚ğ—‡ğ—€(D):On utilise les donnÃ©es d'entraÃ®nementPour chaque caractÃ©ristiqueâ€¦...On calcule les deux paramÃ¨tresOn retourne tous les paramÃ¨tres (c-Ã -d, le modÃ¨le)\nÌ‚y=nâˆj=1p(x(ğ—ğ–¾ğ—Œğ—)j|Î¼j,Ïƒ2j)ğ—‚ğ–¿Ì‚y<Ïµ:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–ºğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)ğ–¾ğ—…ğ—Œğ–¾:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ—‹ğ–¾ğ—€ğ—ğ—…ğ–ºğ—‹ğ–½ğ–ºğ—ğ–ºğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)=nâˆj=11Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼j)22Ïƒ2j)ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡(x(ğ—ğ–¾ğ—Œğ—),Ïµ,Î¼1,Ïƒ21,â€¦,Î¼n,Ïƒ2n):Objectif: calculer la probabilitÃ© qu'une nouvelle donnÃ©e soit rÃ©guliÃ¨reOn utilise les lois normales dÃ©terminÃ©es prÃ©cÃ©demmentOn calcule la probabilitÃ© jointe que la nouvelle donnÃ©e soit rÃ©guliÃ¨reSi elle est en dessous du seuil, on la considÃ¨re comme une anomalie \nNotre modÃ¨le est le produit des lois normales de chaque caractÃ©ristique"
      }
    ]
  },
  {
    "id": "a7d570b0-6dfb-43f0-a379-349c2fb970de",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_30.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartLimitations de notre modÃ¨le\n30\nQuelles sont les limitations de notre modÃ¨le ?Exemple: considÃ©rons les donnÃ©es suivantes\nEst-ce que la donnÃ©e rouge est une anomalie ou non ?\nLimitation: lâ€™indÃ©pendance des caractÃ©ristiques est une hypothÃ¨se trop restrictive pour ces situationsVisuellement: il semble que oui car elle est fort diï¬€Ã©rente de nos donnÃ©es d'entraÃ®nementDans notre modÃ¨le: chaque caractÃ©ristique prise sÃ©parÃ©ment possÃ¨de une valeur probableSoucis: on ne tient pas compte de la corrÃ©lation entre nos caractÃ©ristiquesDiï¬ƒcultÃ©: cette anomalie ne sera pas probablement pas dÃ©tectÃ©e par notre modÃ¨leNature de lâ€™anomalie: câ€™est d'avoir une valeur basse pour x1 et une haute pour x2 simultanÃ©ment"
      }
    ]
  },
  {
    "id": "649ff107-b984-433f-9dcf-da7be2b18dcf",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_31.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDistribution normale multivariÃ©e\n31Principe clef: la probabilitÃ© dâ€™occurence pour chaque caractÃ©ristique est rÃ©git par sa propre loi normaleÎ¼âˆˆâ„:espÃ©ranceÏƒ2âˆˆâ„:variance\nModÃ¨leÂ actuel:Â Ì‚y=p(x1|Î¼1,Ïƒ21)Ã—â€¦Ã—p(xn|Î¼n,Ïƒ2n)=nâˆj=1p(xj|Î¼j,Ïƒ2j)Total:2nÂ paramÃ¨tres\nComment peut-on tenir compte des corrÃ©lations entre les caractÃ©ristiques ?ModÃ¨le basÃ© sur une distribution normale multivariÃ©e\nÌ‚y=p(x|Î¼,Î£)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))Î£âˆˆâ„nÃ—n:matriceÂ variance-covarianceÂ (variance/covarianceÂ entreÂ chaqueÂ paireÂ deÂ caractÃ©ristiques)Î¼âˆˆâ„n:vecteurÂ desÂ espÃ©rancesÂ (uneÂ valeurÂ pourÂ chaqueÂ caractÃ©ristique)Principe: il sâ€™agit dâ€™une gÃ©nÃ©ralisation de la loi normale Ã  plusieurs dimensions\nDiï¬€Ã©rence fondamentale: permet d'exprimer la corrÃ©lation entre les caractÃ©ristiquesApprentissage: dÃ©terminer les paramÃ¨tres du modÃ¨leIntuition: on aura un modÃ¨le plus expressif que le prÃ©cÃ©dent"
      }
    ]
  },
  {
    "id": "418dd518-5988-4125-a01e-712f6cb7090c",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_32.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart32Distribution normale multivariÃ©e - visualisation\nEspÃ©rance centrÃ©e en (0,0)MÃªme variance pour chaque caractÃ©ristiqueEspÃ©rance centrÃ©e en (0,0)Les caractÃ©ristiques x1 sont plus dispersÃ©esEspÃ©rance centrÃ©e en (-2,3)MÃªme varianceDiagonale de la matrice: correspond Ã  la variance de chaque caractÃ©ristiqueÎ¼=(00)\nÎ£=(1001)\nÎ¼=(00)\nÎ£=(5001)\nÎ¼=(âˆ’23)\nÎ£=(1001)\nCes distributions pouvaient dÃ©jÃ  Ãªtre modÃ©lisÃ©e avec notre premier modÃ¨le (covariances nulles)"
      }
    ]
  },
  {
    "id": "7570d4b5-01b9-405e-a708-7a4314ea6840",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_33.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart33Distribution normale multivariÃ©e - visualisation\nLes deux caractÃ©ristiques ont  tendance Ã  augmenter ensembleExpressivitÃ© du modÃ¨le: ces situations ne pouvaient pas Ãªtre modÃ©lisÃ©es avec notre premier modÃ¨leCovariance positiveUne caractÃ©ristique a tendance Ã   diminuer lorsque l'autre augmenteCovariance nÃ©gativeSituation quelconqueÎ¼=(00)\nÎ£=(10.90.91)\nÎ¼=(00)\nÎ£=(1âˆ’0.5âˆ’0.51)\nÎ¼=(âˆ’12)\nÎ£=(0.6âˆ’0.7âˆ’0.71)\nGÃ©nÃ©ralitÃ©: sâ€™adapte Ã  nâ€™importe quel nombre de dimensions"
      }
    ]
  },
  {
    "id": "d114d355-28ad-4341-8d20-ba41bb5606cb",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_34.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nMaximumÂ deÂ vraisemblance:Â maxÎ¼,Î£L(Î¼,Î£|x(1),â€¦,x(m))DÃ©tection d'anomalies - second modÃ¨le\n34Total:Â n+n+(n2âˆ’n)2Â paramÃ¨tresÂ paramÃ¨tresÂ Ã Â dÃ©terminerÂ (contreÂ 2nÂ dansÂ notreÂ modÃ¨leÂ prÃ©cÃ©dent)\nObjectif: trouver la meilleure loi normale multivariÃ©ePrincipe: prendre les paramÃ¨tres maximisant la fonction de vraisemblanceBonne nouvelle: il sâ€™agit encore dâ€™un rÃ©sultat statistique connuNotez bien que chaque donnÃ©e est un vecteur de dimension n x 1\nCombien de paramÃ¨tres doit-on dÃ©terminer pour n caractÃ©ristiques ?Vecteur de n paramÃ¨tresMatrice symÃ©trique de taille n x nVecteurÂ desÂ moyennes:Â Î¼=(âˆ’12)MatriceÂ variance/covariance:Â Î£=(0.6âˆ’0.7âˆ’0.71)Exemple: considÃ©rons des donnÃ©es Ã  deux caractÃ©ristiques\nAttention: ne pas oublier de considÃ©rer que la matrice variance/covariance est symÃ©trique\nÎ¼=1mmâˆ‘i=1x(i)\nÎ£=1mmâˆ‘i=1(x(i)âˆ’Î¼)(x(i)âˆ’Î¼)T"
      }
    ]
  },
  {
    "id": "dc423c8f-8f0b-4bce-b4cd-7d8b0a177638",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_35.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartAlgorithme de dÃ©tection d'anomalies - second modÃ¨le\n35Phase d'entraÃ®nement\nPhase d'Ã©valuationObjectif: dÃ©terminer les paramÃ¨tres de la loi normale multivariÃ©eOn utilise les donnÃ©es d'entraÃ®nement\nOn retourne tous les paramÃ¨tres (c-Ã -d, le modÃ¨le)Objectif: calculer la probabilitÃ© qu'une nouvelle donnÃ©e soit rÃ©guliÃ¨reOn utilise la loi normale multivariÃ©e dÃ©ï¬nieEn dessous du seuil: on considÃ¨re la donnÃ©e comme une anomalie La probabilitÃ© est dÃ©ï¬nie selon cette loi normale\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡Î¼,Î£Î¼=1mmâˆ‘i=1x(i)Î£=1mmâˆ‘i=1(x(i)âˆ’Î¼)(x(i)âˆ’Î¼)Tğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–³ğ—‹ğ–ºğ—‚ğ—‡ğ—‚ğ—‡ğ—€ğ–¬ğ—ğ—…ğ—ğ—‚(D):\nDÃ©ï¬nition du vecteur des espÃ©rancesDÃ©ï¬nition de la matrice variance/covariance\nÌ‚y=p(x(ğ—ğ–¾ğ—Œğ—)|Î¼,Î£)ğ—‚ğ–¿Ì‚y<Ïµ:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–ºğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)ğ–¾ğ—…ğ—Œğ–¾:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ—‹ğ–¾ğ—€ğ—ğ—…ğ–ºğ—‹ğ–½ğ–ºğ—ğ–ºğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–¬ğ—ğ—…ğ—ğ—‚(x(ğ—ğ–¾ğ—Œğ—),Ïµ,Î¼,Î£):"
      }
    ]
  },
  {
    "id": "e055c871-cb68-486e-8c98-06741c4222b3",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_36.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartComparaison entre les deux modÃ¨les\n36\nQuelles sont les forces et faiblesses des deux modÃ¨les ?\nAvantage: croissance linÃ©aire des paramÃ¨tres avec le nombre de caractÃ©ristiquesInconvÃ©nient: manque d'expressivitÃ© pour considÃ©rer les corrÃ©lations Avantage: trÃ¨s peu coÃ»teux Ã  calculerDistribution normale univariÃ©e\nÌ‚y=nâˆj=1p(xj|Î¼j,Ïƒ2j)=nâˆj=11Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼j)22Ïƒ2j)Î¼âˆˆâ„:espÃ©ranceÏƒ2âˆˆâ„:variance\n#paramÃ¨tres:2nDistribution normale multivariÃ©e\nÌ‚y=p(x|Î¼,Î£)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))Î£âˆˆâ„nÃ—n:matriceÂ covarianceÎ¼âˆˆâ„n:vecteurÂ desÂ espÃ©rances\n#paramÃ¨tres:2n+(n2âˆ’n)2InconvÃ©nient: la croissance des paramÃ¨tres est maintenant quadratiqueAvantage: le modÃ¨le assez expressif pour capturer les corrÃ©lationsInconvÃ©nient: plus coÃ»teux Ã  calculer (inversion dâ€™une matrice)Diï¬ƒcultÃ©: requiert un grand nombre de donnÃ©es (au moins plus que n) pour avoir une matrice inversible"
      }
    ]
  },
  {
    "id": "59e8a591-b4d4-47a4-868d-3d5203a61d6f",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_37.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartDiï¬€Ã©rences avec l'apprentissage supervisÃ©\n37\nQue pensez-vous de cette idÃ©e ?\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}\n(x(i),y(i))avecÂ y(i)=0â†’donnÃ©eÂ rÃ©guliÃ¨re(x(i),y(i))avecÂ y(i)=1â†’anomalieIdÃ©e: collecter un grand nombre de donnÃ©es concernant la tÃ¢che Ã  Ã©tudier\nApprentissage supervisÃ©: suppose un grand nombre dâ€™exemples de chaque classeDiï¬ƒcultÃ©: on a rarement un nombre suï¬ƒsant de donnÃ©es pour les anomaliesAnomalie: diï¬ƒcile Ã  caractÃ©riser (peu semblable Ã  tout ce qu'on a pu voir)MÃªme si on peut avoir beaucoup de donnÃ©es concernant les cas sains\nApprentissage non-supervisÃ©: pas sensible Ã  cette diï¬ƒcultÃ©\nConsÃ©quence: il peut Ãªtre intÃ©ressant de passer Ã  une mÃ©thode supervisÃ©eAu ï¬l du temps: on peut avoir assez de donnÃ©es pour caractÃ©riser une anomalieExemple: dÃ©tection de spams (qui sont devenus trÃ¨s frÃ©quents)Passage progressif Ã  de lâ€™apprentissage supervisÃ©"
      }
    ]
  },
  {
    "id": "435e433b-ff42-4bb3-8415-2e77db433c5e",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_38.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartTable des matiÃ¨res\n38Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e"
      }
    ]
  },
  {
    "id": "599b4494-fa53-48bf-8984-b77d970a18b0",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_39.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartSynthÃ¨se des notions vues\n39Cours donnÃ©s Ã  PolytechniqueINF8111: Fouille de donnÃ©es (Daniel Aloise)LOG6308: SystÃ¨mes de recommandation (Michel Desmarais)\nApprentissage non-supervisÃ©DÃ©ï¬nition: apprentissage qui se fait sur base de donnÃ©es non-labellisÃ©esPrincipe: exploitation de la similaritÃ© entre les donnÃ©es pour l'apprentissage\nD:{x(1),x(2),â€¦,x(m)}ProblÃ¨me de regroupementObjectif: regrouper les donnÃ©es similaires en groupes (clusters) cohÃ©rentsMÃ©thode de rÃ©solution: algorithme K-meansPrincipe: recherche locale pour minimiser une fonction de coÃ»tAmÃ©liorations possibles: redÃ©marrages, choix des positions initiales, etc.\nProblÃ¨me de dÃ©tection d'anomalies\nObjectif: prÃ©dire la probabilitÃ© quâ€™une nouvelle donnÃ©e soit rÃ©guliÃ¨reModÃ©lisation: produit de lois normales univariÃ©es ou une loi normale mutivariÃ©ePrincipe: maximiser une fonction de vraisemblance"
      }
    ]
  },
  {
    "id": "0b506643-3045-4476-882f-d35c99150684",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_40.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin Cappart\nExemples de questions d'examen\n401. Eï¬€ectuer plusieurs itÃ©rations de l'algorithme k-means sur un jeu de donnÃ©e 2. Savoir dÃ©tecter si une donnÃ©e est une anomalie Ã  l'aide des modÃ¨les vusThÃ©oriePratique1. Donner deux applications et exemple d'application de l'apprentissage non-supervisÃ©s 2. Expliquer les garanties de convergence de l'algorithme k-means 3. Expliquer les avantages/inconvÃ©nients des deux modÃ¨les pour la dÃ©tection d'anomalie"
      }
    ]
  },
  {
    "id": "6a6cf28d-19fc-4a59-a6d4-04c3eb8f8311",
    "image": "extracted_content\\M8-apprentisage-non-supervise\\images\\page_41.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Quentin CappartINF8215 - Intelligence artiï¬cielleMÃ©thodes et algorithmesApprentissage non-supervisÃ©: FIN\nDALLE: Detecting an anomaly, by Picasso"
      }
    ]
  }
]