[
  {
    "page": 1,
    "text": " \nQuentin CappartINF8175 - Intelligence artiﬁcielleMéthodes et algorithmesModule 9: Apprentissage par renforcement"
  },
  {
    "page": 2,
    "text": "Quentin CappartContenu du cours\n2Considérations pratiques et sociétalesModule 10: Utilisation en industrie, éthique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: Stratégies de recherche Module 2: Recherche en présence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisé Module 7: Réseaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisé Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques \n"
  },
  {
    "page": 3,
    "text": "Quentin CappartTable des matières\n3Apprentissage par renforcement1. Déﬁnition de l'apprentissage par renforcement 2. Illustration sur diﬀérents problèmes 3. Déﬁnition et formalisation d'un processus de décisions séquentielles 4. Formalisation d'un agent d'apprentissage par renforcement 5. Diﬃcultés en apprentissage par renforcementCe module ne fait pas partie de la matière d'examen\n"
  },
  {
    "page": 4,
    "text": "Quentin CappartCaractéristiques de l'apprentissage par renforcement\n4Apprentissage superviséL'apprentissage se faisait à l'aide de données labelliséesD:{(x(1),y(1)),(x(2),y(2)),…,(x(m),y(m))}L'objectif est d'arriver à apprendre sur base de données dont on connait la vraie valeurApprentissage non-superviséApprentissage qui se fait uniquement sur base de données non-labelliséesD:{x(1),x(2),…,x(m)}L'objectif est d'utiliser la similarité entre les données pour orienter l'apprentissageApprentissage par renforcementL'apprentissage de l'agent se fait sur base d'un processus d'essais-erreursRequiert un moyen d'évaluer la qualité des essais que l'agent eﬀectueL'agent va collecter de nouvelles données au fur et à mesure de ses essaisL'agent agit dans un environnement donnéSituations où un agent doit trouver la meilleure séquence de décisions pour remplir un objectifChaque action faite est susceptible de modiﬁer l'état de l'environnement"
  },
  {
    "page": 5,
    "text": "Quentin CappartApplication en robotique\n5\nhttps://www.youtube.com/watch?v=n2gE7n11h1YLearning to Walk via Deep Reinforcement Learning. Tuomas Haarnoja et al. Robotics: Science and Systems (RSS). 2019."
  },
  {
    "page": 6,
    "text": "Quentin CappartAlphaGo: Match contre Lee Sedol\n6\nhttps://www.youtube.com/watch?v=WXuK6gekU1YNotez la surprise générale lors du mouvement d'AlphaGo"
  },
  {
    "page": 7,
    "text": "Quentin CappartIllustration d'un problème de décisions séquentielles\n7\nPacman (Première version du jeu date de 1980)ObjectifCollecter le plus de points avant de se faire mangerToutes les informations concernant le plateauPosition du Pacman, fantômes, bonus, etc.Etat (state)Tout ce qui est pertinent pour la prise de décisionActionDécisions que le joueur peut faireRécompense (reward)Une action va changer l'état actuelUne action va engendrer un signal de récompenseSignal utilisé pour orienter la prise de décisionValeur négative pour une mauvaise actionMouvement à gauche, droite, haut, bas ou rien faire- 100 si on se déplace sur un fantômeValeur positive pour une bonne action+ 10 si on prend un bonusProcessus de décisions séquentiellesEﬀectuer la meilleure séquence de décisions aﬁn de réaliser un objectifExemple"
  },
  {
    "page": 8,
    "text": "Quentin CappartVue globale d'un processus de décisions séquentielles\n8AgentEnvironmentActionStateRewardAgentL'intelligence prenant les décisions (ou eﬀectuant les actions) aﬁn d'accomplir son objectifEnvironnementDéﬁnit la dynamique du monde (états du système, les actions possibles, l'impact des actions, etc.)Englobe toutes les informations utiles et pouvant être disponibles pour la prise de décisionsLe monde dans lequel l'agent va interagir\nL'agent et l'environnement sont en continuelle interactionL'agent modiﬁe l'environnement par ses actionsL'environnement présente des situations diﬀérentes à l'agent"
  },
  {
    "page": 9,
    "text": "Quentin CappartRécompense dans un processus de décisions séquentielles\n9Interprétation de la récompenseN'importe quel objectif à atteindre peut être décrit comme une maximisation des récompenses cumulées espérées de toutes les actions eﬀectuées.Hypothèse de la récompense\nPeut sembler restrictif, mais est assez ﬂexible et applicable dans beaucoup de situationsSignal numérique qui indique à quel point l'agent a bien agi pour un certain instant Element fondamental qui est utilisé pour formaliser l'objectif à atteindre\nHypothèse fondamentale en apprentissage par renforcement\nQue pensez vous de cette hypothèse ?L'objectif correspond ainsi à maximiser le total des récompenses cumulées espérées\nCependant, il peut être diﬃcile de déﬁnir certaines tâches en terme de récompenses...On parle de récompense espérée, car certaines actions peuvent donner diﬀérentes récompensesMaximisation des récompenses\nNote"
  },
  {
    "page": 10,
    "text": "Quentin CappartExemples de récompenses\n10\nJeu du casse-briquesLe score du jeu (fonction du nombre de briques cassées)\nPousse l'agent à maximiser son score\nJeu d'échec+1 en cas de victoire0 en cas d'égalité-1 en cas de défaite\nQue pensez vous de l'idée d'intégrer des connaissances du jeu (heuristiques) dans la fonction de récompense ?\nPortefeuille d'investissement+x pour chaque dollar gagné par vos placements-x pour chaque dollar perdu par vos placements"
  },
  {
    "page": 11,
    "text": "Quentin CappartExemples de récompenses (mauvaise conception)\n11\nCoastRunner 7\nhttps://openai.com/blog/faulty-reward-functions/Jeu de course avec des bateauxBesoin de passer par diﬀérents checkpointsPlusieurs bonus peuvent être collectés (turbo, etc.)Signal de récompenseRécompense positive si on passe par un checkpointRécompense positive si on prend un bonus \nQue pensez-vous de cette conception ?L'agent préfère collecter les bonus plutôt que ﬁnir la course\nOn récompense des sous-objectifs basés sur notre connaissance du jeu, et non l'objectif ﬁnal\nSnakeCollecter le maximum de pointsSignal de récompense+1 si on collecte un fruit\nQue pensez-vous de cette conception ?-1 pour chaque déplacement (pour aller sur le fruit au plus vite)L'agent va être pousser à se suicider pour limiter les pertes"
  },
  {
    "page": 12,
    "text": "Quentin CappartFormalisation d'une fonction objectif\n12Maximiser le total des récompenses cumuléesObjectif\nQue pensez-vous de prendre, à chaque étape, l'action  qui donne le plus de récompenses dans l'immédiat ?Les actions faites peuvent avoir un impact à long terme ExemplesUn investissement engendre des frais immédiats, mais peut prendre des années pour être proﬁtableEﬀectuer une maintenance a un coût, mais peut empêcher un dysfonctionnement dans les prochains joursLes meilleures récompenses peuvent arriver très tard, et être conditionnelles à des actions eﬀectués tôtMaximiser chaque récompense immédiate ne revient pas à maximiser la récompense globaleSouvent, il est préférable de sacriﬁer une récompense immédiate, pour une meilleure au long termeRécompense à long terme\nAvez-vous des exemples concrets ?"
  },
  {
    "page": 13,
    "text": "Quentin CappartProcessus de décision de Markov (MDP)\n13AgentEnvironmentActionStateRewardEnsembles essentielst=1,2,3,…:instants temporels (time step)St:l'état de l'environnement au temps tAt:l'action prise par l'agent au temps tp:S×A×S→[0,1]:la fonction de probabilité d'aller d'un état à un autre en faisant une certaine actionA:l'ensemble de toutes les actions possiblesS:l'ensemble de tous les états possibles\nr:S×A→ℝ:la fonction de récompense, donnant une valeur à l'action faite à partir d'un certain étatFormalisation d'un MDPHypothèse fondamentale d'un MDPChaque état est markovien\n"
  },
  {
    "page": 14,
    "text": "Quentin CappartEtat markovien\n14ℙ(St+1|St)=ℙ(St+1|S1,…,St)Intuitivement, l'état actuel contient toutes les informations utiles du passéEtat Markovien\nConnaissant uniquement la position de la voiture,  peut-on prédire la position ?Exemple: voiture en mouvementEtat amélioré: position, vitesse, accélération, etc.Un état est markovien s'il contient toutes les informations utiles du passéLa probabilité d'aller à un autre état ne dépend que de notre état présent\nAndrey Markov  (1856-1922)Une fois que l'on a notre état actuel, on peut jeter l'historique\nQue faire en cas d'états non-markoviens ?Augmenter l'état avec de nouvelles informations Etat incomplet: positionHypothèse réaliste, et simpliﬁant énormément les algorithmes de décisions"
  },
  {
    "page": 15,
    "text": "Quentin CappartVariantes des MDPs\n15Processus de décision de Markov stochastique\nProcessus de décision de Markov partiellement observable (POMDP)Il en va de même pour la récompenseChaque action comporte de l'aléatoire, et peut vous amener à diﬀérents états\nL'agent n'a qu'une vision incomplète de l'environnementVariante généralement considérée dans la formalisation standardL'agent doit construire sa propre représentation de son état\nC'est pourquoi on parle de maximisation de la récompense cumulée espérée\nL'agent ne reçoit qu'une observation de l'état de l'environnement"
  },
  {
    "page": 16,
    "text": "Quentin Cappart\nComposantes d'un agent d'apprentissage par renforcement\n16Modèle de l'environnement (model)\nPolitique de sélection d'actions (policy)Fonction de valeur (value function)La représentation que l'agent se fait de l'environnementLa représentation que l'agent se fait de la qualité de chaque état et de chaque action \nLa décision de l'action à prendre étant donné notre état\nPlacement  à cet endroit++=\nhttps://cf.shopee.ph/ﬁle/ 673d6d6c4e658a40ea481efda94820e4_tn\nEst-ce qu'un être humain peut être modélisé par ces trois composantes ?Chaque agent de RL est basé sur une ou plusieurs de ces composantes\n+Placement en  haut/droite=+Placement en  haut/droite=..."
  },
  {
    "page": 17,
    "text": "Quentin CappartModèle de l'environnement (model)\n17IntuitionReprésentation qu'a l'agent sur le fonctionnement de l'environnementp(s′ |s,a)=ℙ[St=s′ |St−1=s,At−1=a]Probabilité d'aller sur un état suivant à partir d'une action faite à l'état actuel\nr(s,a)=𝔼[Rt|St=s,At=a]Fonction de transitionp:S×S×A→[0,1]Fonction de récompenseEspérance de la récompense immédiate si une action est faite à partir de l'état actuelr:S×A→ℝNotez que cette formalisation tient compte d'un processus de décisions stochastiquesCommentairesReprend les informations sur l'eﬀet des actions (modiﬁcation de l'état, et récompenses collectées) \nEn général, l'agent ne sait pas comment fonctionne l'environnement et doit le découvrir par essais/erreurs"
  },
  {
    "page": 18,
    "text": "Quentin CappartPolitique de sélection d'action (policy)\n18Déﬁni le comportement de l'agentEtant donné notre état actuel, quelle action doit être prisea=π(s)π(a|s):ℙ[At=a|St=s]π:S→AIntuitionPolitique déterministeEtant donné mon état, mon action sera toujours la mêmePolitique stochastiqueEtant donné mon état, le choix de mon action est sujet à de l'aléatoire contrôlé \nEn quelles situations cela peut-il être bénéﬁque pour l'agent ?Notez la diﬀérence avec un environnement stochastiqueIci, c'est notre agent qui a un comportement stochastiqueParfois, la stratégie optimale implique d'agir de manière aléatoire\n"
  },
  {
    "page": 19,
    "text": "Quentin CappartFonction de valeur (value function)\n19Fonction de valeur d'un état (state-value function)Représentation interne que l'agent se fait de la qualité de chaque étatSomme espérée des récompenses obtenues à partir d'un état, si on suit une certaine politique de sélection\nVπ(s)=𝔼[Rt+γRt+1+γ2Rt+2+γ3Rt+3+…|St=s] (formalisation générale)Discounting factorValeur comprise entre 0 et 1γ∈[0,1]Permet de considérer le fait qu'il est préférable d'avoir une récompense tôt plutôt que tardValeur de 0: l'agent ne va considérer que la récompense immédiateValeur de 1: l'agent va considérer toutes les récompenses avec la même importanceVπ(s)=𝔼[Rt+Rt+1+Rt+2+Rt+3+…|St=s]\nLa valeur des récompenses futures décroit avec le tempsVπ(s)=𝔼[Rt|St=s]Vπ(s)=𝔼[Rt+Rt+1+Rt+2+Rt+3+…|St=s]Fonction de valeur d'une action (action-value function)Qπ(s,a)=𝔼[Rt+γRt+1+γ2Rt+2+γ3Rt+3+…|St=s,At=a] (formalisation générale)On peut également construire une fonction de la qualité d'une action à partir d'un certain état"
  },
  {
    "page": 20,
    "text": "Quentin CappartExemple: l'environnement du labyrinthe\n20Environnement du labyrintheS'échapper du labyrinthe en eﬀectuant le moins de déplacements\nADActions: déplacement d'une casep(s′ |s,L)=1 if s′  is the left (L) cell of s\nr(s,a)=−1∀s∈S,∀a∈Ap(s′ |s,L)=0 otherwisep(s′ |s,T)=1 if s′  is the top (T) cell of sp(s′ |s,T)=0 otherwisep(s′ |s,R)=1 if s′  is the right (R) cell of sp(s′ |s,R)=0 otherwisep(s′ |s,B)=1 if s′  is the bottom (B) cell of sp(s′ |s,B)=0 otherwiseFonctions de probabilité (ou de transition)\nFonction de récompenseEtats: la case où l'agent se trouveA={left, right, top, bottom}S=ensemble des cases du labyrinthe"
  },
  {
    "page": 21,
    "text": "Quentin CappartExemple d'un agent d'apprentissage par renforcement\n21Fonction de valeur d'un état (state-value function)\nAD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1Vπ(s)=𝔼[Rt+Rt+1+Rt+2+Rt+3+…|St=s]γ=1 (sans discount)Politique de sélection ADPolitique déterministeValeur de -1 par cases pour atteindre l'arrivée depuis une certaine casePour chaque état, quelle action doit-être eﬀectuée ?a=π(s) (déﬁni par les ﬂèches)ModèleL'agent a un accès au modèle complet de l'environnementIl connait parfaitement ls fonctions de transition et de récompense\nQuelle est la qualité de chaque état ?CommentairesNotez que l'agent considéré est déjà entraînéOn n'a pas encore abordé la manière de faire l'apprentissage"
  },
  {
    "page": 22,
    "text": "Quentin CappartCatégorisation d'agents : avec ou sans modèle\n22Agent utilisant un modèle (model-based agent)Contient dans sa représentation un modèle complet de comment l'environnement fonctionneUn modèle de l'environnement n'est pas disponible pour l'agent (ou pas utilisé)Donne la possibilité d'utiliser des algorithmes de recherche (A* par exemple)\nComment créer un agent qui résout ce genre de problèmes ?Plusieurs façons et choix de conception possiblesUn premier critère de distinction est l'utilisation/ou non d'un modèle complet de l'environnementLes fonctions de transition et de récompense sont entièrement connuesAgent n'utilisant pas un modèle (Model-free agent)L'agent ne peut apprendre le fonctionnement de l'environnement que par ses expériencesComme vous pouvez vous y attendre, c'est une tâche très diﬃcileRend diﬃcile l'utilisation d'algorithmes de rechercheL'objectif est d'apprendre par des essais-erreurs une politique de sélection ou une fonction de valeur"
  },
  {
    "page": 23,
    "text": "Quentin CappartPlaniﬁcation et apprentissage\n23Deux techniques fondamentales pour résoudre des problèmes de décision séquentielsApprentissage par renforcementPlaniﬁcation (planning) \nApproches hybridesSuppose que le modèle de l'environnement est connuL'agent utilise ce modèle pour améliorer sa politique de sélection\nUtilisation de l'apprentissage par renforcement pour apprendre un modèle de l'environnementRésolution du problème en utilisant des algorithmes de planiﬁcationPeut prendre par exemple la forme d'exploration dans un arbre d'étatsL'agent interagit avec l'environnement aﬁn d'améliorer sa politique de sélectionLe modèle de l'environnement est inconnuLa résolution est gouvernée par de l'apprentissage essais-erreurs\nLa résolution est gouvernée par un algorithme de recherche exploitant le mieux le modèleLa diﬃculté est que le nombres de possibilités est exponentiellement largeRésolution d'un MDP"
  },
  {
    "page": 24,
    "text": "Quentin CappartRésolution par planiﬁcation\n24Algorithmes de résolutionProgrammation dynamique (policy iteration, value iteration)Algorithmes de recherche (A*, minimax, monte carlo tree search)Labyrinthe simpliﬁé\nADADAD\nADADAD\nAD\nLa diﬃculté vient du fait que le nombre d'états est en général très grand\n"
  },
  {
    "page": 25,
    "text": "Quentin CappartRésolution par apprentissage par renforcement\n25Algorithmes de résolutionAlgorithmes basés sur l'apprentissage d'une fonction de valeur (Q-learning, SARSA, etc.)Algorithmes basés sur l'apprentissage d'une politique de sélection (policy gradient, A3C, PPO, etc.)Labyrinthe simpliﬁé\nLe labyrinthe est inconnu: l'agent doit le découvrir par lui mêmeAgentEnvironmentActionStateRewardAD\nADRécompense: -1"
  },
  {
    "page": 26,
    "text": "Quentin CappartCatégorisation d'agents : value-based et policy-based agents\n26Agent apprenant la fonction de valeur (value-based agent)L'agent se borne à apprendre la meilleure fonction de valeur possibleAvec cette information, la policy devient impliciteL'objectif est d'avoir la meilleure estimation de la qualité de chaque état\nQuelle est cette fonction de sélection implicite ?Chaque fois prendre l'action amenant à l'état qui a la meilleure qualitéAD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1Agent apprenant la fonction de sélection (policy-based agent)\nADL'agent tente d'apprendre directement la meilleure politique de sélectionN'utilise pas de fonction de valeursAgent hybride (actor-critic)L'agent utilise à la fois la fonction de valeur et de sélectionLa fonction de valeur n'est qu'une étape intermédiaireCe qui nous intéresse, au ﬁnal, c'est la politique de sélectionπ(a|s)=argmaxa(Qπ(s,a))∀s∈S"
  },
  {
    "page": 27,
    "text": "Quentin CappartTaxonomie des agents de RL\n27Fonction de valeurValue-based agentsPolitique de sélectionPolicy-based agentsActor-critic\nModèle de l'environnementModel-based agentModel-free agent"
  },
  {
    "page": 28,
    "text": "Quentin CappartCompromis entre l'exploration et l'exploitation\n28\nIntuitionL'apprentissage par renforcement se fait par essais-erreursL'agent doit découvrir une bonne politique à partir d'expériencesNécessite d'eﬀectuer des actions amenant à de bonnes récompensesMais aussi d'explorer des zones inconnues, pouvant être prometteusesExplorationL'objectif est d'acquérir de l'information sur l'environnementExploitationCompromisL'objectif est d'exploiter l'information aﬁn de maximiser la récompenseUn bon agent intègre un compromis entre ces deux aspectsExploitation pure: maximisation de la récompense sur base d'une vision incomplète de l'environnementExploration pure: aucune maximisation de la récompense n'est faiteCréer un agent ayant un bon compromis est un déﬁ majeur en apprentissage par renforcementhttp://ai.berkeley.edu/home.html"
  },
  {
    "page": 29,
    "text": "Quentin CappartExemples de compromis\n29Choix d'un restaurantAller continuellement au même restaurant que l'on sait bonEssayer un nouveau restaurant qui vient d'ouvrirExtraction de matières précieuses (cuivre, fer, or, etc.)Miner dans une zone où on a déjà trouvé des ﬁlonsMiner dans une nouvelle zoneStratégie pour des jeuxSuivre une stratégie qui fonctionne bienEssayer une autre stratégieCampagne de marketingUtiliser des publicités que l'on sait eﬃcacesEssayer des nouveaux prototypes de publicités\nhttp://ai.berkeley.edu/home.html\nCompétition de RL sur Minecraft: https://minerl.io/\nChoix de ﬁlmsRegarder une grosse production hollywoodienneTenter un ﬁlm d'auteurs moins connus"
  },
  {
    "page": 30,
    "text": "Quentin CappartIllustration de la diﬃculté pratique de l'exploration\n30\nDeepMind Blog https://deepmind.com/blog/article/deep-reinforcement-learning\nMontezuma's Revenge\nBreakout\nMontezuma's Revenge requiert une importante faculté exploration pour être résolu"
  },
  {
    "page": 31,
    "text": "Quentin CappartSynthèse des notions vues\n31Apprentissage par renforcementApprentissage qui se fait sur base d'un signal de récompenseL'objectif est de résoudre un processus de décisions séquentielles\nCours donné à PolytechniqueINF8953DE: Reinforcement learning (Sarath Chandar)\nAgentEnvironmentActionStateRewardLe cas standard est un processus de décision de MarkovEnsemble d'étatsEnsemble d'actionsFonction de transitionFonction de récompense\nComposantes d'un agent Modèle de l'environnement: représentation que l'agent se fait de l'environnementFonction de valeur: représentation que l'agent se fait sur la qualité de chaque étatPolitique de sélection: action que l'agent va eﬀectuer, s'il se trouve dans un état spéciﬁqueAutres ressourcesDeepMind: Cours en lignehttps://deepmind.com/learning-resources/reinforcement-learning-series-2021Livre: Reinforcement learning: an introduction (Sutton et Barto, 2nd)\n"
  },
  {
    "page": 32,
    "text": " \nQuentin CappartINF8175 - Intelligence artiﬁcielleMéthodes et algorithmesApprentissage par renforcement: FIN"
  }
]