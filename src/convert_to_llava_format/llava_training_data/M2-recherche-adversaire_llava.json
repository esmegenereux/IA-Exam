[
  {
    "id": "c9110d04-4c00-4a0f-8403-104d60891af4",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_1.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "INF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 2: Recherche adversarielle"
      }
    ]
  },
  {
    "id": "37ef8272-804a-4e4c-aa3f-831ec6c8edfe",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_2.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Contenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "1cc95fa5-1bb2-4339-9f5c-1fb765d00b05",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_3.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n3Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "1f390c89-6e23-41fe-9bd6-fde00a6e5222",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_4.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Motivation\n4Limitation des stratÃ©gies de recherche vuesHypothÃ¨se sous-jacente: lâ€™agent est seul Ã  agir dans son environnement, et rien n'interfÃ¨re avec luiAvantage: permet de modÃ©liser un grand nombre de situations et de problÃ¨mesInconvÃ©nient: lâ€™hypothÃ¨se est trop restrictive pour des situations impliquant d'autres agentsNos agents actuels sont conÃ§us pour rÃ©aliser la meilleure sÃ©quence d'actions aï¬n d'atteindre un objectifEnvironnement compÃ©titifEnvironnement dans lequel plusieurs agents Ã©voluent aï¬n de rÃ©aliser un objectif qui leur est propreCaractÃ©ristiques: engendre gÃ©nÃ©ralement de la compÃ©tition ou de la coopÃ©ration entre les agentsStratÃ©gie de recherche qui tient compte du fait que l'agent doit rÃ©aliser son objectif dans un environnement compÃ©titif, contre un ou des adversairesRecherche adversarielle"
      }
    ]
  },
  {
    "id": "5ec9edaa-a5a1-4870-ba4b-9dda7cd9641d",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_5.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Environnements compÃ©titifs rÃ©els\n5Vous avez un objectif en tÃªte, mais votre interlocuteur aussi !Exemple 3: nÃ©gociations\nExemple: nÃ©gociation de salaires, marchandage, etc.Chacun d'entre vous veut tirer un avantage de l'accordImplique gÃ©nÃ©ralement des objectifs communs, et individuelsExemple: achat d'une maison, vente aux enchÃ¨res, appel d'oï¬€re\nExemple 1: achat d'un bien avec plusieurs acheteursVous Ãªtes plusieurs Ã  vouloir acheter un bienObjectif de lâ€™acheteur: acheter le bien au prix le plus basObjectif du vendeur: vendre son bien au plus haut prixExemple 2: ï¬xer le prix d'un produit\nPlusieurs entreprises ont mis en vente un produit similaireObjectif de chaque entreprise: avoir une bonne part de marchÃ©Action possible pour une entreprise: baisser le prix, mais pas trop !\nLa plupart des environnements rÃ©els sont compÃ©titifs"
      }
    ]
  },
  {
    "id": "0b760d8a-de70-465b-97f9-97493ccd0999",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_6.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Environnement compÃ©titif populaire: les jeux\n6Raison 2: rÃ¨gles bien dÃ©ï¬nies et sans ambiguÃ¯tÃ©Raison 3: facilitÃ© pour comparer et Ã©valuer les agents Raison 5: diï¬€Ã©rents niveaux de complexitÃ© en fonction du jeu choisi\nRaison 1: environnement contrÃ´lable et facilement reprÃ©sentable Raison 4: Ã©vite les alÃ©as des environnements rÃ©els\nLes jeux constituent l'environnement compÃ©titif par excellence pour dÃ©velopper des nouvelles stratÃ©gies"
      }
    ]
  },
  {
    "id": "cc043b70-bec7-4f95-a471-fbc776e7fd4e",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_7.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Diï¬€Ã©rentes catÃ©gories de jeuxAxes principaux: alÃ©atoire, nombre de joueurs, information disponible, somme du jeu, temps-rÃ©el, etc.PrÃ©sence d'alÃ©atoireJeu dÃ©terministe: aucun alÃ©atoire n'est impliquÃ©Jeu stochastique: une part d'alÃ©atoire est prÃ©sente\nNombre de joueursCatÃ©gorie: un seul ou plusieurs adversairesDynamique du jeu: donne lieu Ã  des alliances, trahisons, conï¬‚its d'intÃ©rÃªts, etc.\nPlusieurs adversaires: les stratÃ©gies des joueurs sont parfois alignÃ©es (coopÃ©ration)Exemple: Ã  un moment du jeu, on se met ensemble contre le joueur le plus fortInformations disponiblesInformation parfaite: on connait parfaitement l'environnementInformation incomplÃ¨te: certaines donnÃ©es de l'environnement sont inconnues\n7Certaines informations ne peuvent Ãªtre disponibles qu'Ã  un joueur spÃ©ciï¬que Ses stratÃ©gies peuvent ï¬‚uctuer au cours de la partie en fonction de notre intÃ©rÃªt\nIl existe une multitude de jeux, dÃ©ï¬nis par diï¬€Ã©rentes caractÃ©ristiques"
      }
    ]
  },
  {
    "id": "fa056d9a-dce4-4e83-95c0-782c1004a716",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_8.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Diï¬€Ã©rentes catÃ©gories de jeux\n8Somme du jeuJeu Ã  somme nulle: ce qui est bon pour un joueur est mauvais pour l'autre (compÃ©tition pure)Formellement: la somme des gains d'un joueurs avec les pertes des autres vaut 0Exemple: aux Ã©checs, une victoire (1) d'un joueur engendre une dÃ©faite de l'autre joueur (-1)Jeu Ã  somme non nulle: ce qui est bon pour un joueur n'est pas forcÃ©ment mauvais pour l'autre\nMÃªme Ã  deux joueurs, la coopÃ©ration ou l'indiï¬€Ã©rence est possibleDynamique du jeuJeux physiquesTour par tour: chaque joueur eï¬€ectue leurs actions tour par tourLimitation de temps: le temps de dÃ©cision pour chaque joueur est limitÃ© (exemple: tournoi d'Ã©checs)Temps rÃ©el: chaque joueur eï¬€ectue ses actions en temps rÃ©el\nConclusionIl existe une multitude de type de jeux diï¬€Ã©rentsChaque jeu possÃ¨de certaines caractÃ©ristiques, Ã  des degrÃ©s diï¬€Ã©rents\nCaractÃ©ristique: implique gÃ©nÃ©ralement des aspects mÃ©canique et de robotiqueDiï¬ƒcultÃ©: lâ€™environnement plus diï¬ƒcile Ã  contrÃ´ler (qu'est-ce qu'une action ?)"
      }
    ]
  },
  {
    "id": "92e32de3-80b4-4f41-aa9a-17ee3e41530f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_9.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "CatÃ©gorie de rÃ©fÃ©rence\n9Cheminement scientiï¬que: Les chercheurs en IA ont commencÃ© Ã  s'intÃ©resser Ã  une catÃ©gorie de jeuxRaison: il sâ€™agit de la catÃ©gorie conceptuellement la plus simple et servant de base pour les autres variantesCatÃ©gorie de rÃ©fÃ©rence: deux joueurs, dÃ©terministe, tour par tour, Ã  information parfaite, et Ã  somme nullePar contre, elle oï¬€re dÃ©jÃ  un dÃ©ï¬ intÃ©ressant de part le nombre immense de possibilitÃ©s :-)\nDans un premier temps: câ€™est Ã  cette catÃ©gorie de jeux que nous nous intÃ©resserons principalementDans un deuxiÃ¨me temps: on verra ensuite comment Ã©tendre nos algorithmes Ã  d'autres catÃ©goriesCe domaine est trÃ¨s vaste, et nous ne pourront malheureusement pas tout couvrir\nQuelle catÃ©gorie de jeux va t-on considÃ©rer pour la conception de nos algorithmes ?"
      }
    ]
  },
  {
    "id": "fdc004b5-44e0-4cd8-bcaf-7c0f3822535d",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_10.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Formalisation standard d'un jeu\n10Un jeu de cette famille peut Ãªtre formellement dÃ©ï¬ni par les Ã©lÃ©ments suivants:Jeux dÃ©terministe, tour par tour, Ã  information parfaite, Ã  somme nulle\nS:unÂ ensembleÂ d'Ã©tatsÂ (contenantÂ unÂ Ã©tatÂ initialÂ (s0)Â etÂ un/desÂ Ã©tatsÂ terminaux)ğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ:(Sâ†’2A):l'ensembleÂ desÂ actionsÂ permisesÂ Ã Â l'Ã©tatÂ sğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡:(SÃ—Aâ†’S):retourneÂ leÂ nouvelÂ Ã©tatÂ Ã©manantÂ d'uneÂ actionÂ Ã Â unÂ Ã©tatÂ prÃ©cÃ©dentğ—ğ—ğ—‹ğ—‡:(Sâ†’P):leÂ joueurÂ devantÂ jouerÂ Ã Â l'Ã©tatÂ sğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’:(SÃ—Pâ†’â„):scoreÂ obtenuÂ pourÂ unÂ joueurÂ Ã Â unÂ Ã©tatÂ terminalÂ spÃ©ciï¬queğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…:(Sâ†’{0,1}):indiqueÂ siÂ unÂ Ã©tatÂ estÂ terminalÂ (leÂ jeuÂ estÂ terminÃ©)A:unÂ ensembleÂ d'actionsP:l'ensembleÂ desÂ joueursÂ deÂ laÂ partieÂ (2Â dansÂ notreÂ cadre)\nSimilaritÃ© 1: lâ€™ensemble des Ã©tats, des actions, et la fonction de transition forment un graphe des Ã©tatsDiï¬€Ã©rences: les autres Ã©lÃ©ments rajoutent le concept de joueurs multiples, avec chacun leur scoreSimilaritÃ© 2: la dynamique du jeu peut ainsi Ã©galement se reprÃ©senter comme un arbre de recherche\nEn rÃ©alitÃ©, c'est trÃ¨s similaire Ã  la dÃ©ï¬nition d'un problÃ¨me de recherche (module 1)"
      }
    ]
  },
  {
    "id": "25179e60-0b7f-4abb-a6d9-66ed4f3a7c23",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_11.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Exemple: Tic-tac-toe\n11\nEtat initialActions possibles\nEtats terminauxComme le jeu est Ã  somme nulle, le score pour le joueur 2 est l'opposÃ© de celui du joueur 110-1-101Score du premier joueur\nJoueur 1 (X)Joueur 2 (O)"
      }
    ]
  },
  {
    "id": "b1417039-c150-478a-aee0-ef9df93385dc",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_12.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Solution pour un joueur\n12\nQue peut-on faire ?Diï¬ƒcultÃ©: on ne sait pas ce que l'adversaire va jouer !A lâ€™idÃ©al: on souhaite une politique qui va nous amener Ã  gagner le jeuTerminologie: on parle Ã©galement d'une politique de sÃ©lectionFonction qui indique l'action Ã  faire pour un Ã©tat spÃ©ciï¬quePolitique de sÃ©lection (policy)\nÏ€:Sâ†’A\nObjectif du premier joueur: maximiser son score (fonction d'utilitÃ© une fois un Ã©tat terminal atteint)Objectif du deuxiÃ¨me joueur: minimiser le score du premier joueur (jeu Ã  somme nulle)ConsÃ©quence: cela revient Ã  prÃ©voir tous les coups Ã  l'avance dans le jeu\nAttention: notez la diï¬€Ã©rence avec le module 1 ou une solution est une sÃ©quence d'action\nUne solution pour un joueur revient Ã  savoir quoi jouer pour chaque Ã©tat \nHypothÃ¨se: on suppose que lâ€™adversaire Ã  un comportement rationnel (optimal)Autrement dit: il va jouer au mieux pour rÃ©aliser son objectifRÃ©pondre Ã  cette question amÃ¨ne Ã  dÃ©ï¬nir la notion de solution optimale"
      }
    ]
  },
  {
    "id": "8fcf2bc9-59ac-4677-8dc8-c865a0ef4129",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_13.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Intuition d'une stratÃ©gie de recherche optimale\n13IdÃ©e: est de considÃ©rer que l'adversaire (et nous-mÃªme) allons jouer au mieux pour rÃ©aliser notre objectifLe premier joueur veut maximiser son score (joueur MAX)Le deuxiÃ¨me joueur veut minimiser le score du premier joueur (joueur MIN): noeud correspondant au tour du joueur MAX (pointe vers le haut pour maximiser): noeud correspondant au tour du joueur MIN (pointe vers le bas pour minimiser)\n46212835214\nQuelle est la meilleure action Ã  rÃ©aliser pour le joueur actuel (MAX) ?Noeud racine: indique le joueur qui doit jouer (3 actions possibles)Noeuds terminaux: contiennent le score du joueur actuel (MAX)ABC\nRegardons ce comportement sur un arbre de recherche"
      }
    ]
  },
  {
    "id": "39bdff0c-3bd9-4331-abdb-c26882a1a3bd",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_14.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Intuition d'une stratÃ©gie de recherche optimale\n14Principe: le meilleur score atteignable pour MAX est 14 via l'action CAction: le joueur MAX dÃ©cide de rÃ©aliser l'action C\nQue pensez vous de ce raisonnement ?Faille: on ne tient pas compte de ce que va faire le joueur MINComportement optimal: le joueur MIN rÃ©agira en choisissant l'action FRÃ©sultat: cela amÃ¨ne Ã  un score de 2 pour MAX\n46212835214ABCDEF\nPremiÃ¨re idÃ©e"
      }
    ]
  },
  {
    "id": "42770299-c468-4bca-8a2e-648d22b252d4",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_15.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Intuition d'une stratÃ©gie de recherche optimale\n1546212835214ABCPrincipe: pour chaque Ã©tat, calculer le score pouvant Ãªtre obtenu\n322min(3,12,8)min(2,4,6)min(14,5,2)max(3,2,2)3Calcul du score: de maniÃ¨re rÃ©cursive depuis les Ã©tats terminauxHypothÃ¨se: les deux joueurs ont un comportement optimal (ils sont rationnel)Le score d'un Ã©tat MIN reprend le score minimum de ses successeursLe score d'un Ã©tat MAX reprend le score maximum de ses successeurs\nL'action optimale pour MAX est A, ce qui lui amÃ¨nera Ã  un score de 3 (si MIN fait un coÃ»t optimal)\nDeuxiÃ¨me idÃ©e"
      }
    ]
  },
  {
    "id": "65fb0448-3b0e-49e2-8fde-29ee0bd63be9",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_16.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Valeur Minimax\n16Valeur qui indique le score ï¬nal (pour le joueur MAX) qui sera obtenu Ã  partir d'un Ã©tat, si les deux joueurs jouent de faÃ§on optimaleValeur Minimax\nğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹)ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s)maxaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—†ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a))ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹minaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—†ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a))ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ—‚ğ—‡ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(s)={324623128325214ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹"
      }
    ]
  },
  {
    "id": "d18e55b3-2159-45d0-9f69-170ba85c57df",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_17.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "StratÃ©gie de recherche minimax\n17\nğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(s):vâ‹†=âˆ’âˆmâ‹†=âŠ¥ğ–¿ğ—ˆğ—‹ğ–¾ğ–ºğ–¼ğ—aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)âŸ¨v,_âŸ©=ğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² )sâ€² =ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a)ğ—‚ğ–¿v>vâ‹†:vâ‹†=vmâ‹†=ağ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©\nğ—†ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(s0):âŸ¨v,mâŸ©=ğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(s0)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨v,mâŸ©ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹),âŠ¥âŸ©\nğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(s):ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s):vâ‹†=âˆmâ‹†=âŠ¥ğ–¿ğ—ˆğ—‹ğ–¾ğ–ºğ–¼ğ—aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)âŸ¨v,_âŸ©=ğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² )sâ€² =ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a)ğ—‚ğ–¿v<vâ‹†:vâ‹†=vmâ‹†=ağ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹),âŠ¥âŸ©Terminal: on retourne le scoreAucune action n'est choisie Ã  ce stadeOn analyse toutes les actionsScore pour les Ã©tats MAX\nOn retourne la meilleure action et sa valeurOn prend le score des Ã©tats successeurs (MIN)On se met du point de vue du joueur MAXOn retourne la meilleure action (m) et sa valeur (v)On lance la recherche au noeud racine\nLe score est initialisÃ© Ã  la plus petite valeur\nValeur obtenue Ã  la racine: reï¬‚Ã¨te le rÃ©sultat obtenu si les deux joueurs sont optimauxL'action choisie est celle permettant ce scÃ©narioS'il est meilleur (plus grand) que celui actuel...On le retient, ainsi que l'action46212835214s0\nFormalisons ce fonctionnement par un algorithme"
      }
    ]
  },
  {
    "id": "40c10822-008a-45ad-90e1-bdf59dccc0d4",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_18.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Illustration de l'algorithme minimax\n18ac462b1283d5214ExÃ©cution de l'algorithmeEtapeÂ 1:a=max(b,c,d)EtapeÂ 2:a=max(min(3,12,8),c,d)EtapeÂ 3:a=max(3,c,d)EtapeÂ 4:a=max(3,min(2,4,6),d)EtapeÂ 5:a=max(3,2,d)EtapeÂ 6:a=max(3,2,min(14,5,2))EtapeÂ 7:a=max(3,2,2)EtapeÂ 8:a=3On cherche Ã  calculer la valeur de l'Ã©tat a\nQuel type de recherche cet algorithme exÃ©cute ?Eï¬ƒcacitÃ© de l'algorithmeAnalyse: lâ€™algorithme revient Ã  faire une recherche en profondeur exhaustiveComplet et optimal: en supposant que le jeu ne soit pas inï¬niComplexitÃ©Â temporelle:Â ğ’ª(bm)(trÃ¨sÂ coÃ»teux)Â -Â requiertÂ d'explorerÂ TOUTÂ l'arbreComplexitÃ©Â spatiale:Â ğ’ª(bm)Â (trÃ¨sÂ eï¬ƒcace)Mauvaise nouvelle: lâ€™algorithme impraticable pour la plupart des jeuxBonne nouvelle: il sert nÃ©anmoins de base pour construire des algorithmes plus eï¬ƒcaces"
      }
    ]
  },
  {
    "id": "c2118b5e-abc2-424b-b091-aa09ea2bc6f5",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_19.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Limitation de la recherche minimax\n19b:nombreÂ moyenÂ deÂ possibilitÃ©sÂ parÂ coupÂ dansÂ leÂ jeuÂ (branchingÂ factor)m:nombreÂ moyenÂ deÂ coupsÂ pourÂ ï¬nirÂ leÂ jeuÂ (profondeurÂ deÂ l'arbreÂ -Â ply)Observation gÃ©nÃ©rale: le nombre de coups disponibles varie souvent au cours dâ€™une partieb=4m=9TailleÂ deÂ l'arbre:Â 49=262144\nPossible pour minimaxExemple: tic-tac-toe\nIl est intÃ©ressant dâ€™avoir une idÃ©e du nombre dâ€™Ã©tats possibles dans un jeu\nb=2.8m=70TailleÂ deÂ l'arbre:Â 2.870â‰ˆ1031\nTrop coÃ»teux pour minimaxExemple: jeu de damesb=35m=80TailleÂ deÂ l'arbre:Â 3580â‰ˆ10123\nTrop coÃ»teux pour minimaxExemple: jeu dâ€™Ã©checs\nb=250m=150TailleÂ deÂ l'arbre:Â 250150â‰ˆ10359\nTrop coÃ»teux pour minimaxExemple: jeu de GoRegardons quelques exemples de jeuâ€¦\nCependant, on est arrivÃ© Ã  avoir des performances surhumaines pour ces jeu (champions du monde battus)IntÃ©rÃªt: cela va nous donner une idÃ©e de la diï¬ƒcultÃ© du jeu \nhttps://en.wikipedia.org/wiki/Game_complexity"
      }
    ]
  },
  {
    "id": "cb30eb92-6573-4703-9fcb-041a24203757",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_20.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Pistes d'amÃ©lioration\n20\nAvez vous des idÃ©es d'amÃ©lioration pour notre recherche minimax ?(1) Eviter d'eï¬€ectuer du calcul inutile (ne pas explorer certaines parties de l'arbre)(2) Ne rÃ©ï¬‚Ã©chir qu'un certain nombre de coups Ã  l'avance (rÃ©duire la profondeur de la recherche) (3) Ne considÃ©rer que les actions les plus prometteuses (rÃ©duire le facteur de branchement)(4) IntÃ©grer d'autres mÃ©canismes Ã  la recherche (mÃ©moire, connaissance experte, apprentissage, etc.)Ces amÃ©liorations peuvent amener Ã  des agents de trÃ¨s bonne qualitÃ©, sur base d'un minimax(5) Utiliser une autre stratÃ©gie de recherche (Monte-Carlo tree search - MCTS)\nStockï¬shRecherche minimax Ã  sa baseUn des meilleurs logiciels aux Ã©checshttps://stockï¬shchess.org/\nAlphaGo (et amÃ©liorations dÃ©rivÃ©es)Meilleur logiciel au jeu de GoMCTS comme composant essentiel...avec de l'apprentissage profond\nOn est dans une situation ou notre Minimax n'est pas capable de rÃ©soudre des jeux non triviaux\nUne grande partie du reste de ce module est dÃ©diÃ©e Ã  explorer ces diï¬€Ã©rentes pistes"
      }
    ]
  },
  {
    "id": "48140547-e4e2-40f2-acf7-f61aa6fd420c",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_21.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n21Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "54fa4226-2cf7-4e1b-9dd3-6ef13890f13f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_22.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Source d'ineï¬ƒcacitÃ© dans la recherche Minimax \n22\nEst-ce nÃ©cessaire de parcourir tout l'arbre pour calculer le score du noeud racine ?RÃ©sultat: le noeud racine contient ainsi le score pouvant Ãªtre atteint et l'action en dÃ©coulantPrincipe minimax: la valeur de tous les Ã©tats terminaux est propagÃ©e vers le noeud racineObservation: tous les noeuds de l'arbre sont explorÃ©s pour rÃ©aliser ce calculExemple: considÃ©rons la situation oÃ¹ la valeur minimax de (b) a dÃ©jÃ  Ã©tÃ© calculÃ©e (noeuds ï¬ls explorÃ©s)\n312382461452\n(a)(c)(d)(e)(f)(b)Etape 1: le noeud (a) est MAXDÃ©duction: sa valeur sera AU MOINS 3Etape 2: le noeud (c) est MINObservation: la valeur du noeud (d) est 2DÃ©duction: (c) sera AU PLUS 2Etape 3: Ã©lagage de (e) et de (f) (opÃ©ration de pruning)Observation: quelque soit la valeur des autres ï¬ls de (c), ils n'auront aucun impact sur la valeur de (a)DÃ©duction: on peut ainsi les Ã©laguer (couper ces branches de l'arbre) et Ã©viter du travail inutile\nOk! commenÃ§ons par identiï¬er les sources dâ€™ineï¬ƒcacitÃ©s dans le minimax\nAutrement dit: peut-on faire un rÃ©sultat Ã©quivalent en explorant moins de noeuds ?"
      }
    ]
  },
  {
    "id": "b3f6eab4-57c8-497b-a71d-cdf7e536d494",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_23.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Alpha-beta pruning (Ã©lagage alpha-beta)\n23ConsidÃ©rons un noeud n de l'arbre (MIN dans ce cas)(3) Le joueur MAX a une action potentielle de se dÃ©placer au noeud n(1) Soit m un autre noeud MIN plus haut que n dans l'arbre (dÃ©jÃ  calculÃ©)\n(4) Lorsque se dÃ©placer sur m ou m' est un meilleur choix(2) Soit m' un autre noeud MIN au mÃªme niveau que n dans l'arbre (dÃ©jÃ  calculÃ©)\nEn quelles circonstances ne voudra t-il jamais se dÃ©placer au noeud n ?(5) On peut obtenir cette information en explorant certains descendants de nm\nm'n\n(6) On explore les descendants jusqu'Ã  Ãªtre sur que n est moins bon que m ou mâ€™(7) Lorsqu'on a cette certitude, on peut supprimer les branches de n non explorÃ©esCe raisonnement peut Ãªtre fait pour les joueurs MAX et MIN symÃ©triquementâ‰¥mâ‰¥mâ€² AmÃ©lioration de lâ€™algorithme minimax visant Ã  supprimer les branches de l'arbre qui n'ont  aucun impact sur le calcul des valeurs minimaxAlpha-beta pruning (Ã©lagage alpha-beta)\nEn dÃ©tectant ces situations, on pourra ainsi Ã©laguer une partie des possibilitÃ©snâ‰¤m(ouÂ mâ€² )"
      }
    ]
  },
  {
    "id": "43ebb3ad-64bc-4491-9174-bad3a6353e8f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_24.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Alpha-beta pruning (Ã©lagage alpha-beta)\n24Î²:valeurÂ duÂ meilleurÂ choixÂ trouvÃ©Â actuellementÂ pourÂ MINÂ surÂ leÂ cheminÂ d'unÂ noeudÂ jusqu'Ã Â laÂ racineÎ±:valeurÂ duÂ meilleurÂ choixÂ trouvÃ©Â actuellementÂ pourÂ MAXÂ surÂ leÂ cheminÂ d'unÂ noeudÂ jusqu'Ã Â laÂ racinePrincipe: ces bornes sont propagÃ©es de successeurs en successeurs lors de la rechercheObjectif: repÃ©rer quelles parties de l'arbre n'auront aucun impact sur les valeurs minimaxCas 1: rÃ¨gle dâ€™Ã©lagage pour un noeud MINObjectif du noeud: est avoir la plus petite valeur possibleElagage: on supprime les branches d'un noeud MIN si sa valeur actuelle est infÃ©rieure Ã  alphaSigniï¬cation: ce noeud n'a aucune chance d'Ãªtre meilleur qu'un autre dÃ©jÃ  trouvÃ© pour MAX Objectif du noeud: est avoir la plus grande valeur possibleElagage: on supprime les branches d'un noeud MAX si sa valeur actuelle est supÃ©rieure Ã  betaSigniï¬cation: ce noeud n'a aucune chance d'Ãªtre meilleur qu'un autre dÃ©jÃ  trouvÃ© pour MIN\nComment implÃ©menter ce mÃ©canisme eï¬ƒcacement ?IdÃ©e: maintenir deux valeurs supplÃ©mentaires lors de la recherche (bornes alpha-beta)InterprÃ©tation:scoreÂ minimumÂ queÂ leÂ joueurÂ MAXÂ estÂ assurÃ©Â d'obtenirÂ (borneÂ infÃ©rieureÂ surÂ leÂ score)InterprÃ©tation:scoreÂ maximumÂ queÂ leÂ joueurÂ MAXÂ peutÂ obtenirÂ (borneÂ supÃ©rieureÂ surÂ leÂ score)\nComment utiliser ces bornes ?\nCas 2: rÃ¨gle dâ€™Ã©lagage pour un noeud MAX\nâ‰¤Î±\nâ‰¥Î²"
      }
    ]
  },
  {
    "id": "8f89a834-7830-4972-9970-62588b09eece",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_25.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Alpha-beta pruning (Ã©lagage alpha-beta)\n25\nğ–ºğ—…ğ—‰ğ—ğ–ºğ–¡ğ–¾ğ—ğ–ºğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(s0):âŸ¨v,mâŸ©=ğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(s0,âˆ’âˆ,+âˆ)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨v,mâŸ©Î±Î²\nğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(s,Î±,Î²):ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s):vâ‹†=âˆ’âˆmâ‹†=âŠ¥ğ–¿ğ—ˆğ—‹ğ–¾ğ–ºğ–¼ğ—aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)âŸ¨v,_âŸ©=ğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² ,Î±,Î²)sâ€² =ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a)ğ—‚ğ–¿v>vâ‹†:vâ‹†=vmâ‹†=ağ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹),âŠ¥âŸ©\nÎ±=max(Î±,vâ‹†)ğ—‚ğ–¿vâ‹†â‰¥Î²:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©Initialisation des bornes Ã  +- l'inï¬ni (aucune information ne peut Ãªtre dÃ©duite Ã  ce stade)\nMÃ J de alpha, avec la plus haute valeur des ï¬lsPruning si une valeur est supÃ©rieure Ã  beta\nğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(s,Î±,Î²):ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s):vâ‹†=âˆmâ‹†=âŠ¥ğ–¿ğ—ˆğ—‹ğ–¾ğ–ºğ–¼ğ—aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)âŸ¨v,_âŸ©=ğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² ,Î±,Î²)sâ€² =ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a)ğ—‚ğ–¿v<vâ‹†:vâ‹†=vmâ‹†=ağ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹),âŠ¥âŸ©\nÎ²=min(Î²,vâ‹†)ğ—‚ğ–¿vâ‹†â‰¤Î±:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©Raisonnement similaire pour MINPropagation des bornes aux noeuds ï¬ls\nOn a ainsi un algorithme concret intÃ©grant le mÃ©canisme dâ€™Ã©lagage alpha-beta !"
      }
    ]
  },
  {
    "id": "a56bd744-1989-4d88-81e1-8303c99512dc",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_26.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Illustration de l'algorithme\n26Beta garde la valeur de 3La valeur du noeud MIN est propagÃ©e au parent12382461452Initialisation de la recherche et des bornesvâ‹†=âˆ’âˆÎ±=âˆ’âˆÎ²=âˆ\nBeta garde la valeur de 3 (car meilleure que 12 pour MIN)12382461452vâ‹†=âˆ’âˆÎ±=âˆ’âˆÎ²=âˆ\n12382461452vâ‹†=âˆ’âˆÎ±=âˆ’âˆÎ²=âˆPropagation des bornes actuelles au noeud ï¬ls12382461452vâ‹†=âˆ’âˆÎ±=âˆ’âˆÎ²=âˆvâ‹†=âˆÎ±=âˆ’âˆÎ²=âˆ\nBeta prend la valeur de 3 (plus grande valeur possible pour MIN)vâ‹†=âˆ’âˆ\n12382461452Î±=âˆ’âˆÎ²=âˆvâ‹†=3Î±=âˆ’âˆÎ²=3Î²=min(3,12)âˆ’âˆ<3:ğ–®ğ–ª\nvâ‹†=3Î±=âˆ’âˆÎ²=3Î²=min(3,8)âˆ’âˆ<3:ğ–®ğ–ª12382461452vâ‹†=3Î±=3Î²=âˆ3Î±=max(âˆ’âˆ,3)vâ‹†=3Î±=âˆ’âˆÎ²=3Î²=min(âˆ,3)âˆ’âˆ<3:ğ–®ğ–ªÎ±<vÂ (pasÂ deÂ pruning)âˆ>3:ğ–®ğ–ªÎ²>vÂ (pasÂ deÂ pruning)\nâŸ¨v,_âŸ©=ğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² ,Î±,Î²)"
      }
    ]
  },
  {
    "id": "256c9fc5-92ec-404c-93c4-594f6a299695",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_27.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Illustration de l'algorithme\n27Observation: on arrive Ã  un noeud oÃ¹ alpha est plus grand que beta Raison 1: on a dÃ©jÃ  un chemin oÃ¹ le joueur MAX pourra AU MOINS obtenir un score de 3 (valeur alpha)Raison 2: a ce noeud, le joueur MAX pourra AU PLUS obtenir un score de 2 (valeur assurÃ©e pour MIN)Elagage: Il n'y a aucun intÃ©rÃªt de continuer l'expansion de ce noeud312382461452vâ‹†=3Î±=3Î²=âˆÎ±=3Î²=âˆvâ‹†=âˆPropagation des bornes actuelles au noeud ï¬ls312382461452vâ‹†=3Î±=3Î²=âˆÎ±=3Î²=2vâ‹†=2Î²=min(âˆ,2)3<2:ğ–¯ğ–±ğ–´ğ–­ğ–¨ğ–­ğ–¦!Beta prend la valeur de 2 (plus grande valeur possible pour MIN)\nLa valeur du noeud MIN est propagÃ©e au parentÎ±=3Î²=2vâ‹†=223123821452vâ‹†=âˆÎ±=3Î²=âˆPropagation des bornes actuelles au noeud ï¬lsvâ‹†=3Î±=3Î²=âˆ23123821452vâ‹†=3Î±=3Î²=âˆÎ±=max(3,2)âˆ>3:ğ–®ğ–ª\nâŸ¨v,_âŸ©=ğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² ,Î±,Î²)"
      }
    ]
  },
  {
    "id": "807f9250-f33e-404e-a319-e3365cb72c6d",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_28.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Illustration de l'algorithme\n28Observation: on est encore Ã  une situation oÃ¹ alpha est plus grand que beta, mais on a ï¬ni la recherche !vâ‹†=14Î±=3Î²=14Î²=min(âˆ,14)3<14:ğ–®ğ–ªBeta prend la valeur de 14 (plus grande valeur possible pour MIN)\nvâ‹†=5Î±=3Î²=5Î²=min(14,5)3<5:ğ–®ğ–ªvâ‹†=2Î±=3Î²=2Î²=min(5,2)3<2:ğ–¯ğ–±ğ–´ğ–­ğ–¨ğ–­ğ–¦!\nBeta prend la valeur de 5 (plus grande valeur possible pour MIN)Beta prend la valeur de 2 (plus grande valeur possible pour MIN)vâ‹†=3Î±=3Î²=âˆvâ‹†=3Î±=3Î²=âˆvâ‹†=âˆÎ±=3Î²=âˆPropagation des bornes actuelles au noeud ï¬ls23123821452vâ‹†=3Î±=3Î²=âˆ231238214523232123821452vâ‹†=3Î±=3Î²=âˆ23123821452\nLâ€™algorithme a ï¬ni son exÃ©cution, et on a Ã©vitÃ© lâ€™exploration de deux noeuds"
      }
    ]
  },
  {
    "id": "f66f9d17-1667-4f11-bccc-f7c5cbbfbd9f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_29.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "PropriÃ©tÃ©s de l'algorithme\n29Observation 1: les valeurs minimax (v) sont toujours propagÃ©es vers le noeud parentObservation 2: les bornes alpha-beta sont toujours propagÃ©es vers les noeuds ï¬lsObservation 3: les bornes beta sont mises Ã  jour uniquement par les noeuds MINObservation 4: les bornes alpha sont mises Ã  jour uniquement par les noeuds MAXSignal dâ€™Ã©lagage: dÃ¨s quâ€™on est Ã  un noeud ou alpha est plus grand que betaAutrement dit: cet algorithme n'eï¬€ectue aucune approximation ! \nEst-il possible de pruner plus de branches dans notre exemple ?En pratique: des sous-arbres complets peuvent Ãªtre prunÃ©s, et non seulement des noeuds feuilles\n3232123821452(a)(b)(c)(d)\nLa valeur minimax obtenue Ã  la racine sera la mÃªme que celle obtenue par la recherche minimaxExactitude de lâ€™Ã©lagage alpha-beta\nBonne nouvelle: au niveau calculatoire, on ne rajoute aucune opÃ©ration coÃ»teuseOui: en Ã©tendant le noeud (d) avant (b) et (c)RÃ©sultat: on aurait directement obtenu le signal de pruningElagage supplÃ©mentaire: les noeuds (c) et et (b)"
      }
    ]
  },
  {
    "id": "964eea27-ab0a-46b5-848f-c12e2e2777de",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_30.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Ordre de considÃ©ration des actions\n30IdÃ©e 1: Ã©tendre d'abord les pires actions possibles pour le joueur actuel\nOrdre peu eï¬ƒcace: aucun pruning possible\nğ—†ğ–ºğ—‘ğ–µğ–ºğ—…ğ—ğ–¾(s,Î±,Î²):ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s):vâ‹†=âˆ’âˆmâ‹†=âŠ¥ğ–¿ğ—ˆğ—‹ğ–¾ğ–ºğ–¼ğ—aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)âŸ¨v,_âŸ©=ğ—†ğ—‚ğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(sâ€² ,Î±,Î²)sâ€² =ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a)ğ—‚ğ–¿v>vâ‹†:vâ‹†=vmâ‹†=ağ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹),âŠ¥âŸ©\nÎ±=max(Î±,vâ‹†)ğ—‚ğ–¿vâ‹†â‰¥Î²:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨vâ‹†,mâ‹†âŸ©IdÃ©e 2: Ã©tendre d'abord les meilleures actions pour le joueur actuel\nOrdre eï¬ƒcace: 4 noeuds prunÃ©s (environ 30% des noeuds de l'arbre)322351426421283\nRÃ¨gle d'or pour la sÃ©lectionAux noeuds MAX: Ã©tendre les actions ayant le plus haut scoreAux noeuds MIN: Ã©tendre les actions ayant le plus petit score\nA t-on accÃ¨s Ã  ces valeurs ?323283122462514\nL'eï¬ƒcacitÃ© du pruning alpha-beta dÃ©pend fortement de l'ordre dans lequel les actions sont Ã©tendues"
      }
    ]
  },
  {
    "id": "0fd2b1bc-690b-4367-a26c-8251fdae9404",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_31.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "SÃ©lection d'un ordre des actions\n31Explication: cela revient Ã  obtenir la valeur minimax (et donc rÃ©soudre le problÃ¨me)Etendre d'abord les actions qui sont plus propices Ã  Ãªtre d'une bonne qualitÃ© pour le joueur actuelOrdre de sÃ©lection des actions dans alpha-beta\nCompromis possible: on peut tenter d'estimer Ã  quel point une action est bonne pour un joueurIntuitivement: c'est ce qu'on fait lorsqu'on joue en considÃ©rant d'abord ce qui nous semble le plus favorable\nRechercheÂ minimaxÂ simple:Â ğ’ª(bm)Alpha-betaÂ pruningÂ avecÂ unÂ mauvaisÂ ordre:Â ğ’ª(bm)Â (aucuneÂ amÃ©lioration)Alpha-betaÂ pruningÂ avecÂ unÂ ordreÂ optimal:Â ğ’ª(bm)=ğ’ª(bm/2)Bonne nouvelle: le pruning peut rendre possible une exploration deux fois plus profonde !Exemple pour les Ã©checs: considÃ©rer d'abord les captures, puis les menaces, puis les dÃ©placementsUtilisation d'une heuristique: estimer les Ã©tats intermÃ©diaires et les trier en fonction d'une heuristique\nMalheureusement, on nâ€™a pas accÃ¨s Ã  ces valeurs !\nQuel impact lâ€™Ã©lagage a t-il sur la complexitÃ© temporelle de la recherche minimax simple ?\nNouveauÂ facteurÂ deÂ branchement:Â bâ†’bÂ (preuveÂ enÂ lectureÂ complÃ©mentaire)Alpha-betaÂ pruningÂ avecÂ unÂ ordreÂ alÃ©atoire:Â â‰ˆğ’ª(b3m/4)"
      }
    ]
  },
  {
    "id": "60151e2d-2e3c-4c6b-b2bf-57156ada754f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_32.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Chemins redondants et table de transposition\n32\nXXXXXXOOOOOOXXX..................\nVoyez-vous du travail inutile dans cet arbre de recherche ?Observation: un mÃªme Ã©tat est prÃ©sent plusieurs fois dans lâ€™arbre de rechercheSource nâ€™ineï¬ƒcacitÃ©: les sous-arbres dâ€™Ã©tats identiques seront aussi identiquesMauvaise nouvelle: ne pas tenir compte de ce phÃ©nomÃ¨ne engendre de grosses pertes de performances"
      }
    ]
  },
  {
    "id": "698a0468-71bb-4744-abe9-67aeef35f75f",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_33.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Chemins redondants et table de transposition\n33SÃ©quence d'actions diï¬€Ã©rentes menant Ã  un mÃªme Ã©tat dans l'arbre de rechercheChemin redondant\nMÃ©moire qui va retenir les Ã©tats dÃ©jÃ  visitÃ©s lors de la rechercheTable de transposition\nCe mÃ©canisme est trÃ¨s puissant ! (permet une exploration deux fois plus profondes aux Ã©checs)MÃªme philosophie que la recherche en graphe retenant les Ã©tats dÃ©jÃ  explorÃ©s (module prÃ©cÃ©dent)Principe: avant d'Ã©tendre un Ã©tat, on va vÃ©riï¬er qu'il ne se trouve pas dÃ©jÃ  dans la mÃ©moireImplÃ©mentation eï¬ƒcace: avec une fonction de hachage (p.e., hachage de Zobrist - lecture complÃ©mentaire)"
      }
    ]
  },
  {
    "id": "a9111fd8-1bac-4b39-a2f7-cf0200d461ad",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_34.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n34Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "8732d8f9-6679-46c3-8065-106e3d971877",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_35.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "StratÃ©gies pour amÃ©liorer la recherche\n35Plan de secours: utiliser des heuristiques pour estimer la qualitÃ© des Ã©tats intermÃ©diairesInconvÃ©nient: contrairement Ã  l'alpha-beta pruning, on eï¬€ectue des approximations\nMauvaise nouvelle: on est encore trÃ¨s loin de pouvoir Ãªtre eï¬ƒcace Ã  des jeux complexes (Ã©checs, Go, etc.)\nQue peut-on faire ?Autre mauvaise nouvelle: on doit abandonner l'idÃ©e de calculer une stratÃ©gie optimaleInconvÃ©nient: on est gÃ©nÃ©ralement trÃ¨s sensible Ã  la qualitÃ© des heuristiques choisiesObjectif: rÃ©duire la taille de lâ€™espace de recherche\nPlusieurs stratÃ©gies sont possibles selon la faÃ§on dont les heuristiques sont intÃ©grÃ©es Ã  la recherche"
      }
    ]
  },
  {
    "id": "5f8bfd1e-ec47-49ec-aa3f-b5543c66edfd",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_36.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "StratÃ©gies de type A\n36Principe: ï¬xer une profondeur limite pour une exploration complÃ¨teImplication pratique: lâ€™exploration de l'arbre est large mais peu profonde\nComment obtenir la valeur minimax des Ã©tats de la profondeur non ï¬nale ?\nValeur ?StratÃ©gie visant Ã  eï¬€ectuer une recherche jusquâ€™Ã  une certaine profondeur de lâ€™arbreStratÃ©gie de type A"
      }
    ]
  },
  {
    "id": "e35b5cc7-4b3b-48c6-8d43-8e05a892dc78",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_37.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Heuristique Minimax\n37Principe identique Ã  la valeur minimax, sauf qu'on utilise une fonction heuristique pour Ã©valuer la qualitÃ© des Ã©tats de la profondeur limiteHeuristique minimax\nğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¤ğ—ğ–ºğ—…ğ—ğ–ºğ—ğ—‚ğ—ˆğ—‡(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹)ğ—‚ğ–¿ğ—‚ğ—Œğ–¢ğ—ğ—ğ—ˆğ–¿ğ–¿ğ–£ğ–¾ğ—‰ğ—ğ—(d)maxaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a),d+1)ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹minaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a),d+1)ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ—‚ğ—‡ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(s,d)={\nHeuristicEvaluation(               ,maxPlayer)Principe 1: les Ã©tats de la profondeur limite sont estimÃ©s suivant notre heuristique d'Ã©valuationEn pratique: combinaison possible et souhaitÃ©e avec de l'alpha-beta pruningPrincipe 2: les valeurs sont propagÃ©es jusqu'Ã  la racine de la mÃªme faÃ§on que par la recherche minimaxImplication pratique: on Ã©vite de devoir aller Ã  la ï¬n de l'arbre pour pouvoir calculer l'action Ã  eï¬€ectuerHeuristicEvaluation(               ,maxPlayer)"
      }
    ]
  },
  {
    "id": "2d8e3430-b896-47e5-86cc-65a249b728b3",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_38.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Fonction heuristique des les jeux\n38Nature de lâ€™heuristique: la signiï¬cation de sa valeurModule prÃ©cÃ©dent: estimation de la distance entre un Ã©tat et l'Ã©tat ï¬nalModule actuel: estimation du score (utility) qui pourra Ãªtre obtenu via cet Ã©tat \nPourÂ lesÂ Ã©tatsÂ terminaux:Â ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¤ğ—ğ–ºğ—…ğ—ğ–ºğ—ğ—‚ğ—ˆğ—‡(s,p)=ğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,p)\nPourÂ lesÂ Ã©tatsÂ nonÂ terminaux:Â minsâˆˆSğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,p)â‰¤ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¤ğ—ğ–ºğ—…ğ—ğ–ºğ—ğ—‚ğ—ˆğ—‡(s,p)â‰¤maxsâˆˆSğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,p)Exemple: jeu d'Ã©checsValeurs ï¬nales: score de -1 pour une dÃ©faite, +1 pour une victoire, 0 pour une Ã©galitÃ© (point de vue blanc)\nInterprÃ©tation: Ã©tat mitigÃ©, avec une estimation d'un lÃ©ger avantage pour les blancsEstimation de lâ€™heuristique: 0.3\nInterprÃ©tation: Ã©tat montrant un clair avantage pour les blancsEstimation de lâ€™heuristique: 0.95Intuition: on voudra se retrouver dans un Ã©tat avec une valeur Ã©levÃ©e pour nous"
      }
    ]
  },
  {
    "id": "a25ab948-bd0a-4f9b-9017-6f1664f60379",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_39.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Conception d'une heuristique\n39PrÃ©cision: fortement corrÃ©lÃ©e avec le score ï¬nal atteignable depuis l'Ã©tatEï¬ƒcacitÃ©: pas trop coÃ»teuse Ã  calculer (l'objectif est de rendre la recherche plus rapide)La construction d'une heuristique revient souvent Ã  un processus en deux Ã©tapes(1) CaractÃ©risation: consiste Ã  transformer un Ã©tat en une sÃ©rie de caractÃ©ristiques (features) (2) Evaluation: consiste Ã  Ã©valuer la qualitÃ© d'un Ã©tat, sur base des caractÃ©ristiques identiï¬Ã©es\nExemple: jeu dâ€™Ã©checs(1) CaractÃ©risation sur base du matÃ©rielJoueurÂ blanc:Â ğ—‡_ğ—‰ğ—‚ğ—ˆğ—‡ğ—Œ=3,ğ—‡_ğ–¼ğ–ºğ—ğ–ºğ—…ğ—‚ğ–¾ğ—‹ğ—Œ=1,â€¦JoueurÂ noir:Â ğ—‡_ğ—‰ğ—‚ğ—ˆğ—‡ğ—Œ=6,ğ—‡_ğ–¼ğ–ºğ—ğ–ºğ—…ğ—‚ğ–¾ğ—‹ğ—Œ=0,â€¦(2) Evaluation pour une combinaison linÃ©aire\nCalcul des poids: soit par connaissance experte, ou apprentissage automatiqueValeurÂ desÂ piÃ¨ces:Â wğ—‰ğ—‚ğ—ˆğ—‡ğ—Œ=1,wğ–¼ğ–ºğ—ğ–ºğ—…ğ—‚ğ–¾ğ—‹ğ—Œ=3,â€¦\nQuelles sont les caractÃ©ristiques essentielles d'une bonne heuristique ?\nh(s)=w1x1(s)+w2x2(s)+â€¦+wmxm(s)\nhğ—†ğ–ºğ—ğ–¾ğ—‹ğ—‚ğ–¾ğ—…(s,p)=ğ—‡_ğ—‰ğ—‚ğ—ˆğ—‡ğ—Œ(s,p)+3Ã—ğ—‡_ğ–¼ğ–ºğ—ğ–ºğ—…ğ—‚ğ–¾ğ—‹ğ—Œ(s,p)+3Ã—ğ—‡_ğ–¿ğ—ˆğ—ğ—Œ(s,p)+5Ã—ğ—‡_ğ—ğ—ˆğ—ğ—‹ğ—Œ(s,p)+â€¦\nhblanc(s)=hğ—†ğ–ºğ—ğ–¾ğ—‹ğ—‚ğ–¾ğ—…(s,ğ–»ğ—…ğ–ºğ—‡ğ–¼)âˆ’hğ—†ğ–ºğ—ğ–¾ğ—‹ğ—‚ğ–¾ğ—…(s,ğ—‡ğ—ˆğ—‚ğ—‹)"
      }
    ]
  },
  {
    "id": "1250a9f4-625f-4ec8-929e-0099cc7dc43a",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_40.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Dangers des stratÃ©gies de type A\n40(1) Forte dÃ©pendance Ã  lâ€™heuristique: haut risque dâ€™une mauvaise action si lâ€™heuristique est mauvaise\nQuels sont les dangers/diï¬ƒcultÃ©s dâ€™une stratÃ©gie de type A ?(2) Eï¬€et de myopie: un Ã©tat paraÃ®t bon Ã  la profondeur limite, mais ne lâ€™est pas rÃ©ellement\nSituation Ã  un noeud limite: avantage pour les noirs (meilleur matÃ©riel)Diï¬ƒcultÃ©: la reine noire est non protÃ©gÃ©e et peut Ãªtre prise gratuitement par la tour du joueur blancEn rÃ©alitÃ©, l'Ã©tat est favorable pour les blancs (non visible Ã  cause de la profondeur limite)Exemple: considÃ©rons une heuristique basÃ©e sur le matÃ©riel de chaque joueur\nTour des blancs\nQuâ€™en pensez vous ?\nPrise de la reine noire\nAvez-vous une idÃ©e pour mitiger cette diï¬ƒcultÃ© ?"
      }
    ]
  },
  {
    "id": "69a2fbc4-6fe0-4814-9b6b-f5e32b04c379",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_41.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Dangers des stratÃ©gies de type A\n41AmÃ©lioration possible: autoriser de couper la recherche que sur les quiescent states\nDiï¬ƒcultÃ©: pas toujours aisÃ© de repÃ©rer un quiescent state\nTour des blancs\nPrise de la reine noirePosition sÃ©curitaire pour couper la recherche (sans action importante en attente)Etat sÃ©curitaire (quiescent state)\nExemple aux Ã©checs: ne pas couper la recherche lorsquâ€™une capture importante est en cours\nğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¤ğ—ğ–ºğ—…ğ—ğ–ºğ—ğ—‚ğ—ˆğ—‡(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹)ğ—‚ğ–¿ğ–®ğ—‡ğ–¢ğ—ğ—ğ—ˆğ–¿ğ–¿ğ–®ğ—‹ğ–¦ğ—‹ğ–¾ğ–ºğ—ğ–¾ğ—‹(d)âˆ§ğ—‚ğ—Œğ–°ğ—ğ—‚ğ–¾ğ—Œğ–¼ğ–¾ğ—‡ğ—(s)maxaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a),d+1)ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹minaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a),d+1)ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ—‚ğ—‡ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹ğ—ğ–¾ğ—ğ—‹ğ—‚ğ—Œğ—ğ—‚ğ–¼ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(s,d)={Autre amÃ©lioration: arrÃªter dâ€™Ã©tendre un noeud selon certains critÃ¨res (p.ex., dÃ©faite ou victoire assurÃ©e)"
      }
    ]
  },
  {
    "id": "8214b7d4-40b5-422a-843a-37e0043cb82e",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_42.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "StratÃ©gies de type B\n42Principe: ne pas considÃ©rer les actions qui sont mauvaises suite Ã  notre Ã©valuation heuristique\nSimilaire Ã  la faÃ§on humaine de raisonner (de par notre facultÃ© limitÃ©e Ã  considÃ©rer trop de possibilitÃ©s)RÃ©alisation simple de cette stratÃ©gie: n'Ã©tendre que les n meilleurs mouvements suivant l'heuristiqueDanger: il est possible que la meilleure action soit supprimÃ©e par ce mÃ©canismeAmÃ©lioration: intÃ©grer une profondeur variable (profond pour les bonnes actions, progressivement plus faible)StratÃ©gie visant Ã  Ã©tendre en prioritÃ© que les actions qui sont les plus prometteusesStratÃ©gie de type B\nImplication pratique: lâ€™exploration de l'arbre est Ã©troite mais profondeIdÃ©alement, on souhaite explorer jusquâ€™en bas de lâ€™arbre, mais pour un exemple restreint de scÃ©narios"
      }
    ]
  },
  {
    "id": "5c4810eb-c4f2-473d-a57c-1bf935533e86",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_43.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "DÃ©buts et ï¬ns de partie\n43\nQuelles sont les particularitÃ©s dâ€™un dÃ©but et dâ€™une ï¬n de partie dans un jeu ?DÃ©but de partieParticularitÃ© 1: lâ€™arbre de recherche est immense (exploration complÃ¨te impensable)Fin de partie\nDans les deux cas, on Ã©conomise du temps dâ€™exÃ©cution par du prÃ©-calcul et de la consommation mÃ©moireConsÃ©quence: on ï¬nit souvent par rÃ©aliser les mÃªme actions en dÃ©but de partieParticularitÃ© 2: les consÃ©quences Ã  long terme des actions sont diï¬ƒciles Ã  prÃ©voirIdÃ©e: suivre une table dâ€™ouverture pour les premiers mouvements, au lieu dâ€™explorer lâ€™arbreConstruction de la table: souvent sur base de connaissances expertes ou de statistiques\nParticularitÃ© 1: lâ€™exploration complÃ¨te de lâ€™arbre est possible mais reste coÃ»teuseParticularitÃ© 3: les consÃ©quences dâ€™une mauvaise action peuvent Ãªtre irrÃ©mÃ©diablesParticularitÃ© 2: beaucoup de ï¬n de partie peuvent se ressembler (p.e., KBNK)IdÃ©e: suivre une table de ï¬n de partie pour les derniers mouvements, au lieu dâ€™explorerConstruction de la table: par lâ€™exÃ©cution antÃ©rieure dâ€™une stratÃ©gie de recherche (Ã©ventuellement coÃ»teuse)"
      }
    ]
  },
  {
    "id": "6ecc0d10-0ede-4cfb-851d-79da150569ed",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_44.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "https://stockï¬shchess.org/Cas d'Ã©tude: Stockï¬sh\n44Combinaison des idÃ©es prÃ©cÃ©dentes: minimax, alpha-beta, et tables de transpositionStockï¬sh: logiciel entiÃ¨rement dÃ©diÃ© Ã  jouer aux Ã©checsCrÃ©ation: en 2004 (dÃ©veloppement toujours actif)CaractÃ©ristiques: open-source et code disponible sur Github\nhttps://github.com/oï¬ƒcial-stockï¬sh/Stockï¬shStratÃ©gie de recherche\nPerformances largement supÃ©rieures aux humainsUtilisation pratique: outil d'analyse sur les plateformes de jeu en ligne (Lichess)\nPerformancesIngrÃ©dient principal: heuristique excellente mais trÃ¨s complexe\nFragment de la fonction heuristiqueStockï¬sh: profondeur de plus de 30 actionsMinimax avec alpha-beta pruning: profondeur de 8 actionsSouvent la premiÃ¨re ou deuxiÃ¨me place aux concours dâ€™IAMÃ©canisme additionnel: gestion spÃ©ciï¬que des les ï¬ns de partie"
      }
    ]
  },
  {
    "id": "d788cbdd-9033-4f2c-8b23-93cb6fce8128",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_45.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n45Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "66b3e5c1-8c3c-47d4-9760-09b0e8d50667",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_46.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Motivation d'un autre algorithme de recherche\n46Diï¬ƒcultÃ©: les stratÃ©gies vues dÃ©pendent fortement de la qualitÃ© des heuristiquesConsÃ©quence: si lâ€™heuristique est mauvaise, les performances seront mÃ©diocresConsÃ©quence: pour certains jeux, il est trÃ¨s diï¬ƒcile de construire une heuristique convenable\nQue faire dans cette situation ?\nAvec les mÃ©thodes vues, Ãªtes-vous capables de crÃ©er une bonne AI pour n'importe quel jeu ? \nEn thÃ©orie: oui ! (dans un jeu dÃ©terministe, tour par tour, etc.)En pratique: on doit Ãªtre capable de construire une bonne heuristiqueDiï¬ƒcultÃ©: construire une heuristique comme pour Stockï¬sh demande des connaissances expertes\nJeu de Go: le matÃ©riel n'est pas une bonne indication d'une victoireCivilization: rÃ¨gles trÃ¨s complexes"
      }
    ]
  },
  {
    "id": "a7d64eca-c9dc-4f06-9d34-65d628957392",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_47.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Arbre de recherche de Monte Carlo - Monte Carlo Tree Search (MCTS)\n47Faiblesses du minimax heuristique avec alpha-beta pruningFaiblesse 1: extrÃªmement dÃ©pendant de la qualitÃ© de lâ€™heuristique\nFaiblesse 2: exploration profonde diï¬ƒcile avec un grand facteur de branchementJeu de Go: cas pathologique par excellente (b = 361 en dÃ©but de partie)Monte Carlo tree search (MCTS)StratÃ©gie alternative qui n'utilise nativement aucune heuristiqueIdÃ©e: estimer la qualitÃ© des Ã©tats Ã  l'aide de simulations (alÃ©atoires) de partiesLes Ã©tats amenant souvent Ã  une victoire seront considÃ©rÃ©s comme prometteursLâ€™arbre est construit en Ã©tendant en prioritÃ© des Ã©tats prometteursAvantage: la stratÃ©gie est gÃ©nÃ©rique dans le sens oÃ¹ elle peut Ãªtre exportÃ©e facilement de jeu en jeuDiï¬ƒcultÃ©: la meilleure faÃ§on de rÃ©aliser les simulations n'est pas clairement dÃ©ï¬nieAvantage: ne demande pas de dÃ©ï¬nir une fonction d'Ã©valuationDiï¬ƒcultÃ©: de par la nature alÃ©atoire de cette stratÃ©gie, on risque de manquer des lignes de jeu importantes\nSubtilitÃ©: la construction tient aussi compte de lâ€™incertitude que lâ€™on peut avoir sur la qualitÃ© dâ€™un Ã©tat"
      }
    ]
  },
  {
    "id": "623a9672-62be-4cd6-940c-74560a8aca56",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_48.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: Algorithme\n48Fonctionnement de l'algorithmeUn arbre de recherche est construit successivement via 4 phases fondamentales\nLes Ã©tapes (1) Ã  (4) sont rÃ©pÃ©tÃ©es jusqu'Ã  ce que le budget temps soit Ã©puisÃ©(1) SÃ©lection: Ã  partir de la racine, choisir des actions aï¬n d'arriver Ã  un un noeud feuille de l'arbre actuel(2) Expansion: Ã©tendre les noeuds successeurs du noeud feuille prÃ©cÃ©demment atteint(3) Simulation: prendre un successeur, simuler une partie Ã  partir de ce noeud, et observer le score obtenu(4) Backpropagation: propager cette information jusqu'Ã  la racine et mettre Ã  jour certaines valeur\nSÃ©lection\nExpansionBackpropagation\nscore = 20\nScore = 20score = 20\nSimulation\nScore = 20"
      }
    ]
  },
  {
    "id": "f582cc3d-c4e3-421c-80ac-95dc2e2f59fa",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_49.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: pseudocode et initialisation\n49t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)Initialisation de l'arbre de recherche Ã  partir de l'Ã©tat actuel (position actuelle du plateau)ItÃ©ration: tant qu'un nombre d'itÃ©rations limite n'est pas atteint, on eï¬€ectue les 4 phases\nValeur de retour: on choisit une action Ã  eï¬€ectuer sur base de lâ€™arbre partiellement construitImplÃ©mentation: fournir une procÃ©dure complÃ¨te pour ces diï¬€Ã©rentes fonctionsN(n):Â leÂ nombreÂ totalÂ deÂ simulationsÂ ayantÂ impliquÃ©Â unÂ noeudÂ nU(n):LeÂ scoreÂ totalÂ cumulÃ©Â deÂ toutesÂ lesÂ simulationsÂ pourÂ unÂ noeudÂ nInitialisation: crÃ©ation dâ€™un noeud racine correspondant Ã  l'Ã©tat actuel du jeuPrincipe: deux statistiques seront maintenues pour chaque noeud Ã©tenduValeur initiale: ces valeurs sont initialisÃ©es Ã  zÃ©ro pour la racineU=0N=0Phase de sÃ©lection: retourne un noeud feuille en vue dâ€™une extensionPhase d'extension: le noeud feuille est Ã©tendu et un de ses ï¬ls est retournÃ©Phase de simulation: le noeud ï¬ls est simulÃ© aï¬n d'en estimer sa valeurPhase de backpropagation: l'information est remontÃ©e jusqu'Ã  la racine en modiï¬ant l'arbreParamÃ¨tres: l'Ã©tat sur lequel exÃ©cuter la recherche, et le nombre d'itÃ©rations limite\nA\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):\nStratÃ©gie de recherche basÃ©e sur une rÃ©pÃ©tition de ces 4 phases fondamentales Arbre de recherche de Monte-Carlo (MCTS- Monte-Carlo tree search)"
      }
    ]
  },
  {
    "id": "980256ff-387a-4dc6-89bb-b00b1014c89c",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_50.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de sÃ©lection\n50Phase de sÃ©lectionObjectif: atteindre un noeud feuille depuis la racine en vue de lâ€™Ã©tendre\nComment sÃ©lectionner le noeud feuille Ã  Ã©tendre ?C'est ici que les statistiques des noeuds seront utilisÃ©esPremiÃ¨re idÃ©e: Ã  partir de la racine, choisir chaque fois le noeud ayant obtenu le meilleur score moyen\nğ–ºğ—ğ–¾ğ—‹ğ–ºğ—€ğ–¾ğ–²ğ–¼ğ—ˆğ—‹ğ–¾(n)=U(n)N(n)U=12U=0U=12N=31U=5N=10U=7N=20N=30N=1ACBEDExemple: considÃ©rons l'arbre suivant obtenu suite Ã  quelques itÃ©rations\nğ–ºğ—ğ–¾ğ—‹ğ–ºğ—€ğ–¾ğ–²ğ–¼ğ—ˆğ—‹ğ–¾(B)=1230=0.4ğ–ºğ—ğ–¾ğ—‹ğ–ºğ—€ğ–¾ğ–²ğ–¼ğ—ˆğ—‹ğ–¾(C)=01=0AuÂ noeudÂ Aâ†’SÃ©lection de BSÃ©lection de DRÃ©sultat: D sera le prochain noeud Ã  Ã©tendre\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)Intuition: on priorise le scÃ©nario qui en moyenne a abouti au meilleur score cumulÃ© lors des simulations\nğ–ºğ—ğ–¾ğ—‹ğ–ºğ—€ğ–¾ğ–²ğ–¼ğ—ˆğ—‹ğ–¾(D)=510=0.5ğ–ºğ—ğ–¾ğ—‹ğ–ºğ—€ğ–¾ğ–²ğ–¼ğ—ˆğ—‹ğ–¾(E)=720=0.35AuÂ noeudÂ Bâ†’"
      }
    ]
  },
  {
    "id": "af8b68ce-fb86-476e-9d96-3fac7c48baa5",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_51.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de sÃ©lection - principe UCT\n51\nQuelle est la faiblesse de cette idÃ©e ?U=12U=5N=10U=7N=20U=12N=30U=0N=1N=31ACBEDPrincipe: on choisit la meilleure action selon lâ€™estimation actuelleDiï¬ƒcultÃ©: nos estimations sont imparfaites !Surtout au dÃ©but de la rechercheSurtout lorsque le nombre de simulation est faibleExemple: Le noeud C n'a Ã©tÃ© simulÃ© qu'une seule fois, ainsi l'indication de son score actuel est peu ï¬ableCorrection possible: inciter Ã©galement Ã  la sÃ©lection des noeuds qui ont Ã©tÃ© peu simulÃ©s (haute incertitude)SÃ©lectionne dâ€™un Ã©tat successeur selon la valeur UCB (upper conï¬dence bound)RÃ¨gle de sÃ©lection UCT (upper conï¬dent bound applied to trees)\nAvec C une constante permettant de balancer le poids donnÃ© Ã  l'explorationğ–´ğ–¢ğ–¡ğŸ£(n)=U(n)N(n)+CÃ—lnN(ğ–¯ğ–ºğ—‹ğ–¾ğ—‡ğ—(n))N(n)LaÂ valeurÂ deÂ CÂ estÂ gÃ©nÃ©rallementÂ choisieÂ expÃ©rimentalementÂ (valeurÂ thÃ©oriqueÂ deÂ 2)Intuition: sÃ©lection sur base de la valeur moyenne (terme de gauche) et lâ€™incertitude (terme de droite)"
      }
    ]
  },
  {
    "id": "f27cc248-427f-4dd1-929c-2c2a0504c566",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_52.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de sÃ©lection - principe UCT\n52U=12U=5N=10U=7N=20U=12N=30U=0N=1N=31ACBED\nğ–´ğ–¢ğ–¡ğŸ£(n)=U(n)N(n)+CÃ—lnN(ğ–¯ğ–ºğ—‹ğ–¾ğ—‡ğ—(n))N(n)Premier terme: favorise l'exploitation d'une bonne actionğ–´ğ–¢ğ–¡ğŸ£(B)=1230+2Ã—ln3130=1.08ğ–´ğ–¢ğ–¡ğŸ£(C)=01+2Ã—ln311=3.7DeuxiÃ¨me terme: favorise l'exploration des noeuds peu simulÃ©sSÃ©lection de l'action menant au noeud C Fonctionnement: plus le nombre de simulation est bas, plus la valeur est grandeAutres rÃ¨gles de sÃ©lection (Ã  ne pas connaÃ®tre)Avec V(n), la variance dâ€™eï¬€ectuer lâ€™action menant au noeud nAvec P(n), une probabilitÃ© de sÃ©lectionner lâ€™action menant au n (calculÃ©e via un rÃ©seau de neurones)ğ–´ğ–¢ğ–¡ğŸ£ğ—ğ—ğ—‡ğ–¾ğ–½(n)=U(n)N(n)+CÃ—lnN(ğ–¯ğ–ºğ—‹ğ–¾ğ—‡ğ—(n))N(n)Ã—min(14,V(n)+2ln(ğ–¯ğ–ºğ—‹ğ–¾ğ—‡ğ—(n))N(n))\nğ– ğ—…ğ—‰ğ—ğ–ºğ–¹ğ–¾ğ—‹ğ—ˆğ–²ğ–¾ğ—…ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡(n)=U(n)N(n)+CÃ—P(n)Ã—N(ğ–¯ğ–ºğ—‹ğ–¾ğ—‡ğ—(n))1+N(n)"
      }
    ]
  },
  {
    "id": "df96d60d-1040-4d27-9e0b-8a894f8d0a0c",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_53.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase d'expansion\n53Phase d'expansion\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)DÃ©clenchement: dÃ¨s quâ€™un noeud feuille est atteint lors de la sÃ©lectionObjectif: gÃ©nÃ©rer les successeurs du noeud feuilleConsÃ©quence: le noeud n'est plus une feuille (sauf si lâ€™Ã©tat est terminal)Condition dâ€™expansion: que si le noeud a dÃ©jÃ  Ã©tÃ© simulÃ©\nConditionÂ pourÂ l'expansion:N(n)>0\nU=12U=0U=12N=31\nU=5N=10U=7N=20N=30N=1ACBEDInitialisation dâ€™un noeud: Les statistiques du noeud sont mises Ã  0ACBEDGFU=12U=0U=12N=31\nU=5N=10U=7N=20N=30N=1U=0N=0U=0N=0"
      }
    ]
  },
  {
    "id": "57a82ad0-c40e-429e-a89a-50a5d9d48c35",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_54.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de simulation\n54Phase de simulation (rollout, playout)\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)DÃ©clenchement: dÃ¨s quâ€™un noeud feuille atteint (sÃ©lection ou expansion)Objectif: obtenir une estimation de la qualitÃ© de lâ€™Ã©tatValeur de retour: Le score ï¬nal obtenu Ã  l'issue de la simulationConsÃ©quence: les statistiques du noeud simulÃ© sont mises Ã  jour\nN(n)=1Â (uneÂ seuleÂ simulationÂ aÂ Ã©tÃ©Â faiteÂ pourÂ ceÂ noeud)U(n)=vÂ (laÂ valeurÂ deÂ ceÂ noeudÂ estÂ leÂ rÃ©sultatÂ deÂ laÂ simulation)ACBEDGFscore = 20U=12U=0U=12N=31\nU=5N=10U=7N=20N=30N=1U=0N=0U=0N=0v=20ACBEDGFU=12U=0U=12N=31\nU=5N=10U=7N=20N=30N=1U=20N=1U=0N=0Principe: simuler le reste dâ€™une partie Ã  partir de cet Ã©tat\nComment rÃ©aliser une (bonne) simulation ?"
      }
    ]
  },
  {
    "id": "2dc33ca4-0108-47c8-b213-60541cda8fc2",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_55.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de simulation\n55IdÃ©e 1: simulation alÃ©atoireAvantage: peut fonctionner pour des jeux simples (donne une stratÃ©gie potable)Principe: les deux joueurs choisissent leurs actions alÃ©atoirement lors des simulationsInconvÃ©nient: performances moindres pour des jeux plus complexesIdÃ©e 2: intÃ©gration d'heuristiquesPrincipe: utiliser une heuristique pour biaiser les simulations vers des bonnes actions Avantage: donne de meilleurs rÃ©sultats en pratiqueInconvÃ©nient: demande quand mÃªme d'intÃ©grer des heuristiques, souvent non trivialesExemple: donner une probabilitÃ© de sÃ©lection plus haute Ã  une capture aux Ã©checsIdÃ©e 3: principe de l'auto-entrainement (self-play)(1) Partir d'un agent alÃ©atoirePrincipe: faire jouer l'agent contre lui-mÃªme lors des simulationsAlphaGo Zero: 29 millions de parties en self-play(2) mettre Ã  jour lâ€™agent aprÃ¨s x itÃ©rations, et lâ€™utiliser pour les nouvelles simulationsIntuition: l'agent va rencontrer un adversaire de plus en plus eï¬ƒcace\nAvantage: trÃ¨s simple Ã  mettre en place\nAvantage: la dÃ©pendance Ã  lâ€™heuristique est moindre que pour le cas minimax"
      }
    ]
  },
  {
    "id": "7bce0f4d-65ba-43f6-a8ca-b26c54ffee22",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_56.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: phase de backpropagation\n56Phase de backpropagation\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)ACBEDGFU=12U=0U=12N=31\nU=5N=10U=7N=20N=30N=1U=20N=1U=0N=0Objectif: mettre Ã  jour les statistiques des noeuds aprÃ¨s la simulation\nN(n)=N(n)+1Â (onÂ aÂ faitÂ uneÂ simulationÂ deÂ plusÂ impliquantÂ ceÂ noeud)U(n)=U(n)+vÂ (leÂ scoreÂ deÂ ceÂ noeudÂ estÂ misÂ Ã Â jourÂ avecÂ leÂ rÃ©sultat)\nProchaine itÃ©ration: on rÃ©pÃ¨te le processus avec le nouvel arbre et les statistiques mises Ã  jourACBEDGFU=12U=20U=32N=32\nU=5N=10U=7N=20N=30N=2U=20N=1U=0N=0DÃ©clenchement: aprÃ¨s chaque simulationImpact: sur tous les noeuds du chemin jusquâ€™Ã  la racine\nCritÃ¨re dâ€™arrÃªt: aprÃ¨s la rÃ©alisation de x itÃ©rations, ou sur base dâ€™un budget temps allouÃ©"
      }
    ]
  },
  {
    "id": "94cdc48c-aeb3-47a0-aff7-538673ab8e84",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_57.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS: choix de l'action Ã  exÃ©cuter\n57\nQuelle action choisiriez vous ? celle amenant Ã  B ou C ?Choix de l'action\nğ–¬ğ–¢ğ–³ğ–²(s0,Î˜):t=ğ—‚ğ—‡ğ—‚ğ—ğ–³ğ—‹ğ–¾ğ–¾(s0)ğ–¿ğ—ˆğ—‹iâˆˆ0ğ—ğ—ˆÎ˜:t=ğ–»ğ–ºğ–¼ğ—„ğ—‰ğ—‹ğ—ˆğ—‰ğ–ºğ—€ğ–ºğ—ğ–¾(v,nchild)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(s0)nleaf=ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—(t)nchild=ğ–¾ğ—‘ğ—‰ğ–¾ğ—‡ğ–½(nleaf)v=ğ—Œğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾(nchild)Objectif principal: dÃ©terminer la meilleure action Ã  exÃ©cuter\nN=1ACBEDGFU=12U=20U=32N=32\nU=5N=10U=7N=20N=30N=2U=20U=0N=0\nChoix intuitif: l'action ayant le meilleur score moyen\nQuel est le soucis de cette sÃ©lection ?\nğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(n)=U(n)N(n)â†’SelectionÂ deÂ CCe choix est trÃ¨s dangereux !Danger: engendre un haut risque de favoriser les actions incertainesMeilleur choix: prendre lâ€™action qui a Ã©tÃ© simulÃ©e le plus de fois\nğ–»ğ–¾ğ—Œğ—ğ– ğ–¼ğ—ğ—‚ğ—ˆğ—‡(n)=N(n)â†’SelectionÂ deÂ BExemple: seulement 2 simulations ont Ã©tÃ© faites pour CIntuition: la sÃ©lection UCB assure dÃ©jÃ  que les actions les plus simulÃ©es sont de bonne qualitÃ©"
      }
    ]
  },
  {
    "id": "54fc44fb-ac79-4a24-b237-bfa3f5fb7d6e",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_58.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "MCTS vs Minimax (avec alpha-beta pruning et heuristiques)\n58\nQuand devriez choisir ou mÃ©thode ou une autre ?Diï¬ƒcile Ã  rÃ©pondre, sans rÃ©ponse claire et dÃ©ï¬nitiveArguments en faveur de Minimax (+ amÃ©liorations)On peut uniquement Ã©noncer certaines observations(1) Lorsque vous avez une excellente heuristique que vous souhaitez utiliser(2) Lorsque le facteur de branchement est faible (exploration plus profonde)Arguments en faveur de MCTS(1) Lorsquâ€™il est trÃ¨s diï¬ƒcile de concevoir une bonne heuristique(2) Lorsque le facteur de branchement est grand (diï¬ƒcultÃ© pour minimax)(3) Lorsque vous voulez facilement transposer votre agent Ã  un autre jeuHybridation possible: construire un agent basÃ© sur ces deux stratÃ©giesExemple: utilisation dâ€™une heuristique dans une simulation(3) Lorsque des actions critiques sont impliquÃ©es (action ayant un gros impact)\nBonus: se combine assez bien avec de l'apprentissage automatique\nExemple: MCTS en dÃ©but de partie, et minimax par la suite\nACBEDGF"
      }
    ]
  },
  {
    "id": "6c6710e8-ec23-4246-a84e-bb5eaebe0f0a",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_59.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n59Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "cb565cbe-7061-4ad0-a621-b97c9588e531",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_60.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Extension Ã  d'autres types de situations\n60\nHypothÃ¨se fondamentale: on a supposÃ© que notre adversaire Ã©tait rationnel (qu'il joue le mieux possible)Limitation: il existe une multitude d'autres jeux (ou situations rÃ©elles) quâ€™on ne sait pas encore gÃ©rerAutres variantes: plus de deux joueurs, sujets Ã  l'alÃ©atoire, avec des informations cachÃ©es, etc.\nRelaxation de lâ€™hypothÃ¨se fondamentale: lâ€™adversaire nâ€™est pas forcÃ©ment rationnel\nCadre actuel: jeux Ã  deux joueurs, dÃ©terministe, tour par tour, Ã  information parfaite, et Ã  somme nulle\nQue se passe t-il si on sort de ce cadre ?"
      }
    ]
  },
  {
    "id": "66d19932-18cc-4ded-a91c-c2a109f360f6",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_61.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Minimax avec des joueurs non rationnels\n61Rappel minimax: on a la garantie que l'action choisie est optimale (si la recherche est ï¬nie)Supposition: l'adversaire est rationnel et va eï¬€ectuer la meilleure action possibleEn pratique: on a aucune assurance que lâ€™adversaire joue rationnellement\nEst-ce que minimax garde son caractÃ¨re optimal ?Bonne nouvelle: on garde lâ€™assurance dâ€™avoir un score au moins aussi bonMauvaise nouvelle: on risque de manquer un meilleur scoreIntuition: minimax envisage le pire scÃ©nario qui peut se produire pour nous\n109101010509ab\n(1) Raisonnement minimax: amÃ¨ne Ã  exÃ©cuter l'action a(2) En cas dâ€™erreur: si l'adversaire fait une erreur (action b)RÃ©sultat: le score obtenu sera alors de 10RÃ©sultat possible: on aurait pu obtenir un score de 50Note: MCTS n'est pas sensible Ã  cela (ne suppose pas un adversaire optimal)Piste d'amÃ©lioration: modÃ©liser le comportement non-optimal de l'adversaire\nExemple: arbre Ã  deux niveaux de dÃ©cisions"
      }
    ]
  },
  {
    "id": "4162ffa4-4ec6-453c-b8ec-ab5ded5458b6",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_62.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Minimax avec plus de deux joueurs\n62Jeu multi-joueur: jeu qui implique plus de deux joueursConsÃ©quence: chaque joueur a sa propre fonction d'utilitÃ© qu'il souhaite maximiserEtat terminal: donne le score atteint pour chaque joueurAdaptation du minimax: chaque joueur va faire le meilleur coup possible pour lui\nDynamique du jeu: en fonction des intÃ©rÃªts communs, des alliances peuvent Ãªtre formÃ©es et dÃ©truitesNote: le jeu n'est pas forcÃ©ment Ã  somme nulle\n(1,2,6)(4,2,3)(6,1,2)(7,4,1)(5,1,1)(0,5,2)(7,7,1)(5,4,5)(1,2,6)(6,1,2)(0,5,2)(5,4,5)(1,2,6)(0,5,2)(1,2,6)Maximiser la valeur bleueMaximiser la valeur rougeMaximiser la valeur verte"
      }
    ]
  },
  {
    "id": "571a3fb8-acb3-4da5-bdfe-304cc47799d3",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_63.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Minimax avec la prÃ©sence d'alÃ©atoire\n63\nAlÃ©atoire explicite: dynamique propre du jeu (p.e., un lancement de dÃ©s)Adversaires imprÃ©visibles: l'adversaire pas toujours rationnelActions imprÃ©visibles: l'action faite peut Ã©chouer (tir au but d'un joueur)Chance node: nouveau type de noeud, reprÃ©sentant les diï¬€Ã©rents scÃ©narios possibles\nNiveau 1: le joueur actuel (MAX) a un choix entre deux actionsNiveau 2: un Ã©vÃ¨nement alÃ©atoire se produit (lancer de dÃ© de l'adversaire avec 6 rÃ©sultats Ã©quiprobables)Niveau 3: lâ€™adversaire (MIN) a ensuite un choix entre deux actions\n1/61/61/61/61/61/61/61/61/61/61/61/6\nJeu stochastique: jeu qui implique une prÃ©sence dâ€™alÃ©atoirePoint de considÃ©ration: rajouter cette notion dans une recherche minimax"
      }
    ]
  },
  {
    "id": "dd38ad75-f61d-428b-abf9-f1198d1bc796",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_64.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Valeur Expectiminimax\n64Valeur minimax: reï¬‚Ã¨te le rÃ©sultat obtenu en une situation extrÃªme (meilleure stratÃ©gie des deux joueurs)Valeur expectiminimax: tient Ã©galement compte du rÃ©sultat moyen obtenu lors des chance nodesInterprÃ©tation: reprÃ©sente le meilleur rÃ©sultat moyen pouvant Ãªtre obtenu si les deux joueurs sont optimauxValeur qui indique le score ï¬nal espÃ©rÃ© (pour le joueur MAX) qui sera obtenu Ã  partir d'un Ã©tat, si les deux joueurs jouent de faÃ§on optimaleValeur Expectiminimax\nğ—ğ—ğ—‚ğ—…ğ—‚ğ—ğ—’(s,ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹)ğ—‚ğ–¿ğ—‚ğ—Œğ–³ğ–¾ğ—‹ğ—†ğ—‚ğ—‡ğ–ºğ—…(s)maxaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ–¤ğ—‘ğ—‰ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a))ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ–ºğ—‘ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹minaâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)ğ–¤ğ—‘ğ—‰ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a))ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ—†ğ—‚ğ—‡ğ–¯ğ—…ğ–ºğ—’ğ–¾ğ—‹ğ–¤ğ—‘ğ—‰ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(s)={âˆ‘aâˆˆğ–ºğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ—Œ(s)P(a)ğ–¤ğ—‘ğ—‰ğ–¬ğ—‚ğ—‡ğ—‚ğ—†ğ–ºğ—‘(ğ—ğ—‹ğ–ºğ—‡ğ—Œğ—‚ğ—ğ—‚ğ—ˆğ—‡(s,a))ğ—‚ğ–¿ğ—ğ—ğ—‹ğ—‡(s)=ğ–¼ğ—ğ–ºğ—‡ğ–¼ğ–¾ğ–­ğ—ˆğ–½ğ–¾ProbabilitÃ© dâ€™une action (P): chaque action alÃ©atoire a une certaine probabilitÃ© dâ€™Ãªtre obtenueNote: Ã  un chance node, une action correspond Ã  une possibilitÃ© dâ€™un tirage alÃ©atoire"
      }
    ]
  },
  {
    "id": "04deec75-6408-4d12-bb63-e7f572421425",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_65.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "275321640.90.10.90.1ABValeur Expectiminimax\n65\nQuelle action doit Ãªtre faite si on veut maximiser son score moyen ?\nMauvaise nouvelle: en pratique, lâ€™ajout dâ€™alÃ©atoire complexiï¬e Ã©normÃ©ment le problÃ¨meAlpha-beta pruning: peut-on toujours supprimer des branches ? (oui, sous certaines conditions)Taille de lâ€™arbre: considÃ©rablement plus grand car il faut explorer toutes les possibilitÃ©sFonction heuristique: comment les concevoir ?ConsidÃ©rations supplÃ©mentaires23142.11.3A:0.9Ã—2+0.1Ã—3=2.1B:0.9Ã—1+0.1Ã—4=1.32.1(1) Noeuds MIN: on prend la valeur minimale(2) Noeuds alÃ©atoires: on prend la valeur moyenne (espÃ©rance)(3) Noeud MAX: on prend la valeur maximaleConclusion expectiminimax: on va choisir lâ€™action A"
      }
    ]
  },
  {
    "id": "92b3b8ad-f3fe-4379-8945-b7132ac4d134",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_66.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "StratÃ¨goJeux partiellement observables\n66Jeux partiellement observablesInformations cachÃ©es pour tous: informations non connues de tous les joueurs (cartes communes au poker)Informations exclusives: informations disponibles seulement pour certains joueurs (unitÃ©s au StratÃ¨go)\nParticularitÃ©s des stratÃ©gies pour ces jeux\nMauvaise nouvelle: il sâ€™agit de situations trÃ¨s complexes sortant du cadre de ce cours\nStarcraft\nPoker\nWhistCaractÃ©ristiques principales: certains Ã©lÃ©ments du jeu ne sont pas visibles pour les joueurs\nAspect 1: utiliser au mieux les informations qui nous sont disponibles (exploiter une vue locale)Aspect 2: acquÃ©rir de nouvelles informations (avoir une vue globale)Aspect 3: tenir compte que lâ€™adversaire a Ã©galement des informations manquantesUne bonne stratÃ©gie passe gÃ©nÃ©ralement par la construction dâ€™une reprÃ©sentation de lâ€™environnement"
      }
    ]
  },
  {
    "id": "c58883fc-aebb-4d34-9509-b68eaf488f3d",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_67.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n67Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "7025af44-134a-43b3-942a-01cf0434d941",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_68.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Historique des agents dÃ©diÃ©s aux jeux\n68Niveau intermÃ©diaire: performance d'un joueur moyenNiveau expert: capable de battre la majoritÃ© des joueursNiveau surhumain: capable de battre les meilleurs joueurs au monde Jeu rÃ©solu: possible de prÃ©dire le rÃ©sultat d'une partie si les deux joueurs sont optimaux (niveau ultime)Niveau dÃ©butant: performance en dessous d'un joueur humain raisonnable (pas trop nul)Puissance 4\nEtat de lâ€™art: Jeu rÃ©solu (1988)RÃ©sultat: le premier joueur peut forcer la victoire s'il joue de faÃ§on optimale Preuve: basÃ©e sur des raisonnements logiquesJeu de dames (checkers)\nEtat de lâ€™art: jeu rÃ©solu (2007)RÃ©sultat: amÃ¨ne Ã  une Ã©galitÃ© si les deux joueurs sont optimauxPreuve: basÃ©e sur de l'alpha-beta pruning et une base de donnÃ©e des ï¬ns de partieLecture complÃ©mentaire disponible sur Moodle\nComment caractÃ©riser les performances dâ€™un agent intelligent par rapport Ã  la performance humaine ?"
      }
    ]
  },
  {
    "id": "367dd296-3f85-4de7-936e-a939b364147a",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_69.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Historique des agents dÃ©diÃ©s aux jeux\n69Jeu des Ã©checs\nEtat de lâ€™art: performance surhumaine (jeu non rÃ©solu)Moment clÃ©: DeepBlue a battu le champion Garry Kasparov (1997)CaractÃ©ristique: minimax intÃ©grant de l'alpha-beta pruning et des heuristiquesPerformances: capable d'Ã©valuer 100 millions de positions par seconde Performances: capable dâ€™explorer 40 coups Ã  l'avanceBackgammon\nEtat de lâ€™art: performance surhumaine (jeu non rÃ©solu)Diï¬ƒcultÃ© du jeu: fait intervenir de l'alÃ©atoire (lancer de deux dÃ©s)PostÃ©ritÃ©: premiÃ¨re IA capable de battre un champion pour un jeu complexeMoment clÃ©: BKG a battu un champion mondial (1980)CaractÃ©ristiques de lâ€™algorithme: excellente fonction d'Ã©valuationOthello\nEtat de lâ€™art: performance surhumaine (jeu non rÃ©solu)Diï¬ƒcultÃ© du jeu: il est diï¬ƒcile de crÃ©er une bonne fonction d'Ã©valuationMoment clÃ©: Logistello a battu un champion mondial (1997)Heuristique: millions de paramÃ¨tres calibrÃ©s avec une rÃ©gression linÃ©airehttps://skatgame.net/mburo/log.html"
      }
    ]
  },
  {
    "id": "bc809abf-f014-42dc-bfc7-eb803e5f81e7",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_70.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Historique des agents dÃ©diÃ©s aux jeux\n70Jeu de GoEtat de lâ€™art: performance surhumaine (jeu non rÃ©solu)Moment clÃ©: AlphaGo a battu le champion Lee Sedol (2015)MÃ©canismes utilisÃ©s: MCTS, apprentissage supervisÃ©, et par renforcement\nCaractÃ©ristique: utilisation d'un rÃ©seau de neurones (trÃ¨s) profondPostÃ©ritÃ©: AlphaGo Zero qui n'utilise plus de connaissances expertes\nStarcraft 2\nDiï¬ƒcultÃ© du jeu: informations cachÃ©es et jeu temps-rÃ©elEtat de lâ€™art: performance surhumaine (jeu non rÃ©solu)Moment clÃ©: AlphaStar a battu des champions mondiaux (2019)MÃ©canismes utilisÃ©s: apprentissage par renforcement profondCaractÃ©ristique: utilisation intensive du mÃ©canisme de self-play Poker\nDiï¬ƒcultÃ© du jeu: prÃ©sence d'alÃ©atoire et d'informations cachÃ©esMoment clÃ©: Libratus a battu des champions (parties Ã  2 joueurs, 2017)Moment clÃ©: Pluribus a battu des champions (parties Ã  6 joueurs, 2019)Etat de lâ€™art: performance surhumaine (jeu non rÃ©solu)MÃ©canismes utilisÃ©s: apprentissage profond, et notions de thÃ©orie des jeux"
      }
    ]
  },
  {
    "id": "d2295411-cd65-4f27-b94f-2ae4dd60cc56",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_71.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Historique des agents dÃ©diÃ©s aux jeux\n71\nScrabble (2006)\nDota 2 (2018)\nQuake 3 (2019)(1) Jeux demandant une riche interaction entre les joueurs (nÃ©gociation, alliance, trahison, etc.)(3) Jeux demandant une grande part d'exploration pour dÃ©celer les bonnes actions Ã  exÃ©cuter(2) Jeux ayant une composante plus abstraite et faisant appel Ã  l'imagination(4) Jeux sujets aux considÃ©rations du monde rÃ©el (physique des mouvements, etc.)\nNetHack\nIl y a encore plein dâ€™autres jeux ou lâ€™IA atteint des performances surhumaines\nEst-ce qu'il existe encore des jeux oÃ¹ les humains l'emportent ?"
      }
    ]
  },
  {
    "id": "5a35615b-62b8-4c22-9178-72f70919eadb",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_72.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "PercÃ©e rÃ©cente - Diplomacy\n72\nHuman-level play in the game of Diplomacy by combining language models with strategic reasoning (Meta, 2022)Diplomacy: jeu de plateau demandant des interactions trÃ¨s fortes avec les autres joueursCicero: agent dÃ©veloppÃ© par Meta en 2022 et ayant des performances supÃ©rieure au joueur moyen"
      }
    ]
  },
  {
    "id": "2d44755c-545f-4693-a1a1-89d24d6bde92",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_73.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "A vous de jouer !\n73Il est prÃ©fÃ©rable d'aller le plus profond possible dans lâ€™arbre (un seul niveau peut faire une grosse diï¬€Ã©rence)Utilisation dâ€™un budget temps: une stratÃ©gie de type IDS est une bonne idÃ©eImportant: dÃ©ï¬nir une bonne heuristique est un point de conception crucial\n(1) Jouez et analysez vos parties pour identiï¬er les Ã©tats qui sont favorables(2) Analysez les choix faits par votre agent et vÃ©riï¬er leur cohÃ©rence(2) Nâ€™oubliez pas de considÃ©rer les cas limitesDÃ©but: une stratÃ©gie minimax heuristique est le plus simple pour dÃ©marrer\nA votre tour dâ€™implÃ©mentez le meilleur agent pour DivercitÃ© !"
      }
    ]
  },
  {
    "id": "d4317f6d-00ef-4d8e-836d-5b09ecad4bdb",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_74.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "XXXXXXOOOOOOXXX..................Table des matiÃ¨res\n74Recherche adversarielle1. Motivation de la recherche adversarielle 2. CatÃ©gorisation d'environnements compÃ©titifs et des jeux 3. StratÃ©gie de recherche minimax 4. MÃ©canisme du alpha-beta pruning 5. Conception de fonctions d'Ã©valuation (heuristiques) 6. MÃ©canismes supplÃ©mentaires d'amÃ©lioration 7. Arbre de recherche de Monte-Carlo (Monte-Carlo tree search) 8. Extension Ã  d'autres familles de jeux (plusieurs adversaires, prÃ©sence d'alÃ©atoire, etc.) 9. Exemples et historique d'agents dÃ©diÃ©s aux jeux"
      }
    ]
  },
  {
    "id": "33d42050-b6f1-4dfb-8746-1a02905f469b",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_75.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "SynthÃ¨se des notions vues\n75StratÃ©gie de recherche MinimaxRecherche adversarielleContexte: lâ€™agent Ã©volue dans un environnement compÃ©titifEnvironnement: caractÃ©risÃ© par la prÃ©sence d'un ou de plusieurs adversaires\nCadre de rÃ©fÃ©rence: deux joueurs, dÃ©terministe, tour par tour, Ã  information parfaite, et Ã  somme nulle\n324623128325214\nMinimax classique3232123821452\nAlpha-beta pruning\nAvec des heuristiquesAutres amÃ©liorations: tables de transposition, mÃ©thodes hybrides, suppression de branches, etc.Arbre de recherche MC\nDÃ©pendance moindre Ã  une heuristiqueBasÃ© sur des simulations de partiesMÃ©thode assez gÃ©nÃ©rique"
      }
    ]
  },
  {
    "id": "fa96b0e9-e807-40cd-962b-3d5a821eb451",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_76.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "Exemples de questions d'examen\n761. ModÃ©liser une situation comme un problÃ¨me de recherche adversarielle 2. Savoir appliquer correctement un algorithme de recherche sur une situation donnÃ©e 3. Savoir construire des heuristiques pertinentesThÃ©oriePratique1. Expliquer le fonctionnement et les propriÃ©tÃ©s d'une stratÃ©gie de recherche vue 2. Donner les avantages/inconvÃ©nients d'une stratÃ©gie de recherche par rapport Ã  une autre 3. Expliquer les variations possibles de notre cadre de rÃ©fÃ©rence\nMate in 124 movesOh no ! You won again !\nFuturama (Mars University)"
      }
    ]
  },
  {
    "id": "85689ce7-e08e-4fa5-a3fe-21c07c9587af",
    "image": "src\\convert_to_llava_format\\llava_training_data\\images\\M2-recherche-adversaire_page_77.png",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nExplain the content of this page from the AI course material."
      },
      {
        "from": "gpt",
        "value": "INF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesRecherche adversarielle: FIN\nDALLE: A colored pencil drawing by CÃ©zanne of a robot playing chess"
      }
    ]
  }
]