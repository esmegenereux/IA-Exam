[
  {
    "page": 1,
    "text": " \nQuentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 3: Recherche locale"
  },
  {
    "page": 2,
    "text": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques \n"
  },
  {
    "page": 3,
    "text": "Quentin CappartTable des matiÃ¨res\n3Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 4,
    "text": "Quentin CappartRetour sur les modules prÃ©cÃ©dents\n4StratÃ©gies de recherche (module 1)Objectif: trouver la meilleure sÃ©quence d'actions pour atteindre un Ã©tat ï¬nal Ã  partir d'un Ã©tat initial\nSolution: une sÃ©quence d'actions permettant d'atteindre l'Ã©tat ï¬nal\nRecherche adversarielle (module 2)Objectif: trouver les meilleures actions Ã  exÃ©cuter dans un environnement compÃ©titif\nSolution: une politique de sÃ©lection dâ€™actions pour atteindre un Ã©tat ï¬nal\nDans les deux cas, on souhaite trouver la faÃ§on d'atteindre un Ã©tat ï¬nal, possiblement connu"
  },
  {
    "page": 5,
    "text": "Quentin CappartIntroduction aux problÃ¨mes combinatoires\n5\nEst-ce que tous les problÃ¨mes ont ce mÃªme objectif ?\nCaractÃ©ristiques de ces problÃ¨mes: l'Ã©tat ï¬nal nous est inconnu et diï¬ƒcile Ã  trouverObservation fondamentale: la sÃ©quence d'actions pour parvenir Ã  un Ã©tat ï¬nal n'est pas importanteNon: il existe certains problÃ¨mes pour lesquels on ne cherche pas une sÃ©quence dâ€™actions\nSolution au problÃ¨me: trouver un Ã©tat qui satisfait toutes les contraintes du problÃ¨meDiï¬ƒcultÃ©: lâ€™Ã©tat ï¬nal doit respecter plusieurs contraintes\nSudoku\nEntreposage de caissesCrÃ©ation dâ€™un horaire sans conï¬‚it\nChoix dâ€™une solution: entre plusieurs solutions faisables, on prÃ©fÃ¨re gÃ©nÃ©ralement la moins coÃ»teuse\nLes problÃ¨mes de ce type sont connus sous le nom de problÃ¨mes combinatoires"
  },
  {
    "page": 6,
    "text": "Quentin CappartProblÃ¨mes combinatoires de satisfaction\n6Objectif: trouver une solution faisable parmi un ensemble de solutionsSituation avec aucune solution: lâ€™objectif est de prouver qu'il nâ€™en existe aucuneProblÃ¨mes combinatoires de satisfaction (constraint satisfaction problem - CSP)\nTrouver une aiguille dans une botte de foinSolutions faisables: souvent exprimÃ©es par le biais de contraintes"
  },
  {
    "page": 7,
    "text": "Quentin Cappart\nExemples pratiques de CSPs\n7Objectif: planiï¬er la maintenance dâ€™appareils dâ€™un rÃ©seau de distribution dâ€™Ã©nergie RÃ©alisation dâ€™un horaire de maintenance\nConï¬guration autorisÃ©eObjectif: acheter un nouvel ordinateur pour un faire tourner un logicielContraintes:Budget: Max 2000 $CADMÃ©moire vive: Min 16 Go de RAMProcesseur : Intel Core i7-8700K ou AMD Ryzen 5 3600XDisque dur: SSD, min 128 Go...\nCas pratique: utilisÃ© par des fabricants pour donner des produits compatibles avec les besoins d'un client \nContraintes:Finir toutes les maintenance endÃ©ans lâ€™annÃ©ePas dâ€™interruption de services dans le rÃ©seauEmpÃªcher la maintenance simultanÃ©e de deux appareils..."
  },
  {
    "page": 8,
    "text": "Quentin CappartExemples pratiques de CSPs\n8\nhttps://www.dell.com/fr-ca/shop/gaming-and-games/sr/game-desktopsContraintesSolutions faisables\n"
  },
  {
    "page": 9,
    "text": "Quentin CappartProblÃ¨mes combinatoires d'optimisation\n9Objectif: trouver la meilleure solution faisable parmi un ensemble de solutionsProblÃ¨mes combinatoires d'optimisation (constraint optimisation problem - COP)\nTrouver le plus gros lingot d'or dans une botte de foinSituation avec aucune solution: lâ€™objectif est de prouver qu'il nâ€™en existe aucuneSolutions faisables: souvent exprimÃ©es par le biais de contraintesQualitÃ© dâ€™une solution: souvent exprimÃ©e par le biais dâ€™une fonction objectif"
  },
  {
    "page": 10,
    "text": "Quentin CappartExemple pratique de COPs\n10Investissement ï¬nancierObjectif: choisir un ensemble d'investissements maximisant le revenu espÃ©rÃ© Ã©tant donnÃ© un budget limitÃ©InvestissementCoÃ»t ($)Revenu espÃ©rÃ© dans 10 ans ($)A2001Â 000B2001Â 000C2001Â 000D50010Â 000E50010Â 000F80013Â 000G3007Â 000Contrainte: budget maximal de 1000 $AABBAABBS1S1S2S2S3S3S4S4S5S5S6S6S7S7S8S8Objectif: amener des patients Ã  l'hÃ´pital et les ramener aprÃ¨s leur soin (minimiser distance des ambulances) Contraintes: capacitÃ© des vÃ©hicules, respect des horaires des soinsProblÃ¨mes de transport"
  },
  {
    "page": 11,
    "text": "Quentin CappartIntÃ©rÃªt pratique de rÃ©soudre ces problÃ¨mes \n11Exemple: impression dâ€™aï¬ƒchesEconomies:1m2Â deÂ papierÂ spÃ©cialÂ parÂ impressionNombreÂ d'opÃ©rations:60Â parÂ jourEconomiesÂ annuelles:21900m2Â deÂ papierContexte: un imprimeur utilise une grande toile d'un format ï¬xe pour imprimer des commandes de photosObjectif: minimiser la surface non-utilisÃ©e par les photosContraintes: pas de chevauchements entre les aï¬ƒchesImportance en industrieContexte industriel: ce genre de problÃ¨me arrive Ã©normÃ©ment (formalisation trÃ¨s gÃ©nÃ©rale)Raison: beaucoup d'opÃ©rations doivent Ãªtre eï¬€ectuÃ©es un grand nombre de foisIntÃ©rÃªt pratique: eï¬€ectuer eï¬ƒcacement ces opÃ©rations entraine des gains signiï¬catifs sur le long termeOpportunitÃ© pour vous: pouvoir rÃ©soudre eï¬ƒcacement ces problÃ¨mes est une compÃ©tence trÃ¨s demandÃ©e !Exemple: production de composants, transport de biens, stockage, etc.Oï¬€re Ã  Polytechnique: plusieurs cours ne sont dÃ©diÃ©s qu'Ã  la rÃ©solution de ce type de problÃ¨mes\nhttps://www.coursera.org/lecture/automated-reasoning-sat/ general-introduction-and-an-application-to-poster-printing-l7QIO"
  },
  {
    "page": 12,
    "text": "Quentin CappartDiï¬ƒcultÃ© des problÃ¨mes combinatoires\n12Clairement pas rÃ©alisable dâ€™utiliser cette stratÃ©gie dans cette situation !Explication: on ne connaÃ®t pas d'algorithmes Ã  complexitÃ© temporelle polynomiale pour les rÃ©soudre \nhttps://www.youtube.com/ watch?v=AgtOCNCejQ8\nUne Ã©tude des classes de complexitÃ© est en dehors du cadre de notre coursScience Ã©tonnante: vidÃ©o introductive trÃ¨s bien faite Ã  ce sujet\nPrincipe: essayer une solution, Ã©valuer le rÃ©sultat, et rÃ©pÃ©ter jusqu'Ã  ce quâ€™une bonne solution soit trouvÃ©e1047tests1000Ã—109tests/seconde=4.63Ã—1044secondesâ‰ˆ5.35Ã—1039annÃ©es9 possibilitÃ©s par caseâ†’951>1047Â possibilitÃ©s51 cases Ã  complÃ©terDiï¬ƒcultÃ© intrinsÃ¨que: ces problÃ¨mes sont pour la plupart NP-complets ou NP-diï¬ƒcilesConsÃ©quence: le temps dâ€™exÃ©cution devient vite irraisonnable (croissance exponentielle)\nEst-ce que ces problÃ¨mes sont diï¬ƒciles Ã  rÃ©soudre ?\nJâ€™ai accÃ¨s Ã  un super-calculateur, est-ce vraiment irrÃ©aliste de tout tester ?Autre ressource: le cours INF6102 que je donne Ã  lâ€™hiver"
  },
  {
    "page": 13,
    "text": "Quentin CappartLimitations de la recherche exhaustive simple\n13RÃ©solution par recherche exhaustive (bruteforce)Diï¬ƒcultÃ©: la mÃ©thode atteint vite ses limites (consÃ©quence dâ€™une complexitÃ© exponentielle)Exemple: le problÃ¨me de l'investissement implique des dÃ©cisions binaires (sÃ©lection ou non)2 investissements: 4 possibilitÃ©s3 investissements: 8 possibilitÃ©s4 investissements: 16 possibilitÃ©sn investissements: 2^n possibilitÃ©s\nInvestissementCoÃ»t ($)Revenu espÃ©rÃ© dans 10 ans ($)A2001Â 000B2001Â 000C2001Â 000D50010Â 000E50010Â 000F80013Â 000G3007Â 000UtilitÃ©: rÃ©solution envisageable pour les problÃ¨mes ayant un petit espace de recherche (peu de dÃ©cisions) Principe: essayer une solution, Ã©valuer le rÃ©sultat, et rÃ©pÃ©ter jusqu'Ã  ce quâ€™une bonne solution soit trouvÃ©e\nOption 1: ne pas avoir peur d'un algorithme Ã  complexitÃ© exponentielle dans le pire des casOption 2: se contenter d'un algorithme approximatif (non-exhaustif), et s'arrÃªter si une solution est trouvÃ©e"
  },
  {
    "page": 14,
    "text": "Quentin CappartOption 1 - mÃ©thodes de rÃ©solution exhaustive intelligente\n14On a ainsi toujours une recherche exhaustive... mais qui intÃ¨gre une forme d'intelligenceUtilisation d'heuristique: essayer d'abord les solutions les plus prometteusesIntÃ©gration de raisonnements logiques: supprimer les solutions que l'on sait mauvaises Ã  coup sÃ»r\nProgrammation en nombres entiers (MAGI - MTH6404) RÃ©solution SAT (module sur les agents logiques)Programmation par contraintes (module suivant)Principe 1: ne pas avoir peur d'un algorithme Ã  complexitÃ© exponentielle dans le pire des casPrincipe 2: rajouter des mÃ©canismes aï¬n dâ€™accÃ©lÃ©rer le processus de rÃ©solution\nQuels mÃ©canismes peut-on intÃ©grer Ã  une recherche exhaustive ?\nTemps  d'exÃ©cution\nTaille du problÃ¨meRecherche  exhaustive intelligenteRecherche exhaustive (bruteforce)Diï¬ƒcultÃ©: on a toujours une complexitÃ© exponentielleConsÃ©quence: la taille deviendra toujours problÃ©matiqueExemples dâ€™algorithmes de rÃ©solution de ce typeStratÃ©gie de recherche qui sâ€™arrÃªte lorsqu'on a la certitude que la solution trouvÃ©e est optimaleRecherche complÃ¨te (exhaustive)\n"
  },
  {
    "page": 15,
    "text": "Quentin CappartOption 2 - mÃ©thodes de rÃ©solution non exhaustive (recherche incomplÃ¨te)\n15Principe 1: abandonner l'exhaustivitÃ© de la recherche pour amÃ©liorer les performancesPrincipe 2: explorer un sous-ensemble des solutions, et prendre la meilleure trouvÃ©e actuellementSolution 1Solution 2Solution 3Solution 5435Solution 10589......Temps  d'exÃ©cution\nTaille du problÃ¨meRecherche  exhaustive intelligenteRecherche exhaustive (bruteforce)Recherche  IncomplÃ¨teAvantage: processus de rÃ©solution beaucoup plus eï¬ƒcace au niveau du temps d'exÃ©cutionAvantage: fonctionne trÃ¨s bien pour rÃ©soudre de trÃ¨s grands problÃ¨mesInconvÃ©nient: on perd la garantie de trouver la meilleure solution (lâ€™espace non entiÃ¨rement explorÃ©)InconvÃ©nient: impossibilitÃ© de prouver qu'un problÃ¨me est infaisableRecherche locale: mÃ©thode de rÃ©solution incomplÃ¨te par excellence (ce module)MÃ©taheuristiques: amÃ©liorations indispensables Ã  une recherche locale (INF6102)StratÃ©gie de recherche qui arrÃªte son exÃ©cution lorsquâ€™un critÃ¨re d'arrÃªt est atteintRecherche incomplÃ¨te (non exhaustive)\nNote: le critÃ¨re dâ€™arrÃªt est autre quâ€™une exploration complÃ¨te"
  },
  {
    "page": 16,
    "text": "Quentin CappartTable des matiÃ¨res\n16Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 17,
    "text": "Quentin CappartRecherche locale - exemple introductif\n17Exemple: les quatre reines de Westeros\nObjectif: placer les reines sur une grille sans qu'elles ne s'attaquent mutuellementRÃ©solution du problÃ¨me par recherche locale(1) InitialisationChaque reine est placÃ©e alÃ©atoirement sur une colonneSolution initiale, qui est infaisable(2) AmÃ©lioration de la solutionOn bouge la reine ayant le plus de conï¬‚its...... sur la case de sa colonne rÃ©duisant le plus ses conï¬‚itsOn rÃ©pÃ¨te jusqu'Ã  qu'on ne sache plus amÃ©liorer la solutionRÃ©sultat: rÃ©solution en seulement 4 Ã©tapesTaille du problÃ¨meTotal:4Ã—4Ã—4Ã—4=256Â conï¬gurationsÂ possibles"
  },
  {
    "page": 18,
    "text": "Quentin CappartPrincipes de la recherche locale\n18ProcÃ©dure simpliï¬Ã©e de recherche localeEtape 2: on se dÃ©place de solutions en solutions en eï¬€ectuant des mouvements locauxLes mouvements locaux possibles forment un voisinageVoisinage: ensemble des mouvements locaux pouvant Ãªtre fait Ã  partir de notre solution actuelleLa recherche correspond Ã  une exploration dans un graphe ou chaque noeud est une solution candidateIntuition de la procÃ©dureEtape 1: on dÃ©marre d'une solution initiale\nLa recherche locale amÃ©liore la solution en ne considÃ©rant qu'un sous-ensemble d'autres solutionsObjectif: on veut se dÃ©placer au long terme sur la meilleure solution possibleEtape 3: on arrÃªte de se dÃ©placer une fois qu'un critÃ¨re d'arrÃªt est atteint\nNotez bien quâ€™une solution est un Ã©tat, et non une sÃ©quence dâ€™actions (module 1)"
  },
  {
    "page": 19,
    "text": "Quentin CappartRecherche locale - formalisation prÃ©liminaire\n19FormalisationS:l'ensembleÂ desÂ solutionsÂ possiblesN(Sâ†’2S):uneÂ fonctionÂ deÂ voisinagesâˆˆS:uneÂ solutionÂ spÃ©ciï¬queN(s):leÂ voisinageÂ deÂ sf(Sâ†’â„):Â uneÂ fonctionÂ d'Ã©valuationf(s):laÂ valeurÂ deÂ laÂ solutionÂ sProblÃ¨me des quatre reinesReprÃ©sentation d'une solutionS:[âŸ¨1,1,1,1âŸ©,âŸ¨2,1,1,1âŸ©,â€¦,âŸ¨4,4,4,4âŸ©]Vecteur de 4 Ã©lÃ©ments, donnant la ligne de chaque reineEnsemble des solutions: les solutions pouvant Ãªtre gÃ©nÃ©rÃ©es\nf(âŸ¨1,2,1,4âŸ©):10Solution initiale: solution oÃ¹ dÃ©marre la rechercheVoisinage: bouger une seule reine sur sa colonneN(âŸ¨1,2,1,4âŸ©):[âŸ¨2,2,1,4âŸ©,âŸ¨3,2,1,4âŸ©,âŸ¨4,2,1,4âŸ©,âŸ¨1,1,1,4âŸ©,â€¦]Fonction dâ€™Ã©valuation: nombre de conï¬‚its de la solutions:âŸ¨1,2,1,4âŸ©\n1234\n"
  },
  {
    "page": 20,
    "text": "Quentin Cappart\nRecherche locale - Algorithme prÃ©liminaire\n20Question 4: comment choisir un nouvel Ã©tat dans notre voisinage ?Question 2: comment dÃ©ï¬nir un voisinage ? Question 3: comment dÃ©ï¬nir une fonction d'Ã©valuation ?Pseudo-code d'une recherche locale\nQuestions Ã  rÃ©soudreQuestion 1: comment choisir une solution initiale ?ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–ºğ—‡ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—Œğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡sG=[nâˆˆN(s)ğ—Œğ—ğ–¼ğ—ğ—ğ—ğ–ºğ—f(n)<f(s)]ğ—ğ—ğ—‚ğ—…ğ–¾|G|>0:ğ—Œğ–¾ğ—…ğ–¾ğ–¼ğ—ğ–ºğ—‡ğ–¾ğ—ğ—Œğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡sğ–¿ğ—‹ğ—ˆğ—†GG=[nâˆˆN(s)ğ—Œğ—ğ–¼ğ—ğ—ğ—ğ–ºğ—f(n)<f(s)]ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡s\nQuestion 5: est-ce que cet algorithme donne la meilleure solution ?Diï¬€Ã©rents choix existent, et donnent lieu Ã  diï¬€Ã©rents algorithmes\nCes choix dÃ©pendent aussi du problÃ¨me considÃ©rÃ©\n"
  },
  {
    "page": 21,
    "text": "Quentin CappartRecherche locale - formalisation\n21Fonction qui indique les solutions pouvant Ãªtre atteintes Ã  partir d'une solution spÃ©ciï¬queFonction de voisinage\nN(s):Sâ†’2SFonction qui indique quels voisins d'une solution sont valides pour une sÃ©lectionFonction de validitÃ©\nL(N(s),s):2SÃ—Sâ†’2SFonction qui sÃ©lectionne un voisin d'une solution parmi ceux Ã©ligibles dans le voisinageFonction de sÃ©lection\nQ(L(N(s),s),s):2SÃ—Sâ†’SFonction qui donne une valeur sur la qualitÃ© d'une solutionFonction d'Ã©valuation\nf:Sâ†’â„"
  },
  {
    "page": 22,
    "text": "Quentin Cappart\nğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):Recherche locale - Algorithme\n22s=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sConception de ces fonctions pour eï¬€ectuer la rechercheHeuristique de recherche locale\nEntrÃ©es: fonction de voisinage, validitÃ©, sÃ©lection, Ã©valuation, et d'arrÃªtGÃ©nÃ©ration d'une solution initialeOn itÃ¨re jusqu'Ã  ce qu'un critÃ¨re d'arrÃªt soit atteintFonction qui dÃ©ï¬ni quand la recherche doit se terminerCritÃ¨re d'arrÃªt\nÎ˜\nCas standard: une limite sur le nombre d'itÃ©rations ou le temps d'exÃ©cution\nDÃ©ï¬nition du voisinageDÃ©ï¬nition des voisins valides (on ï¬ltre les voisins pour ne garder que ceux valides)SÃ©lection d'un voisinMise Ã  jour de la meilleure solution trouvÃ©e (minimisation)On retient la meilleure solution trouvÃ©e actuellementAlgorithme de recherche locale\nValeur de retour: la meilleure solution trouvÃ©e"
  },
  {
    "page": 23,
    "text": "Quentin CappartRecherche locale - Algorithme\n23\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sProblÃ¨me des quatre reinesFonction de voisinage: bouger une reine sur sa colonne \nN(âŸ¨1,2,1,4âŸ©):[âŸ¨2,2,1,4âŸ©,âŸ¨3,2,1,4âŸ©,âŸ¨4,2,1,4âŸ©,âŸ¨1,1,1,4âŸ©,â€¦]Fonction de validitÃ©: uniquement les voisins rÃ©duisant le nombre de conï¬‚itsL(N(s),s):[nâˆˆN(s)ğ—Œğ—ğ–¼ğ—ğ—ğ—ğ–ºğ—f(n)<f(s)]Fonction de sÃ©lection: prendre le voisin rÃ©duisant le plus les conï¬‚itsQ(L(N(s),s),s):ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡f(s)(L(N(s),s))Fonction dâ€™Ã©valuation: compter le nombre de conï¬‚its d'une solutionf(s):#ğ–¼ğ—ˆğ—‡ğ–¿ğ—…ğ—‚ğ–¼ğ—ğ—Œğ—‚ğ—‡s\nRemarque: il ne sâ€™agit quâ€™un modÃ¨le parmi dâ€™autresQuestion 2: comment dÃ©ï¬nir ces fonctions ?Questions Ã  rÃ©soudreQuestion 1: comment choisir une solution initiale ?Question 3: est-ce que cet algorithme nous donne la meilleure solution ?"
  },
  {
    "page": 24,
    "text": "Quentin CappartAlgorithme du Hill climbing\n24Avantage: facilite la construction dâ€™algorithmes\nEn cas dâ€™Ã©galitÃ©: on choisi un voisin alÃ©atoirement parmi les meilleursFonction de validitÃ©: tous les voisins qui amÃ©liorent la solution actuelleL(N(s),s):[nâˆˆN(s)ğ—Œğ—ğ–¼ğ—ğ—ğ—ğ–ºğ—f(n)<f(s)]Fonction de sÃ©lection: le meilleur parmi tous ces voisinsQ(L(N(s),s),s):kâˆ¼Hğ—ğ—‚ğ—ğ—ğ—‰ğ—‹ğ—ˆğ–»ğ–ºğ–»ğ—‚ğ—…ğ—‚ğ—ğ—’1|H|Les fonctions de voisinage et d'Ã©valuation dÃ©pendent du problÃ¨meH:[nâˆˆL(N(s),s)ğ—Œğ—ğ–¼ğ—ğ—ğ—ğ–ºğ—f(n)=minkâˆˆL(N(s),s)f(k)]IntÃ©rÃªt de notre formalisme\nA t-on d'autres choix pour la fonction de sÃ©lection ?\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sRecherche locale consistant Ã  choisir le meilleur voisin Ã  chaque itÃ©rationAlgorithme du Hill climbing\nCela permet ainsi diï¬€Ã©rentes constructions basÃ©es sur le mÃªme noyauAvantage: dÃ©ï¬ni sans ambiguÃ¯tÃ© le fonctionnement dans la recherche"
  },
  {
    "page": 25,
    "text": "Quentin CappartProblÃ¨me des reines: fonctions de sÃ©lection\n25Fonction de sÃ©lection 1: meilleur voisinFonction de sÃ©lection 2: premier voisin amÃ©liorantAvantage: retourne le meilleur voisinInconvÃ©nient: requiert lâ€™Ã©valuation de tout le voisinage, ce qui peut Ãªtre coÃ»teuxAvantage Ã©vident: ne nÃ©cessite pas dâ€™Ã©valuer systÃ©matiquement tout le voisinageInconvÃ©nient: amÃ¨ne Ã  une sÃ©lection sous optimalePrincipe: sÃ©lectionner le meilleur voisin selon la fonction dâ€™Ã©valuationComplexitÃ©:Â O(n2)Â (demandeÂ deÂ considÃ©rerÂ toutesÂ lesÂ pairesÂ variables/valeurs)\nComplexitÃ©:Â O(n2)Â (demandeÂ deÂ considÃ©rerÂ toutesÂ lesÂ pairesÂ variables/valeursÂ dansÂ leÂ pireÂ desÂ cas)\nnnPrincipe: bouger une reine sur une case de sa colonneTailleÂ duÂ voisinage:Â nÂ reinesÂ donneÂ unÂ voisinageÂ deÂ n2Â voisinsFonction de voisinage considÃ©rÃ©e\nQuelle est la complexitÃ© de cette sÃ©lection ?Principe: sÃ©lectionner le premier voisin qui amÃ©liore la solution\nQ(L(N(s),s),s):ğ–¿ğ—‚ğ—‹ğ—Œğ—kâˆˆL(N(s),s)\nQuelle est la complexitÃ© de cette sÃ©lection ?En pratique: la complexitÃ© moyenne est nettement meilleure, malgrÃ© la complexitÃ© quadratique"
  },
  {
    "page": 26,
    "text": "Quentin CappartProblÃ¨me des reines: fonctions de sÃ©lection\n26Limitation actuelle: on doit considÃ©rer un voisinage entier dans le pire des cas\nAvantage: on garde l'idÃ©e qu'on souhaite trouver un bon voisin qui amÃ©liore, Ã  plus faible coÃ»tFonction de sÃ©lection 3: max/min-conï¬‚ictsAmÃ©lioration possible: faire la sÃ©lection sur un sous-ensemble du voisinageEtape 1: on prend la reine ayant le plus de conï¬‚its (max)Etape 2: on prend la ligne donnant le moins de conï¬‚its (min)ComplexitÃ©:Â ğ’ª(n+n)=ğ’ª(n)Fonction de sÃ©lection 4: min-conï¬‚ictsEtape 1: on prend une reine alÃ©atoireEtape 2: on prend la ligne donnant le moins de conï¬‚its (min)Avantage supplÃ©mentaire: favorise la diversiï¬cation (plus Ã  venir lÃ  dessus)\nnn\nQuelle est la complexitÃ© de cette sÃ©lection ?Etape 1: n opÃ©rations pour trouver le maxEtape 2: n opÃ©rations pour trouver le min\nQuelle est la complexitÃ© de cette sÃ©lection ?Etape 1: nombre constant dâ€™opÃ©rations pour le choix alÃ©atoireEtape 2: n opÃ©rations pour trouver le minComplexitÃ©:Â ğ’ª(1+n)=ğ’ª(n)Remarque: ces quatre fonctions oï¬€rent plusieurs compromis entre qualitÃ© et rapiditÃ© des mouvements"
  },
  {
    "page": 27,
    "text": "Quentin CappartTable des matiÃ¨res\n27Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 28,
    "text": "Quentin CappartProblÃ¨me combinatoire de satisfaction (CSP) - CarrÃ© magique\n28\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sCarrÃ© magique(1) Chaque case doit avoir un nombre diï¬€Ã©rentObjectif: remplir un carrÃ© 3x3 (n x n) avec les nombres de 1 Ã  9 (n*n)Contraintes:(2) La somme de chaque colonne, rangÃ©e, et diagonale doit valoir 15 (T)\nModÃ¨le de recherche localeModÃ©liser une rÃ©solution en recherche locale revient Ã  dÃ©ï¬nir diï¬€Ã©rents Ã©lÃ©ments\nEtape suivante: intÃ©grer ces Ã©lÃ©ments dans un algorithme de rÃ©solutionElement 1: lâ€™ensemble des solutions (espace de recherche)Element 2: la solution initiale (point de dÃ©part de la recherche)Element 3: la fonction dâ€™Ã©valuation (qualitÃ© dâ€™une solution)Element 4: la fonction de voisinage (gÃ©nÃ©ration de nouvelles solutions)Element 5: la fonction de validitÃ© (ï¬ltrage des solutions non voulues)Element 6: la fonction de sÃ©lection (dÃ©placement sur une nouvelle solution)Notre modÃ©lisation va nous permettre dâ€™introduire de nouveaux concepts"
  },
  {
    "page": 29,
    "text": "Quentin CappartCarrÃ© magique - modÃ©lisation par recherche locale\n29\nEnsemble des solutionsDÃ©ï¬nition: toutes les solutions que l'on permet de crÃ©erPlusieurs choix de conception sont possiblesChoix 1: chaque case a un nombre entre 1 et 9\n99=387420489Â solutionsChoix 2: les nombres 1 Ã  9 sont rÃ©partis dans la grilleChoix 2: gÃ©nÃ¨re un espace environ 1000 fois plus petit\"Les petits espaces de recherche,  tu prÃ©fÃ¨reras\"\nQuel choix vous paraÃ®t prÃ©fÃ©rable ?La dÃ©ï¬nition de l'espace de recherche dÃ©pend de la faÃ§on dont les contraintes sont considÃ©rÃ©esContrainte du problÃ¨me restant toujours satisfaite dans chaque solution de l'espaceContrainte dure (hard constraint)\nContrainte du problÃ¨me pouvant Ãªtre non-satisfaite dans une solution de l'espaceContrainte molle (soft constraint)\nChoix 1: les deux contraintes sont mollesChoix 2: la contrainte indiquant que les nombres doivent Ãªtre diï¬€Ã©rents est dure\n9!=362880Â solutions"
  },
  {
    "page": 30,
    "text": "Quentin Cappart\nCarrÃ© magique - solution initiale et voisinage\n30\n692158437\nQue proposeriez-vous comme fonction de voisinage pour ce problÃ¨me ?Solution initialeGÃ©nÃ©ration alÃ©atoire: en crÃ©er une alÃ©atoirement qui satisfait toutes les contraintes duresAstuce: une gÃ©nÃ©ration alÃ©atoire est souvent simple et eï¬ƒcace (bonne diversiï¬cation)\nVoisinage 2-swap: permuter deux chiï¬€res dans le tableauSouhait: avoir un voisinage permettant d'amÃ©liorer la solution, sans Ãªtre trop couteux Ã  calculerErreur frÃ©quente: le voisinage doit aussi toujours prÃ©server nos contraintes dures\n592168437\n591268437\n591263487Notre choix dâ€™espace de recherche: il suï¬ƒt de permuter alÃ©atoirement tous les chiï¬€res dans la grille\nAu plus il y a de contraintes dures, au plus on est limitÃ© dans la dÃ©ï¬nition du voisinageExemple invalide: un voisinage consistant Ã  modiï¬er la valeur d'une case n'est pas valide"
  },
  {
    "page": 31,
    "text": "Quentin CappartCarrÃ© magique - fonction d'Ã©valuation\n31\n692158437\n69215843717141411111717181814181312111716\n69215843717918\n1111221219\nQue proposeriez-vous comme fonction d'Ã©valuationFonction dâ€™Ã©valuation 1: nombre de conï¬‚its\n8 conï¬‚its8 conï¬‚its8 conï¬‚itsInconvÃ©nient: la fonction d'Ã©valuation n'est pas trÃ¨s informativeCause: les solutions du voisinage ne peuvent pas Ãªtre diï¬€Ã©renciÃ©es adÃ©quatementConsÃ©quence: la recherche risque de devenir complÃ¨tement alÃ©atoireObservation: on a souvent des conï¬‚its partoutPrincipe: compter le nombre de contraintes molles qui ne sont pas respectÃ©esDans notre cas, on a un conï¬‚it pour chaque ligne, colonne, et diagonale diï¬€Ã©rente de 15\n692158437\n"
  },
  {
    "page": 32,
    "text": "Quentin CappartCarrÃ© magique - fonction d'Ã©valuation\n32Fonction dâ€™Ã©valuation 2: pondÃ©ration par le degrÃ© de non-satisfaction\n42241123\n6921584371714141414171415\n12111120\nConï¬‚its pondÃ©rÃ©s : 19\n6921584370\n1101\n1101516151414141615Conï¬‚its pondÃ©rÃ©s : 5\n692158437Avantage: cette fonction permet d'orienter plus expressivement la rechercheDans notre cas, chaque conï¬‚it est pondÃ©rÃ© par l'Ã©cart qu'il a avec la valeur recherchÃ©e (15)\n6921584371714141111171718\nConï¬‚its pondÃ©rÃ©s : 9Une solution faisable est trouvÃ©e\"Des fonctions d'Ã©valuation expressives, tu prÃ©fÃ¨reras\"\nPrincipe: pondÃ©rer chaque conï¬‚it par une mesure d'un degrÃ© de non-satisfaction\n692158437"
  },
  {
    "page": 33,
    "text": "Quentin CappartCarrÃ© magique - rÃ©capitulatif\n33ModÃ¨le de recherche localeEspace de recherche: les nombres 1 Ã  9 sont rÃ©partis dans la grilleSolution initiale: les nombres sont rÃ©partis alÃ©atoirement dans la grilleVoisinage: solutions pouvant Ãªtre gÃ©nÃ©rÃ©es par un swap de deux nombresFonction dâ€™Ã©valuation: somme des conï¬‚its pondÃ©rÃ©s de chaque contrainte molleAlgorithme de rÃ©solutionHill climbing: sÃ©lection du meilleur voisin\n692158437\n692158437\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sFonction de sÃ©lection: implicite dans lâ€™algorithme de rÃ©solutionFonction de validitÃ©: implicite dans lâ€™algorithme de rÃ©solution\nExemple dâ€™implÃ©mentation disponible dans les exercices du moduleOn a un modÃ¨le de satisfaction pure (juste trouver une solution faisable) "
  },
  {
    "page": 34,
    "text": "Quentin CappartTable des matiÃ¨res\n34Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 35,
    "text": "Quentin CappartEtat actuel de la situation\n35ModÃ©lisation d'une recherche localeObjectif: concevoir un espace de recherche, une fonction de voisinage, de sÃ©lection, et d'Ã©valuationOn a vu plusieurs techniques et bonnes pratiques de conception pour rendre la recherche eï¬ƒcaceDÃ©ï¬nition de l'espace de recherche: contraintes dures ou mollesFonction de sÃ©lection: trouver un bon compromis entre qualitÃ© de la sÃ©lection et complexitÃ© calculatoireCes notions ont Ã©tÃ© appliquÃ©es concrÃ¨tement sur deux problÃ¨mes (un autre Ã  venir dans le devoir)Algorithme de rÃ©solution\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sObjectif: exÃ©cuter la recherche locale dÃ©ï¬nie prÃ©cÃ©demmentHeuristique de recherche: dÃ©ï¬nition de ce modÃ¨leHill climbing: sÃ©lectionner Ã  chaque Ã©tape le meilleur voisin amÃ©liorantVariantes possibles: sÃ©lectionner un voisin amÃ©liorant\nEst-ce qu'on obtient toujours la meilleure solution ?\nEst-ce qu'on obtient toujours une solution faisable ?"
  },
  {
    "page": 36,
    "text": "Quentin CappartDiï¬ƒcultÃ©s d'un minimum local\n36Algorithme du hill climbingPrincipe: choisir le meilleur voisin Ã  chaque itÃ©rationEn cas dâ€™Ã©galitÃ©: en choisir un alÃ©atoirement parmi les meilleurs\nQuelles sont les limitations de cet algorithme ?ExÃ©cution: la recherche s'arrÃªte une fois que la solution actuelle est meilleure que tous ses voisinsDiï¬ƒcultÃ©: en gÃ©nÃ©ral, on a aucune garantie que cette solution est optimale, voire mÃªme faisableSolution meilleure que tous les voisins, mais qui n'est pas forcÃ©ment la meilleure globalementMinimum local (ou maximum local)\nsÂ estÂ unÂ minimumÂ localâ†”f(s)â‰¤f(n)âˆ€nâˆˆN(s)La meilleure (ou lâ€™une des meilleure) solution de l'espace de recherche (S)Minimum global (ou maximum global)\nsÂ estÂ unÂ minimumÂ globalâ†”f(s)â‰¤f(sâ€² )âˆ€sâ€² âˆˆSRemarque 2: notez bien que ces notions dÃ©pendent de la fonction d'Ã©valuation utilisÃ©eRemarque 1: par dÃ©ï¬nition, un minimum global est Ã©galement un minimum local (mais particulier)"
  },
  {
    "page": 37,
    "text": "Quentin CappartVisualisation d'un minimum local\n37Solution initiale\nMinimum localMinimum globalItÃ©ration de  hill climbing\n"
  },
  {
    "page": 38,
    "text": "Quentin CappartDiï¬ƒcultÃ©s d'un minimum local\n38Visualisation d'une recherche locale\nAVoisinage: eï¬€ectuer un mouvement (G,D,H,B)Fonction d'Ã©valuation: indiquÃ© par le dÃ©gradÃ© de couleur\nConverge t-on vers un minimum global ?Oui: dans le cas de cet exemple prÃ©cis\nQu'est-ce qui cause l'apparition de minima locaux ?Raison 1: le voisinage n'est pas connectÃ©Intuition: le minimum global est parfois impossible Ã  atteindre Ã  partir d'une autre solutionRaison 2: il peut exister plus d'un minimum localEn pratique: c'est rarement le casSolution initiale: coin supÃ©rieur gauche\nRaison dâ€™apparition: Ã©ventuellement la consÃ©quence d'une mauvaise conception du voisinageHill climbing: donne seulement lâ€™assurance dâ€™amener Ã  un minimum localIntuition: il existe parfois (souvent) plusieurs minima locaux et seulement peu dâ€™entre eux sont globauxRaison dâ€™apparition: la fonction d'Ã©valuation est non convexe (cas trÃ¨s frÃ©quent)ConsÃ©quence: un minimum global pourra ainsi Ãªtre manquÃ©"
  },
  {
    "page": 39,
    "text": "Quentin CappartVoisinage connectÃ© et non connectÃ©\n39Un voisinage est connectÃ©, si Ã  partir de n'importe quelle solution de l'espace de recherche,  il est possible d'atteindre un minimum global via la fonction de voisinage Voisinage connectÃ©\nRÃ©ciproquement, un voisinage non connectÃ© est un voisinage qui ne respecte pas cette propriÃ©tÃ©ProblÃ¨me combinatoire de satisfaction: revient Ã  pouvoir obtenir une solution faisableProblÃ¨me combinatoire d'optimisation: revient Ã  pouvoir obtenir la meilleure solution faisableEn supposant que la fonction d'Ã©valuation est cohÃ©rente par rapport Ã  la fonction objectifVoisinage 1: mouvement d'une case (G,D,H,B)Voisinage 2: mouvement d'une ou de deux cases (G,D,H,B)\nEst-ce que ces voisinages sont connectÃ©s ?\nANONOUI\nNotion de connectivitÃ©: propriÃ©tÃ© du voisinage et non du problÃ¨meAction possible: pallier la diï¬ƒcultÃ© d'un voisinage non connectÃ© en un trouvant un autreC'est pourquoi il est important de pouvoir repÃ©rer si un voisinage est connectÃ©"
  },
  {
    "page": 40,
    "text": "Quentin CappartPreuve de connectivitÃ©\n40IntÃ©rÃªt de la connectivitÃ©Il est toujours intÃ©ressant d'avoir un voisinage connectÃ©Avantage: plus grande libertÃ© Ã  la recherche pour trouver de bonnes solutionsAvantage: peut donner des garanties thÃ©oriques de convergence (p.e., simulated annealing) Avantage: un voisinage non connectÃ© peut Ãªtre trÃ¨s diï¬ƒcile Ã  exploiter pour obtenir la solution optimale\nA\nIdÃ©e: construire une recherche amenant au minimum global en utilisant que les mouvements du voisinage\nComment prouver qu'un voisinage est connectÃ© ?\nAstuce: supposer qu'on connaisse le minimum global et utiliser cette information pour diriger la rechercheIntuition: si on peut la construire pour une solution quelconque, alors le voisinage est connectÃ©Diï¬ƒcultÃ©: dÃ©ï¬nir un voisinage connectÃ© nâ€™est pas toujours aisÃ©\nComment construire cette recherche ?La construction de cette recherche est communÃ©ment appelÃ©e preuve de connectivitÃ© Exemples regardons cela pour le problÃ¨me du carrÃ© magique"
  },
  {
    "page": 41,
    "text": "Quentin Cappart\nğ–¬ğ–ºğ—€ğ—‚ğ–¼ğ–²ğ—Šğ—ğ–ºğ—‹ğ–¾ğ–¢ğ—ˆğ—‡ğ—‡ğ–¾ğ–¼ğ—ğ—‚ğ—ğ—‚ğ—ğ—’():Preuve de connectivitÃ©: problÃ¨me du carrÃ© magiqueProblÃ¨me du carrÃ© magiqueVoisinage: permuter deux chiï¬€res (2-swap)Espace de recherche: toutes les permutations de chiï¬€reso:onÂ poseÂ oi,jÂ leÂ chiï¬€reÂ Ã Â laÂ positionÂ (i,j)Â d'uneÂ solutionÂ faisables:onÂ poseÂ si,jÂ leÂ chiï¬€reÂ Ã Â laÂ positionÂ (i,j)Â d'uneÂ solutionÂ initialeÂ quelconqueÂ (permutationÂ deÂ chiï¬€es)\nEst-ce que ce voisinage est connectÃ© ?nÃ—n:tailleÂ deÂ laÂ grille\nğ—Œğ—ğ–ºğ—‰(si,j,oi,j)ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡âŸ¨s1,1â€¦,sn,nâŸ©âŸ¨s1,1â€¦,sn,nâŸ©=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ–¯ğ–¾ğ—‹ğ—†ğ—ğ—ğ–ºğ—ğ—‚ğ—ˆğ—‡()ğ–¿ğ—ˆğ—‹(i,j)âˆˆ1ğ—ğ—ˆn:ğ—‚ğ–¿si,jâ‰ oi,j:On gÃ©nÃ¨re une solution quelconque (permutation alÃ©atoire)Pour chaque case de la grillesi le chiï¬€re de la case n'est pas celui de la solution faisable......On permute avec le bon chiï¬€re (mouvement local autorisÃ© par le voisinage)On retourne ï¬nalement la solution construite, qui est optimale\n276951438o:âŸ¨o1,1,â€¦,o3,3âŸ©\n123456789s:âŸ¨s1,1,â€¦,s3,3âŸ©\nRÃ©sultat: on a construit une recherche amenant Ã  une solution faisable, le voisinage est donc connectÃ© ! \n123456789s1,1s2,3s2,1"
  },
  {
    "page": 42,
    "text": "Quentin CappartTable des matiÃ¨res\n42Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 43,
    "text": "Quentin CappartInsuï¬ƒsance des voisinages connectÃ©s\n43\nEst ce qu'avoir un voisinage connectÃ© est suï¬ƒsant pour garantir une solution faisable ?\n"
  },
  {
    "page": 44,
    "text": "Quentin CappartMinimum local\n44Mauvaise nouvelle: la connectivitÃ© n'est pas une propriÃ©tÃ© suï¬ƒsante pour obtenir un minimum globalDÃ©ï¬ des minima locauxMinimum local: solution meilleure que tous les voisins, mais pas la meilleure globalement\nA\nâˆ€nâˆˆN(s):f(s)â‰¤f(n)\nRisque: lâ€™algorithme de hill climbing risque de tomber misÃ©rablement dans un minimum localVoisinage: mouvement d'une case (G,D,H,B)\nProblÃ©matique aussi prÃ©sente dans d'autres champs de l'intelligence artiï¬cielle (p.e., machine learning)ConsÃ©quence: sâ€™Ã©chapper d'un minimum local est un des dÃ©ï¬s principaux dans une recherche localeBesoin: leur prÃ©sence justiï¬e Ã  elle seule le dÃ©veloppement dâ€™un grand nombre de nouvelles stratÃ©gies\nComment peut-on s'Ã©chapper d'un minimum local ?Cause: apparition lorsquâ€™on a plusieurs minima Ã  notre fonction dâ€™Ã©valuationOpposition avec un minimum global, qui est le meilleur de tous les minima"
  },
  {
    "page": 45,
    "text": "Quentin CappartMinimum local: stratÃ©gies de rÃ©solution\n45StratÃ©gie 1: changer son voisinage (p.e., agrandissement simple)StratÃ©gie 2: commencer Ã  un autre point de dÃ©part (p.e., redÃ©marrages alÃ©atoires)StratÃ©gie 3: accepter de dÃ©grader notre solution actuelle (p.e., simulated annealing)StratÃ©gie 4: accepter de dÃ©grader notre solution actuelle avec une mÃ©moire (p.e., recherche tabou)Les mÃ©ta-heuristiques sont une rÃ©alisation concrÃ¨te de ces stratÃ©giesSolution initiale\nMinimum localMinimum globalItÃ©ration de  hill climbing\nComment peut-on s'Ã©chapper d'un minimum local ?\n"
  },
  {
    "page": 46,
    "text": "Quentin CappartIdÃ©e 1: Augmentation de la taille du voisinage\n46InconvÃ©nient: devient plus coÃ»teux Ã  explorerInconvÃ©nient: pas toujours facile Ã  dÃ©ï¬nirInconvÃ©nient: solution souvent insuï¬ƒsante pour s'Ã©chapper d'un minimum localAvantage: donne la possibilitÃ© de faire de meilleurs mouvementsIdÃ©e 1: augmentation de la taille du voisinagePrincipe: dÃ©ï¬nir la fonction de voisinage de sorte Ã  intÃ©grer un plus grand nombre de voisins\n"
  },
  {
    "page": 47,
    "text": "Quentin CappartCarrÃ© magique - voisinage plus grand\n47L'exploration du voisinage devient vite beaucoup trÃ¨s coÃ»teux (similaire au k-opt)\n692158437\nAvez-vous une idÃ©e d'un voisinage plus grand ?\n5921684372-swap\n692158437\n5921684373-swap\n2-swap:Â complexitÃ©Â temporelleÂ deÂ O(n2),avecÂ nÂ leÂ nombreÂ deÂ cases\nEst-ce que cela nous garantit au moins d'obtenir de meilleures solutions ?3-swap:Â complexitÃ©Â temporelleÂ deÂ O(n3),avecÂ nÂ leÂ nombreÂ deÂ cases4-swap:Â complexitÃ©Â temporelleÂ deÂ O(n4),avecÂ nÂ leÂ nombreÂ deÂ casesk-swap:Â complexitÃ©Â temporelleÂ deÂ O(nk),avecÂ nÂ leÂ nombreÂ deÂ cases\nQuelle est la complexitÃ© d'une sÃ©lection du meilleur voisin dans ce voisinage ?"
  },
  {
    "page": 48,
    "text": "Quentin CappartCarrÃ© magique - voisinage plus grand\n48CarrÃ© magique de taille 4x4Voisinage du 2-swap: 0.04 secondes par itÃ©rationVoisinage du 3-swap: 3.32 secondes par itÃ©ration\nNombre d'itÃ©rationsNombre de conï¬‚its pondÃ©rÃ©s\nCarrÃ© magique de taille 5x5\nVoisinage du 2-swap: 0.11 secondes par itÃ©rationVoisinage du 3-swap: 13.82 secondes par itÃ©rationObservation 3: le temps du 3-swap a considÃ©rablement augmentÃ©\nQu'observe t-on dans ce rÃ©sultat ?Observation 1: le 3-swap rÃ©alise de meilleurs mouvements au dÃ©butObservation 2: il tombe dans un minimum local de moins bonne qualitÃ©On a des observations similairesObservation 4: le temps du 2-swap augmente plus faiblement Fonction de sÃ©lection: le meilleur voisin \nFonction de sÃ©lection: le meilleur voisin \nAinsi, considÃ©rer seulement un voisinage plus grand n'est gÃ©nÃ©ralement pas une amÃ©lioration suï¬ƒsante"
  },
  {
    "page": 49,
    "text": "Quentin CappartRedÃ©marrage Ã  une autre situation initiale\n49InconvÃ©nient: mÃ©canisme pas suï¬ƒsant pour dÃ©couvrir les meilleures solutionsIdÃ©e 2: redÃ©marrer alÃ©atoirement la rechercheEtape 2: une fois le minimum est atteint, relancer une recherche Ã  partir d'une autre solution initialeEtape 1: lancer une procÃ©dure de recherche locale jusqu'Ã  tomber dans un minimum (peut-Ãªtre local)La nouvelle solution initiale est dÃ©terminÃ©e alÃ©atoirement parmi celles de l'espace de recherche\nAvantage: procÃ©dÃ© trÃ¨s facile Ã  mettre en oeuvreAvantage: favorise une exploration diversiï¬Ã©e de l'espaceInconvÃ©nient: les redÃ©marrages augmentent le temps d'exÃ©cution de la recherche\n"
  },
  {
    "page": 50,
    "text": "Quentin CappartCarrÃ© magique - mÃ©thodes des redÃ©marrages\n50CarrÃ© magique de taille 4x4CarrÃ© magique de taille 5x5\n2-swap avec 10 redÃ©marrages: 3.53 secondes au totalCertains des redÃ©marrages ont permis de rÃ©soudre le problÃ¨me !R1R2R3Chaque crÃªte indique qu'un restart est eï¬€ectuÃ©\nNombre d'itÃ©rations2-swap avec 10 redÃ©marrages: 12.58 secondes au totalCette fois ci, aucune solution faisable n'est trouvÃ©e\nAvez-vous une idÃ©e pour amÃ©liorer les performances ?\nLes redÃ©marrages permettent de dÃ©couvrir des nouveaux minima\nAu mieux, il reste 3 conï¬‚itsEn pratique: on sâ€™arrÃªte une fois une solution faisable trouvÃ©e"
  },
  {
    "page": 51,
    "text": "Quentin CappartCarrÃ© magique - mÃ©thodes des redÃ©marrages\n51CarrÃ© magique de taille 5x5CarrÃ© magique de taille 6x6Nombre d'itÃ©rationsMeilleure solution actuellement trouvÃ©e\nSolution faisable trouvÃ©eLa ï¬gure indique la meilleure solution trouvÃ©e actuellementNombre de redÃ©marrages: 100Temps d'exÃ©cution: 122.27 secondesUne solution faisable est trouvÃ©e aprÃ¨s 780 mouvements locaux\nTemps d'exÃ©cution: 300 secondes\nEt avec plus de restarts ou un voisinage plus grand ?Vous pourrez expÃ©rimenter cela par vous mÃªme (via le code donnÃ© dans les exercices du module)La meilleure solution trouvÃ©e a deux conï¬‚itsNombre de redÃ©marrages: 100Aucune solution faisable n'est trouvÃ©e\nDiï¬ƒcultÃ©: la probabilitÃ© qu'une solution alÃ©atoire amÃ¨ne Ã  une solution faisable devient plus petite"
  },
  {
    "page": 52,
    "text": "Quentin Cappart\nğ–¿ğ—ˆğ—‹iâˆˆ1ğ—ğ—ˆÎ“:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—ğ–¶ğ—‚ğ—ğ—ğ–±ğ–¾ğ—Œğ—ğ–ºğ—‹ğ—(N,L,Q,f,Î˜,Î“)s=ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜)ğ—‚ğ–¿f(s)<f(sâ‹†):sâ‹†=ssâ‹†=âŠ¥Recherche locale avec restarts\n52Algorithme de recherche locale avec redÃ©marrage\nParamÃ¨tre Ã  dÃ©ï¬nir: un critÃ¨re d'arrÃªt sur le nombre de redÃ©marrages\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:ğ—‚ğ–¿f(s)<f(sâ‹†):ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–«ğ—ˆğ–¼ğ–ºğ—…ğ–²ğ–¾ğ–ºğ—‹ğ–¼ğ—(N,L,Q,f,Î˜):V=[nâˆˆL(G,s)]sâ‹†=ss=Q(V,s)sâ‹†=sPrincipe: on exÃ©cute simplement plusieurs recherches locales\nBonne pratique: exploitez entiÃ¨rement le temps d'exÃ©cution qui vous est allouÃ©Les redÃ©marrages permettent de s'adapter Ã  n'importe quel temps d'exÃ©cutionSi votre recherche locale prend 1 minute, et que vous en avez 10 Ã  disposition...... redÃ©marrez ! et gardez le meilleur rÃ©sultat trouvÃ©Avantage 1: mÃ©canisme trÃ¨s simple qui s'intÃ¨gre facilement avec les algorithmes de recherche localeAvantage 2: favorise la diversiï¬cation de la recherche"
  },
  {
    "page": 53,
    "text": "Quentin CappartIdÃ©e 3: dÃ©grader notre solution actuelle\n53IdÃ©e 3: autoriser de dÃ©grader notre solutionObjectif: dÃ©grader intelligemment la solution pour s'Ã©chapper d'un minimum localSolution initiale\nMinimum localMinimum globalItÃ©ration de  hill climbing\nSimulated annealing\nGenetic algorithms\nTabu search\nCe module: Ã©tude du simulated annealing (recuit simulÃ©)Lectures complÃ©mentaires (et INF6102): recherche tabou et algorithmes gÃ©nÃ©tiques\n"
  },
  {
    "page": 54,
    "text": "Quentin CappartSimulated annealing - inspiration naturelle\n54\nPrincipes de la mÃ©tallurgieDÃ©but du processusHaute tempÃ©rature: le mÃ©tal est mallÃ©able\nFin processusTout au long du processusPrincipe: diminution progressive de la tempÃ©ratureObjectif: forger une forme avec du mÃ©talMÃ©tal froid: non mallÃ©able et prend une forme dÃ©ï¬nitiveMÃ©tal chaud: mallÃ©able et peut Ãªtre modelÃ©Donne au forgeron la possibilitÃ© de modiï¬er la forme du mÃ©tal, selon le produit ï¬nal vouluDonne la possibilitÃ© de maÃ®triser la solidiï¬cation du mÃ©talFaible tempÃ©rature: le mÃ©tal est solide et non mallÃ©ableLe mÃ©tal prend sa forme ï¬nale\nUne rÃ©duction brusque risque de donner un rÃ©sultat non stable\nSimulated annealing: From basics to applications [Delahaye et al., 2019] "
  },
  {
    "page": 55,
    "text": "Quentin CappartMÃ©taheuristique du simulated annealing (recuit simulÃ©)\n55Fonctionnement gÃ©nÃ©ralRÃ¨gle de sÃ©lection: on tire alÃ©atoirement un voisinCas 1: s'il amÃ©liore la solution courante, on l'accepteCas 2: sinon, on l'accepte avec une certaine probabilitÃ© pÎ”(n,s)=f(n)âˆ’f(s)InterprÃ©tation: cette valeur donne une mesure d'Ã  quel point un voisin est pire que la solution actuelleProbabilitÃ© de sÃ©lection\nIntuition de la sÃ©lection: plus le mouvement est mauvais, plus la probabilitÃ© de sÃ©lection sera faible(1) On considÃ¨re la diï¬€Ã©rence de coÃ»t du voisin n, par rapport Ã  notre solution actuelle sÎ”(n,s)>0Â etÂ Ã©levÃ©:voisinÂ quiÂ dÃ©gradeÂ fortÂ laÂ solutionÎ”(n,s)>0Â etÂ faible:voisinÂ quiÂ dÃ©gradeÂ faiblementÂ laÂ solutionÎ”(n,s)â‰¤0â†’f(n)â‰¤f(s):voisinÂ quiÂ amÃ©lioreÂ ouÂ maintientÂ laÂ solutionÂ (pourÂ uneÂ minimisation)\nNote: ce schÃ©ma probabiliste est Ã©galement connu sous le nom d'algorithme de Metropolis-HastingsPrincipe: permettre la sÃ©lection de voisins moins bon que la solution actuelle\nIdÃ©e gÃ©nÃ©rale: la probabilitÃ© dÃ©pend de la qualitÃ© du voisin, et du nombre d'itÃ©rations dÃ©jÃ  eï¬€ectuÃ©es\n(2) Au plus la recherche avance, au plus on va Ã©viter de dÃ©grader la solutionInterprÃ©tation: on veut faire converger la recherche "
  },
  {
    "page": 56,
    "text": "Quentin CappartSimulated annealing - Principe de sÃ©lection\n56Fonction de sÃ©lectionPrincipe 1: on accepte toujours un voisin qui amÃ©liore la solution actuellePrincipe 2: on accepte un voisin dÃ©gradant avec une certaine probabilitÃ©, qui dÃ©pend de la qualitÃ© du voisinPrincipe 3: cette probabilitÃ© dÃ©croit avec le temps et dÃ©pend d'un paramÃ¨treSÃ©lection en cas de non-dÃ©gradationÎ”(n,s)=f(n)âˆ’f(s)Â (diï¬€Ã©renceÂ duÂ coÃ»tÂ d'unÂ voisinÂ avecÂ laÂ solutionÂ actuelle)SituationÂ deÂ non-dÃ©gradation:Â Î”(n,s)â‰¤0Un voisin amÃ©liorant est toujours choisiSÃ©lection en cas dÃ©gradationSituationÂ deÂ dÃ©gradation:Â Î”(n,s)>0Intuition 1: la probabilitÃ© est modulÃ©e pour Ãªtre plus propice Ã  accepter des voisins faiblement dÃ©gradant\nP(n,s,t)=eâˆ’Î”(n,s)tÂ (probabilitÃ©Â deÂ sÃ©lection)\nÎ”(n,s)P(n,s,t)\nÎ”(n,s)Â Ã©levÃ©:onÂ aÂ uneÂ faibleÂ probabilitÃ©Â deÂ sÃ©lectionÎ”(n,s)Â faible:onÂ aÂ uneÂ hauteÂ probabilitÃ©Â deÂ sÃ©lectionIntuition 2: La probabilitÃ© est paramÃ¨trÃ©e par une valeur t, qui est appelÃ©e la tempÃ©rature de l'algorithmeFaible dÃ©gradation: trÃ¨s haute probabilitÃ© de sÃ©lectionHaute dÃ©gradation:  probabilitÃ© de sÃ©lection quasi nulle"
  },
  {
    "page": 57,
    "text": "Quentin CappartSimulated annealing - tempÃ©rature de l'algorithme\n57Haute tempÃ©rature: haute probabilitÃ© d'accepter un mauvais voisinFaible tempÃ©rature: faible probabilitÃ© d'accepter un mauvais voisin\nProcÃ©dure gÃ©nÃ©rale: diminuer la tempÃ©rature au ï¬l de la recherche\nÎ”(n,s)P(n,s,t)\nSchÃ©maÂ deÂ dÃ©croissance:tk+1=Î±tkÂ (typiquement,Â Î±Â estÂ entreÂ 0.8Â etÂ 0.99)TempÃ©ratureÂ initiale:t0Â (valeurÂ assezÂ Ã©levÃ©eÂ pourÂ autoriserÂ toutesÂ lesÂ sÃ©lections)Â En pratique, la constante multiplicative de dÃ©croissance est un paramÃ¨tre Ã  calibrer par l'utilisateur\nQuel est l'impact de la tempÃ©rature sur la sÃ©lection ?\nQuand prÃ©fÃ¨re t-on avoir une faible ou haute tempÃ©rature ?DÃ©but de la recherche: on veut favoriser l'exploration de l'espace aï¬n de dÃ©couvrir plein de solutionsFin de la recherche: on veut favoriser l'exploitation d'une solution aï¬n d'obtenir le meilleur coÃ»t possiblePrincipe: rÃ©duire la tempÃ©rature selon une suite gÃ©omÃ©triqueSchÃ©ma de dÃ©croissance gÃ©omÃ©triqueCritÃ¨reÂ d'arrÃªt:lorsqu'onÂ n'observeÂ plusÂ aucuneÂ amÃ©liorationÂ dansÂ laÂ recherche\nP(n,s,t)=eâˆ’Î”(n,s)tÂ (probabilitÃ©Â deÂ sÃ©lection)"
  },
  {
    "page": 58,
    "text": "Quentin CappartSimulated annealing - algorithme\n58\ns=ğ—€ğ–¾ğ—‡ğ–¾ğ—‹ğ–ºğ—ğ–¾ğ–¨ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ–²ğ—ˆğ—…ğ—ğ—ğ—‚ğ—ˆğ—‡()G=[nâˆˆN(s)]ğ–¿ğ—ˆğ—‹kâˆˆ1ğ—ğ—ˆÎ˜:\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–²ğ—‚ğ—†ğ—ğ—…ğ–ºğ—ğ–¾ğ–½ğ– ğ—‡ğ—‡ğ–¾ğ–ºğ—…ğ—‚ğ—‡ğ—€(N,L,Q,f,Î˜,t0,Î±):V=[nâˆˆL(G,s)]sâ‹†=scâˆ¼Vğ—ğ—‚ğ—ğ—ğ—ğ—‡ğ—‚ğ–¿ğ—ˆğ—‹ğ—†ğ—‰ğ—‹ğ—ˆğ–»ğ–ºğ–»ğ—‚ğ—…ğ—‚ğ—ğ—’ğ—‚ğ–¿Î”â‰¤0:s=cÎ”=f(c)âˆ’f(s)s=cğ–¾ğ—…ğ—‚ğ–¿Î”>0âˆ§ğ—Œğ—ğ–¼ğ–¼ğ–¾ğ—Œğ—Œğ—ğ—‚ğ—ğ—ğ—‰ğ—‹ğ—ˆğ–»ğ–ºğ–»ğ—‚ğ—…ğ—‚ğ—ğ—’eâˆ’Î”t:t=t0\nt=Î±tğ—‚ğ–¿f(s)<f(sâ‹†):sâ‹†=sLa fonction de validitÃ© doit permettre des voisins dÃ©gradantUn voisin non-dÃ©gradant est toujours prisUn voisin qui dÃ©grade est choisi selon  la probabilitÃ© de sÃ©lectionMise Ã  jour de la tempÃ©ratureVariante 1: schÃ©ma de dÃ©croissance polynomialeEntrÃ©e: critÃ¨re d'arrÃªt, tempÃ©rature initiale, et taux de dÃ©croissanceOn commence avec la recherche avec la tempÃ©rature initiale\nVariante 2: intÃ©gration d'un mÃ©canisme de redÃ©marrageVariante 3: augmentation de la tempÃ©rature quand la recherche ne progresse plusRÃ©sultat thÃ©orique: convergence vers l'optimum si les paramÃ¨tres sont bien calibrÃ©sEn pratique: cette convergence est plus lente qu'une recherche exhaustive\nOn choisi alÃ©atoirement un voisin"
  },
  {
    "page": 59,
    "text": "Quentin CappartZoo des mÃ©taheuristiques\n59Zoo des mÃ©taheuristiques\nhttps://en.wikipedia.org/wiki/Table_of_metaheuristicsIl existe un nombre impressionnant de mÃ©thodes autres que le simulated annealingChacune ont leurs forces, faiblesses, et champs d'application\nTendance: la pertinence de nombreuses mÃ©taheuristiques a Ã©tÃ© justiï¬Ã©e par leur inspiration naturelleJellyï¬sh SearchCuckoo searchAfrican Bufallo optimizationEmperor Penguins Colony\nIterated local searchTabu searchGenetic algorithmsAnt colony optimization\nAller plus loin: MÃ©taheuristiques appliquÃ©es au gÃ©nie informatique (INF6102 - cours que je donne)Ces mÃ©thodes sont communÃ©ment appelÃ©es mÃ©taheuristiques\nL'inspiration naturelle des mÃ©ta-heuristiques est souvent un argument marketing et non scientiï¬queExemples: quelques unes parmi les plus connues (et eï¬ƒcaces)"
  },
  {
    "page": 60,
    "text": "Quentin CappartTable des matiÃ¨res\n60Recherche locale1. ProblÃ¨mes combinatoires de satisfaction et d'optimisation 2. Concepts et principes fondamentaux de la recherche locale 3. Formalisation de la recherche locale 4. Algorithme du hill climbing 5. Diï¬ƒcultÃ© des minima locaux 6. Notion de voisinage connectÃ© 7. MÃ©thodes des redÃ©marrages (restarts) 8. Algorithme du recuit simulÃ© (simulated annealing)\n"
  },
  {
    "page": 61,
    "text": "Quentin CappartSynthÃ¨se des notions vues\n61ProblÃ¨mes combinatoiresSatisfaction (CSP): trouver une solution satisfaisant un ensemble de contraintesOptimisation (COP): CSP oÃ¹ on souhaite Ã©galement optimiser une fonction objectif\nUne solution est un Ã©tat, qui nous est inconnu, et non une sÃ©quence d'action\nInvestissementCoÃ»t ($)Revenu espÃ©rÃ© dans 10 ans ($)A2001Â 000B2001Â 000C2001Â 000D50010Â 000E50010Â 000F80013Â 000G3007Â 000Budget maximal de 1000 $\nRecherche localePrincipe: rÃ©solution en se dÃ©plaÃ§ant de solutions en solutions via des mouvements locauxHill climbing: sÃ©lection systÃ©matique du meilleur voisinRisque: Ãªtre bloquÃ© dans un minimum local\nMÃ©canismes d'amÃ©liorationIdÃ©e 1: utiliser un voisinage connectÃ©IdÃ©e 3: redÃ©marrer alÃ©atoirement la rechercheIdÃ©e 4: accepter de dÃ©grader sa solution (p.e., simulated annealing)IdÃ©e 2: agrandir le voisinage"
  },
  {
    "page": 62,
    "text": "Quentin CappartExemples de questions d'examen\n621. Savoir appliquer un algorithme de recherche locale  2. Proposer une rÃ©solution basÃ©e sur la recherche locale pour rÃ©soudre un problÃ¨me (solution initiale, fonction de voisinage, fonction de sÃ©lection, etc.) 3. Peser le pour et le contre entre deux fonctions de voisinagesThÃ©oriePratique1. Expliquer le fonctionnement d'un algorithme vu 2. Donner les avantages/inconvÃ©nients d'un voisinage large au lieu de restreint 3. Expliquer les principes gÃ©nÃ©raux de la recherche locale\n"
  },
  {
    "page": 63,
    "text": " \nQuentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesRecherche locale: FIN\nDALLE: A queen climbing a mountain in an impressionist style"
  }
]