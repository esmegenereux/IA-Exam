[
  {
    "page": 1,
    "text": " \nQuentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 9: Apprentissage par renforcement"
  },
  {
    "page": 2,
    "text": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques \n"
  },
  {
    "page": 3,
    "text": "Quentin CappartTable des matiÃ¨res\n3Apprentissage par renforcement1. DÃ©ï¬nition de l'apprentissage par renforcement 2. Illustration sur diï¬€Ã©rents problÃ¨mes 3. DÃ©ï¬nition et formalisation d'un processus de dÃ©cisions sÃ©quentielles 4. Formalisation d'un agent d'apprentissage par renforcement 5. Diï¬ƒcultÃ©s en apprentissage par renforcementCe module ne fait pas partie de la matiÃ¨re d'examen\n"
  },
  {
    "page": 4,
    "text": "Quentin CappartCaractÃ©ristiques de l'apprentissage par renforcement\n4Apprentissage supervisÃ©L'apprentissage se faisait Ã  l'aide de donnÃ©es labellisÃ©esD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}L'objectif est d'arriver Ã  apprendre sur base de donnÃ©es dont on connait la vraie valeurApprentissage non-supervisÃ©Apprentissage qui se fait uniquement sur base de donnÃ©es non-labellisÃ©esD:{x(1),x(2),â€¦,x(m)}L'objectif est d'utiliser la similaritÃ© entre les donnÃ©es pour orienter l'apprentissageApprentissage par renforcementL'apprentissage de l'agent se fait sur base d'un processus d'essais-erreursRequiert un moyen d'Ã©valuer la qualitÃ© des essais que l'agent eï¬€ectueL'agent va collecter de nouvelles donnÃ©es au fur et Ã  mesure de ses essaisL'agent agit dans un environnement donnÃ©Situations oÃ¹ un agent doit trouver la meilleure sÃ©quence de dÃ©cisions pour remplir un objectifChaque action faite est susceptible de modiï¬er l'Ã©tat de l'environnement"
  },
  {
    "page": 5,
    "text": "Quentin CappartApplication en robotique\n5\nhttps://www.youtube.com/watch?v=n2gE7n11h1YLearning to Walk via Deep Reinforcement Learning. Tuomas Haarnoja et al. Robotics: Science and Systems (RSS). 2019."
  },
  {
    "page": 6,
    "text": "Quentin CappartAlphaGo: Match contre Lee Sedol\n6\nhttps://www.youtube.com/watch?v=WXuK6gekU1YNotez la surprise gÃ©nÃ©rale lors du mouvement d'AlphaGo"
  },
  {
    "page": 7,
    "text": "Quentin CappartIllustration d'un problÃ¨me de dÃ©cisions sÃ©quentielles\n7\nPacman (PremiÃ¨re version du jeu date de 1980)ObjectifCollecter le plus de points avant de se faire mangerToutes les informations concernant le plateauPosition du Pacman, fantÃ´mes, bonus, etc.Etat (state)Tout ce qui est pertinent pour la prise de dÃ©cisionActionDÃ©cisions que le joueur peut faireRÃ©compense (reward)Une action va changer l'Ã©tat actuelUne action va engendrer un signal de rÃ©compenseSignal utilisÃ© pour orienter la prise de dÃ©cisionValeur nÃ©gative pour une mauvaise actionMouvement Ã  gauche, droite, haut, bas ou rien faire- 100 si on se dÃ©place sur un fantÃ´meValeur positive pour une bonne action+ 10 si on prend un bonusProcessus de dÃ©cisions sÃ©quentiellesEï¬€ectuer la meilleure sÃ©quence de dÃ©cisions aï¬n de rÃ©aliser un objectifExemple"
  },
  {
    "page": 8,
    "text": "Quentin CappartVue globale d'un processus de dÃ©cisions sÃ©quentielles\n8AgentEnvironmentActionStateRewardAgentL'intelligence prenant les dÃ©cisions (ou eï¬€ectuant les actions) aï¬n d'accomplir son objectifEnvironnementDÃ©ï¬nit la dynamique du monde (Ã©tats du systÃ¨me, les actions possibles, l'impact des actions, etc.)Englobe toutes les informations utiles et pouvant Ãªtre disponibles pour la prise de dÃ©cisionsLe monde dans lequel l'agent va interagir\nL'agent et l'environnement sont en continuelle interactionL'agent modiï¬e l'environnement par ses actionsL'environnement prÃ©sente des situations diï¬€Ã©rentes Ã  l'agent"
  },
  {
    "page": 9,
    "text": "Quentin CappartRÃ©compense dans un processus de dÃ©cisions sÃ©quentielles\n9InterprÃ©tation de la rÃ©compenseN'importe quel objectif Ã  atteindre peut Ãªtre dÃ©crit comme une maximisation des rÃ©compenses cumulÃ©es espÃ©rÃ©es de toutes les actions eï¬€ectuÃ©es.HypothÃ¨se de la rÃ©compense\nPeut sembler restrictif, mais est assez ï¬‚exible et applicable dans beaucoup de situationsSignal numÃ©rique qui indique Ã  quel point l'agent a bien agi pour un certain instant Element fondamental qui est utilisÃ© pour formaliser l'objectif Ã  atteindre\nHypothÃ¨se fondamentale en apprentissage par renforcement\nQue pensez vous de cette hypothÃ¨se ?L'objectif correspond ainsi Ã  maximiser le total des rÃ©compenses cumulÃ©es espÃ©rÃ©es\nCependant, il peut Ãªtre diï¬ƒcile de dÃ©ï¬nir certaines tÃ¢ches en terme de rÃ©compenses...On parle de rÃ©compense espÃ©rÃ©e, car certaines actions peuvent donner diï¬€Ã©rentes rÃ©compensesMaximisation des rÃ©compenses\nNote"
  },
  {
    "page": 10,
    "text": "Quentin CappartExemples de rÃ©compenses\n10\nJeu du casse-briquesLe score du jeu (fonction du nombre de briques cassÃ©es)\nPousse l'agent Ã  maximiser son score\nJeu d'Ã©chec+1 en cas de victoire0 en cas d'Ã©galitÃ©-1 en cas de dÃ©faite\nQue pensez vous de l'idÃ©e d'intÃ©grer des connaissances du jeu (heuristiques) dans la fonction de rÃ©compense ?\nPortefeuille d'investissement+x pour chaque dollar gagnÃ© par vos placements-x pour chaque dollar perdu par vos placements"
  },
  {
    "page": 11,
    "text": "Quentin CappartExemples de rÃ©compenses (mauvaise conception)\n11\nCoastRunner 7\nhttps://openai.com/blog/faulty-reward-functions/Jeu de course avec des bateauxBesoin de passer par diï¬€Ã©rents checkpointsPlusieurs bonus peuvent Ãªtre collectÃ©s (turbo, etc.)Signal de rÃ©compenseRÃ©compense positive si on passe par un checkpointRÃ©compense positive si on prend un bonus \nQue pensez-vous de cette conception ?L'agent prÃ©fÃ¨re collecter les bonus plutÃ´t que ï¬nir la course\nOn rÃ©compense des sous-objectifs basÃ©s sur notre connaissance du jeu, et non l'objectif ï¬nal\nSnakeCollecter le maximum de pointsSignal de rÃ©compense+1 si on collecte un fruit\nQue pensez-vous de cette conception ?-1 pour chaque dÃ©placement (pour aller sur le fruit au plus vite)L'agent va Ãªtre pousser Ã  se suicider pour limiter les pertes"
  },
  {
    "page": 12,
    "text": "Quentin CappartFormalisation d'une fonction objectif\n12Maximiser le total des rÃ©compenses cumulÃ©esObjectif\nQue pensez-vous de prendre, Ã  chaque Ã©tape, l'action  qui donne le plus de rÃ©compenses dans l'immÃ©diat ?Les actions faites peuvent avoir un impact Ã  long terme ExemplesUn investissement engendre des frais immÃ©diats, mais peut prendre des annÃ©es pour Ãªtre proï¬tableEï¬€ectuer une maintenance a un coÃ»t, mais peut empÃªcher un dysfonctionnement dans les prochains joursLes meilleures rÃ©compenses peuvent arriver trÃ¨s tard, et Ãªtre conditionnelles Ã  des actions eï¬€ectuÃ©s tÃ´tMaximiser chaque rÃ©compense immÃ©diate ne revient pas Ã  maximiser la rÃ©compense globaleSouvent, il est prÃ©fÃ©rable de sacriï¬er une rÃ©compense immÃ©diate, pour une meilleure au long termeRÃ©compense Ã  long terme\nAvez-vous des exemples concrets ?"
  },
  {
    "page": 13,
    "text": "Quentin CappartProcessus de dÃ©cision de Markov (MDP)\n13AgentEnvironmentActionStateRewardEnsembles essentielst=1,2,3,â€¦:instantsÂ temporelsÂ (timeÂ step)St:l'Ã©tatÂ deÂ l'environnementÂ auÂ tempsÂ tAt:l'actionÂ priseÂ parÂ l'agentÂ auÂ tempsÂ tp:SÃ—AÃ—Sâ†’[0,1]:laÂ fonctionÂ deÂ probabilitÃ©Â d'allerÂ d'unÂ Ã©tatÂ Ã Â unÂ autreÂ enÂ faisantÂ uneÂ certaineÂ actionA:l'ensembleÂ deÂ toutesÂ lesÂ actionsÂ possiblesS:l'ensembleÂ deÂ tousÂ lesÂ Ã©tatsÂ possibles\nr:SÃ—Aâ†’â„:laÂ fonctionÂ deÂ rÃ©compense,Â donnantÂ uneÂ valeurÂ Ã Â l'actionÂ faiteÂ Ã Â partirÂ d'unÂ certainÂ Ã©tatFormalisation d'un MDPHypothÃ¨se fondamentale d'un MDPChaque Ã©tat est markovien\n"
  },
  {
    "page": 14,
    "text": "Quentin CappartEtat markovien\n14â„™(St+1|St)=â„™(St+1|S1,â€¦,St)Intuitivement, l'Ã©tat actuel contient toutes les informations utiles du passÃ©Etat Markovien\nConnaissant uniquement la position de la voiture,  peut-on prÃ©dire la position ?Exemple: voiture en mouvementEtat amÃ©liorÃ©: position, vitesse, accÃ©lÃ©ration, etc.Un Ã©tat est markovien s'il contient toutes les informations utiles du passÃ©La probabilitÃ© d'aller Ã  un autre Ã©tat ne dÃ©pend que de notre Ã©tat prÃ©sent\nAndrey Markov  (1856-1922)Une fois que l'on a notre Ã©tat actuel, on peut jeter l'historique\nQue faire en cas d'Ã©tats non-markoviens ?Augmenter l'Ã©tat avec de nouvelles informations Etat incomplet: positionHypothÃ¨se rÃ©aliste, et simpliï¬ant Ã©normÃ©ment les algorithmes de dÃ©cisions"
  },
  {
    "page": 15,
    "text": "Quentin CappartVariantes des MDPs\n15Processus de dÃ©cision de Markov stochastique\nProcessus de dÃ©cision de Markov partiellement observable (POMDP)Il en va de mÃªme pour la rÃ©compenseChaque action comporte de l'alÃ©atoire, et peut vous amener Ã  diï¬€Ã©rents Ã©tats\nL'agent n'a qu'une vision incomplÃ¨te de l'environnementVariante gÃ©nÃ©ralement considÃ©rÃ©e dans la formalisation standardL'agent doit construire sa propre reprÃ©sentation de son Ã©tat\nC'est pourquoi on parle de maximisation de la rÃ©compense cumulÃ©e espÃ©rÃ©e\nL'agent ne reÃ§oit qu'une observation de l'Ã©tat de l'environnement"
  },
  {
    "page": 16,
    "text": "Quentin Cappart\nComposantes d'un agent d'apprentissage par renforcement\n16ModÃ¨le de l'environnement (model)\nPolitique de sÃ©lection d'actions (policy)Fonction de valeur (value function)La reprÃ©sentation que l'agent se fait de l'environnementLa reprÃ©sentation que l'agent se fait de la qualitÃ© de chaque Ã©tat et de chaque action \nLa dÃ©cision de l'action Ã  prendre Ã©tant donnÃ© notre Ã©tat\nPlacement  Ã  cet endroit++=\nhttps://cf.shopee.ph/ï¬le/ 673d6d6c4e658a40ea481efda94820e4_tn\nEst-ce qu'un Ãªtre humain peut Ãªtre modÃ©lisÃ© par ces trois composantes ?Chaque agent de RL est basÃ© sur une ou plusieurs de ces composantes\n+Placement en  haut/droite=+Placement en  haut/droite=..."
  },
  {
    "page": 17,
    "text": "Quentin CappartModÃ¨le de l'environnement (model)\n17IntuitionReprÃ©sentation qu'a l'agent sur le fonctionnement de l'environnementp(sâ€² |s,a)=â„™[St=sâ€² |Stâˆ’1=s,Atâˆ’1=a]ProbabilitÃ© d'aller sur un Ã©tat suivant Ã  partir d'une action faite Ã  l'Ã©tat actuel\nr(s,a)=ğ”¼[Rt|St=s,At=a]Fonction de transitionp:SÃ—SÃ—Aâ†’[0,1]Fonction de rÃ©compenseEspÃ©rance de la rÃ©compense immÃ©diate si une action est faite Ã  partir de l'Ã©tat actuelr:SÃ—Aâ†’â„Notez que cette formalisation tient compte d'un processus de dÃ©cisions stochastiquesCommentairesReprend les informations sur l'eï¬€et des actions (modiï¬cation de l'Ã©tat, et rÃ©compenses collectÃ©es) \nEn gÃ©nÃ©ral, l'agent ne sait pas comment fonctionne l'environnement et doit le dÃ©couvrir par essais/erreurs"
  },
  {
    "page": 18,
    "text": "Quentin CappartPolitique de sÃ©lection d'action (policy)\n18DÃ©ï¬ni le comportement de l'agentEtant donnÃ© notre Ã©tat actuel, quelle action doit Ãªtre prisea=Ï€(s)Ï€(a|s):â„™[At=a|St=s]Ï€:Sâ†’AIntuitionPolitique dÃ©terministeEtant donnÃ© mon Ã©tat, mon action sera toujours la mÃªmePolitique stochastiqueEtant donnÃ© mon Ã©tat, le choix de mon action est sujet Ã  de l'alÃ©atoire contrÃ´lÃ© \nEn quelles situations cela peut-il Ãªtre bÃ©nÃ©ï¬que pour l'agent ?Notez la diï¬€Ã©rence avec un environnement stochastiqueIci, c'est notre agent qui a un comportement stochastiqueParfois, la stratÃ©gie optimale implique d'agir de maniÃ¨re alÃ©atoire\n"
  },
  {
    "page": 19,
    "text": "Quentin CappartFonction de valeur (value function)\n19Fonction de valeur d'un Ã©tat (state-value function)ReprÃ©sentation interne que l'agent se fait de la qualitÃ© de chaque Ã©tatSomme espÃ©rÃ©e des rÃ©compenses obtenues Ã  partir d'un Ã©tat, si on suit une certaine politique de sÃ©lection\nVÏ€(s)=ğ”¼[Rt+Î³Rt+1+Î³2Rt+2+Î³3Rt+3+â€¦|St=s]Â (formalisationÂ gÃ©nÃ©rale)Discounting factorValeur comprise entre 0 et 1Î³âˆˆ[0,1]Permet de considÃ©rer le fait qu'il est prÃ©fÃ©rable d'avoir une rÃ©compense tÃ´t plutÃ´t que tardValeur de 0: l'agent ne va considÃ©rer que la rÃ©compense immÃ©diateValeur de 1: l'agent va considÃ©rer toutes les rÃ©compenses avec la mÃªme importanceVÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]\nLa valeur des rÃ©compenses futures dÃ©croit avec le tempsVÏ€(s)=ğ”¼[Rt|St=s]VÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]Fonction de valeur d'une action (action-value function)QÏ€(s,a)=ğ”¼[Rt+Î³Rt+1+Î³2Rt+2+Î³3Rt+3+â€¦|St=s,At=a]Â (formalisationÂ gÃ©nÃ©rale)On peut Ã©galement construire une fonction de la qualitÃ© d'une action Ã  partir d'un certain Ã©tat"
  },
  {
    "page": 20,
    "text": "Quentin CappartExemple: l'environnement du labyrinthe\n20Environnement du labyrintheS'Ã©chapper du labyrinthe en eï¬€ectuant le moins de dÃ©placements\nADActions: dÃ©placement d'une casep(sâ€² |s,L)=1Â ifÂ sâ€² Â isÂ theÂ leftÂ (L)Â cellÂ ofÂ s\nr(s,a)=âˆ’1âˆ€sâˆˆS,âˆ€aâˆˆAp(sâ€² |s,L)=0Â otherwisep(sâ€² |s,T)=1Â ifÂ sâ€² Â isÂ theÂ topÂ (T)Â cellÂ ofÂ sp(sâ€² |s,T)=0Â otherwisep(sâ€² |s,R)=1Â ifÂ sâ€² Â isÂ theÂ rightÂ (R)Â cellÂ ofÂ sp(sâ€² |s,R)=0Â otherwisep(sâ€² |s,B)=1Â ifÂ sâ€² Â isÂ theÂ bottomÂ (B)Â cellÂ ofÂ sp(sâ€² |s,B)=0Â otherwiseFonctions de probabilitÃ© (ou de transition)\nFonction de rÃ©compenseEtats: la case oÃ¹ l'agent se trouveA={left,Â right,Â top,Â bottom}S=ensembleÂ desÂ casesÂ duÂ labyrinthe"
  },
  {
    "page": 21,
    "text": "Quentin CappartExemple d'un agent d'apprentissage par renforcement\n21Fonction de valeur d'un Ã©tat (state-value function)\nAD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1VÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]Î³=1Â (sansÂ discount)Politique de sÃ©lection ADPolitique dÃ©terministeValeur de -1 par cases pour atteindre l'arrivÃ©e depuis une certaine casePour chaque Ã©tat, quelle action doit-Ãªtre eï¬€ectuÃ©e ?a=Ï€(s)Â (dÃ©ï¬niÂ parÂ lesÂ ï¬‚Ã¨ches)ModÃ¨leL'agent a un accÃ¨s au modÃ¨le complet de l'environnementIl connait parfaitement ls fonctions de transition et de rÃ©compense\nQuelle est la qualitÃ© de chaque Ã©tat ?CommentairesNotez que l'agent considÃ©rÃ© est dÃ©jÃ  entraÃ®nÃ©On n'a pas encore abordÃ© la maniÃ¨re de faire l'apprentissage"
  },
  {
    "page": 22,
    "text": "Quentin CappartCatÃ©gorisation d'agents : avec ou sans modÃ¨le\n22Agent utilisant un modÃ¨le (model-based agent)Contient dans sa reprÃ©sentation un modÃ¨le complet de comment l'environnement fonctionneUn modÃ¨le de l'environnement n'est pas disponible pour l'agent (ou pas utilisÃ©)Donne la possibilitÃ© d'utiliser des algorithmes de recherche (A* par exemple)\nComment crÃ©er un agent qui rÃ©sout ce genre de problÃ¨mes ?Plusieurs faÃ§ons et choix de conception possiblesUn premier critÃ¨re de distinction est l'utilisation/ou non d'un modÃ¨le complet de l'environnementLes fonctions de transition et de rÃ©compense sont entiÃ¨rement connuesAgent n'utilisant pas un modÃ¨le (Model-free agent)L'agent ne peut apprendre le fonctionnement de l'environnement que par ses expÃ©riencesComme vous pouvez vous y attendre, c'est une tÃ¢che trÃ¨s diï¬ƒcileRend diï¬ƒcile l'utilisation d'algorithmes de rechercheL'objectif est d'apprendre par des essais-erreurs une politique de sÃ©lection ou une fonction de valeur"
  },
  {
    "page": 23,
    "text": "Quentin CappartPlaniï¬cation et apprentissage\n23Deux techniques fondamentales pour rÃ©soudre des problÃ¨mes de dÃ©cision sÃ©quentielsApprentissage par renforcementPlaniï¬cation (planning) \nApproches hybridesSuppose que le modÃ¨le de l'environnement est connuL'agent utilise ce modÃ¨le pour amÃ©liorer sa politique de sÃ©lection\nUtilisation de l'apprentissage par renforcement pour apprendre un modÃ¨le de l'environnementRÃ©solution du problÃ¨me en utilisant des algorithmes de planiï¬cationPeut prendre par exemple la forme d'exploration dans un arbre d'Ã©tatsL'agent interagit avec l'environnement aï¬n d'amÃ©liorer sa politique de sÃ©lectionLe modÃ¨le de l'environnement est inconnuLa rÃ©solution est gouvernÃ©e par de l'apprentissage essais-erreurs\nLa rÃ©solution est gouvernÃ©e par un algorithme de recherche exploitant le mieux le modÃ¨leLa diï¬ƒcultÃ© est que le nombres de possibilitÃ©s est exponentiellement largeRÃ©solution d'un MDP"
  },
  {
    "page": 24,
    "text": "Quentin CappartRÃ©solution par planiï¬cation\n24Algorithmes de rÃ©solutionProgrammation dynamique (policy iteration, value iteration)Algorithmes de recherche (A*, minimax, monte carlo tree search)Labyrinthe simpliï¬Ã©\nADADAD\nADADAD\nAD\nLa diï¬ƒcultÃ© vient du fait que le nombre d'Ã©tats est en gÃ©nÃ©ral trÃ¨s grand\n"
  },
  {
    "page": 25,
    "text": "Quentin CappartRÃ©solution par apprentissage par renforcement\n25Algorithmes de rÃ©solutionAlgorithmes basÃ©s sur l'apprentissage d'une fonction de valeur (Q-learning, SARSA, etc.)Algorithmes basÃ©s sur l'apprentissage d'une politique de sÃ©lection (policy gradient, A3C, PPO, etc.)Labyrinthe simpliï¬Ã©\nLe labyrinthe est inconnu: l'agent doit le dÃ©couvrir par lui mÃªmeAgentEnvironmentActionStateRewardAD\nADRÃ©compense: -1"
  },
  {
    "page": 26,
    "text": "Quentin CappartCatÃ©gorisation d'agents : value-based et policy-based agents\n26Agent apprenant la fonction de valeur (value-based agent)L'agent se borne Ã  apprendre la meilleure fonction de valeur possibleAvec cette information, la policy devient impliciteL'objectif est d'avoir la meilleure estimation de la qualitÃ© de chaque Ã©tat\nQuelle est cette fonction de sÃ©lection implicite ?Chaque fois prendre l'action amenant Ã  l'Ã©tat qui a la meilleure qualitÃ©AD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1Agent apprenant la fonction de sÃ©lection (policy-based agent)\nADL'agent tente d'apprendre directement la meilleure politique de sÃ©lectionN'utilise pas de fonction de valeursAgent hybride (actor-critic)L'agent utilise Ã  la fois la fonction de valeur et de sÃ©lectionLa fonction de valeur n'est qu'une Ã©tape intermÃ©diaireCe qui nous intÃ©resse, au ï¬nal, c'est la politique de sÃ©lectionÏ€(a|s)=argmaxa(QÏ€(s,a))âˆ€sâˆˆS"
  },
  {
    "page": 27,
    "text": "Quentin CappartTaxonomie des agents de RL\n27Fonction de valeurValue-based agentsPolitique de sÃ©lectionPolicy-based agentsActor-critic\nModÃ¨le de l'environnementModel-based agentModel-free agent"
  },
  {
    "page": 28,
    "text": "Quentin CappartCompromis entre l'exploration et l'exploitation\n28\nIntuitionL'apprentissage par renforcement se fait par essais-erreursL'agent doit dÃ©couvrir une bonne politique Ã  partir d'expÃ©riencesNÃ©cessite d'eï¬€ectuer des actions amenant Ã  de bonnes rÃ©compensesMais aussi d'explorer des zones inconnues, pouvant Ãªtre prometteusesExplorationL'objectif est d'acquÃ©rir de l'information sur l'environnementExploitationCompromisL'objectif est d'exploiter l'information aï¬n de maximiser la rÃ©compenseUn bon agent intÃ¨gre un compromis entre ces deux aspectsExploitation pure: maximisation de la rÃ©compense sur base d'une vision incomplÃ¨te de l'environnementExploration pure: aucune maximisation de la rÃ©compense n'est faiteCrÃ©er un agent ayant un bon compromis est un dÃ©ï¬ majeur en apprentissage par renforcementhttp://ai.berkeley.edu/home.html"
  },
  {
    "page": 29,
    "text": "Quentin CappartExemples de compromis\n29Choix d'un restaurantAller continuellement au mÃªme restaurant que l'on sait bonEssayer un nouveau restaurant qui vient d'ouvrirExtraction de matiÃ¨res prÃ©cieuses (cuivre, fer, or, etc.)Miner dans une zone oÃ¹ on a dÃ©jÃ  trouvÃ© des ï¬lonsMiner dans une nouvelle zoneStratÃ©gie pour des jeuxSuivre une stratÃ©gie qui fonctionne bienEssayer une autre stratÃ©gieCampagne de marketingUtiliser des publicitÃ©s que l'on sait eï¬ƒcacesEssayer des nouveaux prototypes de publicitÃ©s\nhttp://ai.berkeley.edu/home.html\nCompÃ©tition de RL sur Minecraft: https://minerl.io/\nChoix de ï¬lmsRegarder une grosse production hollywoodienneTenter un ï¬lm d'auteurs moins connus"
  },
  {
    "page": 30,
    "text": "Quentin CappartIllustration de la diï¬ƒcultÃ© pratique de l'exploration\n30\nDeepMind Blog https://deepmind.com/blog/article/deep-reinforcement-learning\nMontezuma's Revenge\nBreakout\nMontezuma's Revenge requiert une importante facultÃ© exploration pour Ãªtre rÃ©solu"
  },
  {
    "page": 31,
    "text": "Quentin CappartSynthÃ¨se des notions vues\n31Apprentissage par renforcementApprentissage qui se fait sur base d'un signal de rÃ©compenseL'objectif est de rÃ©soudre un processus de dÃ©cisions sÃ©quentielles\nCours donnÃ© Ã  PolytechniqueINF8953DE: Reinforcement learning (Sarath Chandar)\nAgentEnvironmentActionStateRewardLe cas standard est un processus de dÃ©cision de MarkovEnsemble d'Ã©tatsEnsemble d'actionsFonction de transitionFonction de rÃ©compense\nComposantes d'un agent ModÃ¨le de l'environnement: reprÃ©sentation que l'agent se fait de l'environnementFonction de valeur: reprÃ©sentation que l'agent se fait sur la qualitÃ© de chaque Ã©tatPolitique de sÃ©lection: action que l'agent va eï¬€ectuer, s'il se trouve dans un Ã©tat spÃ©ciï¬queAutres ressourcesDeepMind: Cours en lignehttps://deepmind.com/learning-resources/reinforcement-learning-series-2021Livre: Reinforcement learning: an introduction (Sutton et Barto, 2nd)\n"
  },
  {
    "page": 32,
    "text": " \nQuentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesApprentissage par renforcement: FIN"
  }
]