[
  {
    "page": 1,
    "text": " \nINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 8: Apprentissage non-supervisÃ©"
  },
  {
    "page": 2,
    "text": "Contenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques \n"
  },
  {
    "page": 3,
    "text": "Table des matiÃ¨res\n3Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e\n"
  },
  {
    "page": 4,
    "text": "Apprentissage non-supervisÃ©\n4Apprentissage supervisÃ©Principe: apprentissage qui se fait Ã  l'aide de donnÃ©es dont on connaÃ®t la vraie valeur\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}Ce label est indispensable pour entraÃ®ner un modÃ¨le (minimiser la fonction de coÃ»t)\nÂ VraieÂ valeurÂ (groundÂ truth):Â PourÂ chaqueÂ donnÃ©eÂ x(i),Â onÂ connaÃ®tÂ leÂ labelÂ y(i)\nQue se passe t-il si on a pas accÃ¨s Ã  ces labels ? \nType dâ€™apprentissage qui se fait sÃ»r base de donnÃ©es non-labellisÃ©esApprentissage non-supervisÃ©\nNote: on a quand mÃªme besoin de donnÃ©es, mais sans la prÃ©sence dâ€™un labelIntuition: la tÃ¢che est plus compliquÃ©e car on a moins dâ€™information Ã  disposition\nD:{x(1),x(2),â€¦,x(m)}\nOn nâ€™a plus aucun moyen dâ€™entraÃ®ner le modÃ¨leâ€¦"
  },
  {
    "page": 5,
    "text": "Diï¬€Ã©rences avec l'apprentissage supervisÃ©\n5Apprentissage supervisÃ©: apprentissage dâ€™une fonction de sÃ©paration\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}\nD:{x(1),x(2),â€¦,x(m)}Point commun: cela revient Ã  la minimisation dâ€™une fonction coÃ»t (ou la maximisation dâ€™une prÃ©cision)Apprentissage non-supervisÃ©: apprentissage dâ€™une fonction de regroupement ou dâ€™exclusion"
  },
  {
    "page": 6,
    "text": "Application: segmentation du marchÃ©\n6Segmentation du marchÃ©OpÃ©ration eï¬€ectuÃ©e en marketing stratÃ©gique pour diviser la demande en plusieurs groupes cohÃ©rents\nObjectif: permet de dÃ©ï¬nir des oï¬€res ciblÃ©es Ã  chaque groupeEtape 1: Ã©tablir les caractÃ©ristiques des diï¬€Ã©rents clients (critÃ¨res gÃ©ographiques, civils, individuels, etc.)Etape 2: regrouper les clients similaires en un groupe cohÃ©rent (cluster)Etape 3: Ã©tablir des oï¬€res ou des stratÃ©gies ciblÃ©es pour chacun des groupes\nApplications: problÃ¨me prÃ©sent dans Ã©normÃ©ment de contextes impliquant une clientÃ¨le Ã  toucher\n"
  },
  {
    "page": 7,
    "text": "\nApplication: cybersÃ©curitÃ©\n7DÃ©tection de comportements frauduleux\nObjectif: permet de contrÃ´ler automatiquement les comportements suspects dans un rÃ©seauOpÃ©ration eï¬€ectuÃ©e en cybersÃ©curitÃ© pour surveiller le comportement d'utilisateurs dans un rÃ©seauEtape 1: Ã©tablir les caractÃ©ristiques des diï¬€Ã©rents utilisateurs (ressources utilisÃ©es, position actuelle, etc.)Etape 2: Ã©tablir un proï¬l d'utilisation normale en fonction des utilisateurs prÃ©sentsEtape 3: dÃ©tecter les comportements inhabituels\nAn interactive visual analytics approach for network anomaly detection through smart labeling [Fan et al., 2019]\n"
  },
  {
    "page": 8,
    "text": "Application: systÃ¨mes de recommandation\n8SystÃ¨mes de recommandationOpÃ©ration consistant Ã  recommander un ensemble d'entitÃ©s Ã  un utilisateurEtape 1: Ã©tablir les caractÃ©ristiques des utilisateurs (prÃ©fÃ©rences, achats prÃ©cÃ©dents, etc.)Etape 2: dÃ©celer les entitÃ©s qu'un utilisateur serait enclins Ã  vouloir \nApplications: Ã©normÃ©ment dâ€™utilisations dans le commerce en ligne\n"
  },
  {
    "page": 9,
    "text": "Application: visualisation de donnÃ©es\n9Visualisation de donnÃ©esObjectif: oï¬€rir une reprÃ©sentation visuelles des donnÃ©es Ã  des ï¬ns d'analyse par un humain\nMise en Ã©vidence de migrations importantes inattenduesCluster des pays ex-URSS avec l'EthiopieVisualizing Data using t-SNE [van der Maaten and Hinton, 2008]RÃ©duction de donnÃ©es de 100 dimensions Ã  2Les mots semblables sont projetÃ©s Ã  proximitÃ©The World Migration Network: rankings, groups and gravity models [Cappart and Thonet, 2015]\nVersion lisible: https://lvdmaaten.github.io/tsne/ (Words in 2D)\n"
  },
  {
    "page": 10,
    "text": "Table des matiÃ¨res\n10Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e\n"
  },
  {
    "page": 11,
    "text": "ProblÃ¨me du regroupement (clustering)\n11Construction du cluster: besoin de dÃ©ï¬nir un algorithme dÃ©diÃ©ProblÃ¨me consistant Ã  regrouper les donnÃ©es similaires en groupes (clusters) cohÃ©rentsProblÃ¨me du regroupement (clustering)\nNote: il sâ€™agit dâ€™une des grandes familles d'application de l'apprentissage non-supervisÃ©\nEvaluation dâ€™un clustering: besoin de dÃ©ï¬nir une fonction de coÃ»t"
  },
  {
    "page": 12,
    "text": "Algorithme K-means\n12Initialisation: choisir k valeurs alÃ©atoirement, correspondant au centre de k clustersAlgorithme K-meansEtape 1: regrouper les donnÃ©es au centre (centroÃ¯des) du cluster le plus procheEtape 2: mettre Ã  jour les centres des clusters, en fonction des donnÃ©es qui y ont Ã©tÃ© ajoutÃ©es/retirÃ©esEtape 3: rÃ©pÃ©ter les Ã©tapes (1) Ã  (3) jusquâ€™Ã  convergencePrincipe: dÃ©ï¬nir le centre de k groupes, et associer les donnÃ©es au groupe ayant le centre le plus proche\nEtape dâ€™initialisation\nEtape 1: assignation au cluster le plus proche\nEtape 2: mise Ã  jour des centresDÃ©ï¬nition des centres: de maniÃ¨re itÃ©rative, en 3 Ã©tapes successives Ã  partir dâ€™une valeur initiale\nStratÃ©gie de regroupement: les nouvelles donnÃ©es sont assignÃ©es au cluster ayant le centre le plus proche Convergence: lâ€™itÃ©ration nâ€™a pas changÃ© les centres des clusters"
  },
  {
    "page": 13,
    "text": "Algorithme K-means - visualisation\n13Situation initiale\nAssignation au cluster le plus proche\nMise Ã  jour des centres\nAssignation au cluster le plus proche\nMise Ã  jour des centres\nAssignation au cluster le plus proche\nLe processus est rÃ©pÃ©tÃ© jusqu'Ã  ce que les centres ne bougent plus (convergence)"
  },
  {
    "page": 14,
    "text": "Algorithme K-means - Formalisation\n14Notationsk:leÂ nombreÂ deÂ clusters{x(1),x(2),â€¦,x(m)}:lesÂ donnÃ©esÂ d'entraÃ®nementx(i)âˆˆâ„n:dimensionÂ d'uneÂ donnÃ©eÂ (nombreÂ deÂ caractÃ©ristiques)Î¼j:leÂ centreÂ duÂ clusterÂ j:âˆ€jâˆˆ{1,â€¦,k}c(i):indiceÂ duÂ clusterÂ oÃ¹Â laÂ donnÃ©eÂ iÂ aÂ Ã©tÃ©Â assignÃ©eÎ¼jâˆˆâ„n:dimensionÂ duÂ centreÂ duÂ clusterÎ¼c(i):centreÂ duÂ clusterÂ oÃ¹Â laÂ donnÃ©eÂ iÂ aÂ Ã©tÃ©Â assignÃ©eObjectif: minimiser la distance moyenne entre les donnÃ©es et le centre du cluster associÃ©Fonction de coÃ»t dâ€™un regroupement\nJ(c(1),â€¦,c(m),Î¼1,â€¦,Î¼j)=1mmâˆ‘i=1x(i)âˆ’Î¼c(i)2Â avecÂ x(i)âˆ’Î¼c(i)2=(x(i)1âˆ’Î¼1,c(i))2+â€¦+(x(i)nâˆ’Î¼n,c(i))2Â (pourÂ nÂ caractÃ©ristiques)Intuition: cela correspond Ã  la moyenne des distances euclidiennes entre chaque donnÃ©e et le centre associÃ©\nVisualisation pour des donnÃ©es ayant deux caractÃ©ristiques numÃ©riques (rÃ©elles)x(42)Î¼rougeÎ¼vertk=2Â (clusterÂ rougeÂ etÂ vert)x(i)âˆ’Î¼c(i)2=x(42)âˆ’Î¼vert2c(42)=vert"
  },
  {
    "page": 15,
    "text": "Algorithme K-means\n15\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}On donne en entrÃ©e le jeu de donnÃ©es et le nombre de clusters souhaitÃ©sOn place les centres initiaux alÃ©atoirementEtape 1: on assigne les donnÃ©es au cluster le plus procheOn ne considÃ¨re que les donnÃ©es associÃ©es au cluster actuel dans lâ€™itÃ©rationOn retourne les assignations de chaque donnÃ©e, et les centres des clustersSituation initiale\nAssignation au cluster le plus proche\nMise Ã  jour des centroÃ¯desEtape 2: on calcule le nouveau centre du cluster, Ã©tant donnÃ© les points liÃ©sPrincipe: chaque itÃ©ration va rapprocher les centres dâ€™un groupe de donnÃ©es et ainsi rÃ©duire le coÃ»t actuel"
  },
  {
    "page": 16,
    "text": "Analyse de l'algorithme\n16\nEst-ce que cet algorithme va converger ? Bonne nouvelle: chaque Ã©tape rapproche les centres des donnÃ©esConvergence: assurÃ©e car le coÃ»t est bornÃ© (jamais nÃ©gatif)ConsÃ©quence: chaque Ã©tape rÃ©duit ainsi le coÃ»t J\nEst-ce que la solution obtenue est optimale  ?  Mauvaise nouvelle: on risque de tomber dans un minimum localAugmentation du risque: lorsquâ€™on augmente le nombre de clusters ou le nombre de caractÃ©ristiques\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}"
  },
  {
    "page": 17,
    "text": "AmÃ©lioration de l'algorithme\n17\nAvez-vous une idÃ©e pour pallier cette diï¬ƒcultÃ© ? Module 3: dÃ©terminer les meilleurs clusters est un COPCadre gÃ©nÃ©ral: minimiser cette fonction de coÃ»t est NP-diï¬ƒcileModules 3 et 4: mÃ©thodes pour rÃ©soudre ce genre de problÃ¨mesProgrammation par contraintes: diï¬ƒcile Ã  mettre en oeuvreDiï¬ƒcultÃ©: le problÃ¨me implique Ã©normÃ©ment de variables\nAlgorithme K-means: algorithme de recherche localeEtapes 1 et 2: mouvements locaux pour minimiser le coÃ»tInitialisation: points de dÃ©part de la rechercheStratÃ©gie actuelle: de type Hill climbing sans diversiï¬cationExemples: redÃ©marrages, initialisation intelligente, ou utilisation de mÃ©taheuristiquesDiï¬ƒcultÃ©: forte chance de rester coincÃ© dans un minimum localNP-hardness of Euclidean sum-of-squares clustering [Aloise et al., 2009]Bonne nouvelle: on a vu plusieurs mÃ©thodes pour mitiger ce problÃ¨me"
  },
  {
    "page": 18,
    "text": "K-means avec des redÃ©marrages\n18MÃ©thode de redÃ©marrages\nğ–¿ğ—ˆğ—‹iâˆˆ1ğ—ğ—ˆÎ“:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡sâ‹†ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œğ–¶ğ—‚ğ—ğ—ğ–±ğ–¾ğ—Œğ—ğ–ºğ—‹ğ—(D,k,Î“):s=ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k)ğ—‚ğ–¿J(s)<J(sâ‹†):sâ‹†=ssâ‹†=âŠ¥Observation: la solution oÃ¹ lâ€™algorithme converge dÃ©pend des positions initiales des clustersAmÃ©lioration simple: redÃ©marrer plusieurs fois lâ€™algorithme et prendre la meilleure solution trouvÃ©e\nNote: montre les connexions entre les diï¬€Ã©rentes modes de lâ€™intelligence artiï¬cielleAutres amÃ©liorations: utilisation de mÃ©taheuristiques (algorithmes gÃ©nÃ©tiques)\nÎ¼j=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€jâˆˆ{1,â€¦,k}ğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:âˆ€iâˆˆ{1,â€¦,m}:c(i)=ğ–ºğ—‹ğ—€ğ—†ğ—‚ğ—‡jâˆ¥x(i)âˆ’Î¼jâˆ¥2ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡c(1),â€¦,c(m),Î¼1,â€¦,Î¼kâˆ€jâˆˆ{1,â€¦,k}:Î¼j=ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾ğ–¬ğ–¾ğ–ºğ—‡ğ–µğ–ºğ—…ğ—ğ–¾(P)ğ–ªğ–¬ğ–¾ğ–ºğ—‡ğ—Œ(D,k):\nP={x(i)âˆˆDc(i)=j}\nHG-means: A scalable hybrid genetic algorithm for minimum sum-of-squares clustering [Gribel and Vidal, 2019]Avantage: trÃ¨s facile Ã  mettre en oeuvre"
  },
  {
    "page": 19,
    "text": "Algorithme K-means: initialisation des centres des clusters\n19\nComment initialiser alÃ©atoirement les centres ?\nk-means++: The advantages of careful seeding [Arthur and Vassilvitskii, 2006]Option 1: Initialisation totalement alÃ©atoirePrincipe: les valeurs sont initialisÃ©es alÃ©atoirement dans une zone dÃ©limitÃ©eDiï¬ƒcultÃ©: les centres peuvent se retrouver loin des donnÃ©esConclusion: convergence plus lente et mÃ©thode peu utilisÃ©ePrincipe: les valeurs initiales sont prises alÃ©atoirement sur des donnÃ©esAvantage: mÃ©thode simple avec de bons rÃ©sultats en pratiqueDiï¬ƒcultÃ©: certains placements sont moins pertinents que dâ€™autresOption 2: initialisation alÃ©atoire sur des donnÃ©esExemple: 2 centres initialisÃ©s trÃ¨s proches lâ€™un de lâ€™autre\nAlgorithme k-means++:  algorithme implÃ©mentant ce principePrincipe: choisir des centres initialement bien distants des autresOption 3: initialisation alÃ©atoire contrÃ´lÃ©e sur les donnÃ©es"
  },
  {
    "page": 20,
    "text": "Choix du nombre de clusters\n20Diï¬ƒcultÃ© de la tÃ¢che: choix pas toujours clair, et qui peut Ãªtre subjectifAlternatives: plusieurs critÃ¨res de sÃ©lection sont possibles\nOption 2: dÃ©cision en fonction de l'objectif subsÃ©quentExemple: on souhaite que le marchÃ© soit divisÃ© en exactement 3 segmentsPrincipe: le choix est fait sur base de l'objectif de la tÃ¢che de clusteringOption 1: dÃ©cision itÃ©rativeObservation: augmenter le nombre de cluster va rÃ©duire le coÃ»tPrincipe: augmenter ce nombre jusqu'Ã  ce que la rÃ©duction soit minimeExemple de gauche: prendre 4 ou 5 cluster semble appropriÃ©\nOption 3: utilisation dâ€™un autre algorithme de regroupementAï¬ƒnity propagation: le choix de ce nombre est intÃ©grÃ© dans l'algorithme\nCombien de clusters doit-on dÃ©ï¬nir pour ces donnÃ©es ?\n"
  },
  {
    "page": 21,
    "text": "\nTÃ¢che de regroupement: autres alternatives\n21\nhttps://scikit-learn.org/stable/modules/clustering.htmlIl existe Ã©normÃ©ment de variantes et dâ€™autres algorithmes pour rÃ©aliser une tÃ¢che de regroupementAlgorithme K-modes: dÃ©diÃ© aux donnÃ©es avec des catÃ©gories en caractÃ©ristiquesAlgorithme K-prototypes: dÃ©diÃ© aux donnÃ©es avec caractÃ©ristiques quelconques"
  },
  {
    "page": 22,
    "text": "Table des matiÃ¨res\n22Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e\n"
  },
  {
    "page": 23,
    "text": "DÃ©tection d'anomalies\n23Objectif: prÃ©dire la probabilitÃ© qu'une nouvelle donnÃ©e soit habituelleDÃ©cision: si la probabilitÃ© est infÃ©rieure Ã  un seuil, alors c'est une anomalieProblÃ¨me consistant Ã  repÃ©rer les donnÃ©es hors-normes  par rapport Ã  ensemble de donnÃ©es dans la normeDÃ©tection dâ€™anomalies (anomaly detection)\nPrincipe de la dÃ©tection d'anomaliesEtape 2: construire un modÃ¨le reï¬‚Ã©tant une utilisation normale du systÃ¨me, sur base de ces donnÃ©esEtape 3: utiliser ce modÃ¨le aï¬n de prÃ©dire si une nouvelle utilisation est normale ou suspecteEtape 1: collecter un ensemble de donnÃ©es d'utilisation normale d'un systÃ¨me\n"
  },
  {
    "page": 24,
    "text": "DÃ©tection d'anomalies - Formalisation\n24{x(1),x(2),â€¦,x(m)}:lesÂ donnÃ©esÂ d'entraÃ®nementx(i)âˆˆâ„n:dimensionÂ deÂ chaqueÂ donnÃ©eÂ (nombreÂ deÂ caractÃ©ristiques)Ì‚y=f(x):â„™(xÂ estÂ uneÂ utilisationÂ normaleÂ duÂ systÃ¨me)Ì‚yâ‰¥Ïµ:laÂ donnÃ©eÂ xÂ estÂ dansÂ laÂ normeÌ‚y<Ïµ:laÂ donnÃ©eÂ xÂ estÂ uneÂ anomaliex(ğ—ğ–¾ğ—Œğ—):donnÃ©eÂ deÂ test\nUtilisation CPUUtilisation RAM\nHypothÃ¨se que lâ€™on va faire: chaque caractÃ©ristique est rÃ©gie par une loi normale qui lui est propreEn dessous du seuil: considÃ©rÃ© comme une anomalieÏµ=0.75Ïµ=0.5Ïµ=0.1Le gestionnaire pourra ensuite analyser cette utilisation en dÃ©tail\nComment crÃ©er une fonction pour cette tÃ¢che ?Ïµâˆˆ[0,1]:seuilÂ deÂ dÃ©cisionÂ pourÂ laÂ considÃ©rationÂ d'uneÂ anomalieExemple: ensemble de donnÃ©es Ã  deux caractÃ©ristiquesCela va passer par la dÃ©ï¬nition dâ€™une hypothÃ¨se sur la fonction\nLâ€™objectif est dâ€™apprendre une fonction de prÃ©diction"
  },
  {
    "page": 25,
    "text": "Distribution normale (gausienne)\n25\nLoiÂ normale:Â p(x|Î¼,Ïƒ2)=1Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼)22Ïƒ2)Ïƒ2:varianceÂ (paramÃ¨treÂ deÂ laÂ loi)xâˆ¼ğ’©(Î¼,Ïƒ2):xÂ prendÂ uneÂ valeurÂ tirÃ©eÂ d'uneÂ distibutionÂ normaleÂ deÂ paramÃ¨tresÂ Î¼,Ïƒ2Î¼:espÃ©ranceÂ (paramÃ¨treÂ deÂ laÂ loi)HypothÃ¨se: chaque caractÃ©ristique est tirÃ©e d'une loi normale spÃ©ciï¬quex1âˆ¼ğ’©(Î¼1,Ïƒ21)x2âˆ¼ğ’©(Î¼2,Ïƒ22)xnâˆ¼ğ’©(Î¼n,Ïƒ2n)â€¦ConsÃ©quence: chaque caractÃ©ristique a ses propres paramÃ¨tres, relatifs Ã  une loi normale\nCombien de paramÃ¨tres doit-on dÃ©terminer pour des donnÃ©es Ã  n caractÃ©ristiques ?Loi normale: comporte 2 paramÃ¨tres (espÃ©rance, et variance)\nIdÃ©e: les paramÃ¨tres des ces lois normales sont appris Ã  lâ€™aide des donnÃ©es disponiblesTotal: 2n paramÃ¨tres car on a n lois normales dans notre modÃ¨le"
  },
  {
    "page": 26,
    "text": "Fonction de vraisemblance\n26\nComment dÃ©terminer ces paramÃ¨tres ?Objectif: obtenir la loi normale qui est la plus plausible (ou vraisemblable) par rapport Ã  nos donnÃ©es\nBesoin: formaliser mathÃ©matiquement cette notion de vraisemblanceMesure de la vraisemblance d'une distribution statistiques pour les donnÃ©es considÃ©rÃ©esFonction de vraisemblance (likelihood function)\nL(Î¼,Ïƒ2|x(1),â€¦,x(m))=p(x(1)|Î¼,Ïƒ2)Ã—â€¦Ã—p(x(m)|Î¼,Ïƒ2)\nNote: cette fonction donne un score dâ€™Ã  quel point le modÃ¨le (la loi normale) ï¬tte les donnÃ©esPrincipe du calcul: on prend le produit des probabilitÃ©s de chaque donnÃ©e Ã©tant donnÃ© la loiCondition: suppose que les donnÃ©es sont indÃ©pendantes, et identiquement distribuÃ©es\nAttention: confusion courante entre la notion de probabilitÃ© et de vraisemblanceProbabilitÃ©: plausibilitÃ© d'obtenir un Ã©vÃ¨nement alÃ©atoire selon un certain modÃ¨leVraisemblance: plausibilitÃ© d'un modÃ¨le, Ã©tant donnÃ© l'observation de rÃ©alisations d'une variable alÃ©atoire "
  },
  {
    "page": 27,
    "text": "Estimation du maximum de vraisemblance\n27ProblÃ¨me consistant Ã  trouver les paramÃ¨tres d'une loi normale aï¬n de maximiser la fonction de vraisemblance par rapport aux donnÃ©es observÃ©es Maximum de vraisemblance (maximum likelihood estimation - MLE)\nmaxÎ¼,Ïƒ2L(Î¼,Ïƒ2|x(1),â€¦,x(m))Note: cette dÃ©ï¬nition peut Ã©galement Ãªtre Ã©tendue Ã  des modÃ¨les qui ne sont pas des lois normales\nQuelles sont ces valeurs ?Intuition: on souhaite construire la loi normale la plus plausible par rapport aux donnÃ©es Objectif: trouver les valeurs des deux paramÃ¨tres maximisant la fonction de vraisemblance\nPreuve: https://www.youtube.com/watch?v=Dn6b9fCIUpMAttention: notez bien qu'on a une loi normale par caractÃ©ristique des donnÃ©esRÃ©sultat: ces paramÃ¨tres donnent la loi normale la plus vraisemblable pour nos donnÃ©es\nBonne nouvelle: il sâ€™agit dâ€™un rÃ©sultat statistique connu\nNote: la dÃ©ï¬nition des paramÃ¨tres peut Ãªtre beaucoup plus diï¬ƒcile avec dâ€™autres modÃ¨les\nÎ¼=1mmâˆ‘i=1x(i)\nÏƒ2=1mmâˆ‘i=1(x(i)âˆ’Î¼)2"
  },
  {
    "page": 28,
    "text": "Estimations des paramÃ¨tres pour la dÃ©tection d'anomalies\n28HypothÃ¨se 1: chaque caractÃ©ristique vient d'une loi normale spÃ©ciï¬queâ€¦Objectif: trouver les meilleurs paramÃ¨tres relatifs Ã  la distribution normale de chaque paramÃ¨treHypothÃ¨se 2: les caractÃ©ristiques sont considÃ©rÃ©es indÃ©pendantes des autres \nDonnÃ©es d'entrainement\nDistribution de l'utilisation CPU\nDistribution de l'utilisation RAMModÃ¨le ï¬nal: probabilitÃ© jointe des distributions de chaque caractÃ©ristique (leur multiplication) \nÌ‚y=p(x1|Î¼1,Ïƒ21)Ã—â€¦Ã—p(xn|Î¼n,Ïƒ2n)=nâˆj=1p(xj|Î¼j,Ïƒ2j)Suppose l'indÃ©pendance entre chaque caractÃ©ristique\nx1âˆ¼ğ’©(Î¼1,Ïƒ21)\nxnâˆ¼ğ’©(Î¼n,Ïƒ2n)"
  },
  {
    "page": 29,
    "text": "Algorithme de dÃ©tection d'anomalies\n29Phase d'entraÃ®nement\nPhase d'Ã©valuationObjectif: dÃ©terminer les paramÃ¨tres des lois normales\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡Î¼1,Ïƒ21,â€¦,Î¼n,Ïƒ2nğ–¿ğ—ˆğ—‹jâˆˆ{1,â€¦,n}:Î¼j=1mmâˆ‘i=1x(i)jÏƒ2j=1mmâˆ‘i=1(x(i)jâˆ’Î¼j)2ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–³ğ—‹ğ–ºğ—‚ğ—‡ğ—‚ğ—‡ğ—€(D):On utilise les donnÃ©es d'entraÃ®nementPour chaque caractÃ©ristiqueâ€¦...On calcule les deux paramÃ¨tresOn retourne tous les paramÃ¨tres (c-Ã -d, le modÃ¨le)\nÌ‚y=nâˆj=1p(x(ğ—ğ–¾ğ—Œğ—)j|Î¼j,Ïƒ2j)ğ—‚ğ–¿Ì‚y<Ïµ:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–ºğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)ğ–¾ğ—…ğ—Œğ–¾:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ—‹ğ–¾ğ—€ğ—ğ—…ğ–ºğ—‹ğ–½ğ–ºğ—ğ–ºğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)=nâˆj=11Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼j)22Ïƒ2j)ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡(x(ğ—ğ–¾ğ—Œğ—),Ïµ,Î¼1,Ïƒ21,â€¦,Î¼n,Ïƒ2n):Objectif: calculer la probabilitÃ© qu'une nouvelle donnÃ©e soit rÃ©guliÃ¨reOn utilise les lois normales dÃ©terminÃ©es prÃ©cÃ©demmentOn calcule la probabilitÃ© jointe que la nouvelle donnÃ©e soit rÃ©guliÃ¨reSi elle est en dessous du seuil, on la considÃ¨re comme une anomalie \nNotre modÃ¨le est le produit des lois normales de chaque caractÃ©ristique"
  },
  {
    "page": 30,
    "text": "Limitations de notre modÃ¨le\n30\nQuelles sont les limitations de notre modÃ¨le ?Exemple: considÃ©rons les donnÃ©es suivantes\nEst-ce que la donnÃ©e rouge est une anomalie ou non ?\nLimitation: lâ€™indÃ©pendance des caractÃ©ristiques est une hypothÃ¨se trop restrictive pour ces situationsVisuellement: il semble que oui car elle est fort diï¬€Ã©rente de nos donnÃ©es d'entraÃ®nementDans notre modÃ¨le: chaque caractÃ©ristique prise sÃ©parÃ©ment possÃ¨de une valeur probableSoucis: on ne tient pas compte de la corrÃ©lation entre nos caractÃ©ristiquesDiï¬ƒcultÃ©: cette anomalie ne sera pas probablement pas dÃ©tectÃ©e par notre modÃ¨leNature de lâ€™anomalie: câ€™est d'avoir une valeur basse pour x1 et une haute pour x2 simultanÃ©ment\n"
  },
  {
    "page": 31,
    "text": "Distribution normale multivariÃ©e\n31Principe clef: la probabilitÃ© dâ€™occurence pour chaque caractÃ©ristique est rÃ©git par sa propre loi normaleÎ¼âˆˆâ„:espÃ©ranceÏƒ2âˆˆâ„:variance\nModÃ¨leÂ actuel:Â Ì‚y=p(x1|Î¼1,Ïƒ21)Ã—â€¦Ã—p(xn|Î¼n,Ïƒ2n)=nâˆj=1p(xj|Î¼j,Ïƒ2j)Total:2nÂ paramÃ¨tres\nComment peut-on tenir compte des corrÃ©lations entre les caractÃ©ristiques ?ModÃ¨le basÃ© sur une distribution normale multivariÃ©e\nÌ‚y=p(x|Î¼,Î£)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))Î£âˆˆâ„nÃ—n:matriceÂ variance-covarianceÂ (variance/covarianceÂ entreÂ chaqueÂ paireÂ deÂ caractÃ©ristiques)Î¼âˆˆâ„n:vecteurÂ desÂ espÃ©rancesÂ (uneÂ valeurÂ pourÂ chaqueÂ caractÃ©ristique)Principe: il sâ€™agit dâ€™une gÃ©nÃ©ralisation de la loi normale Ã  plusieurs dimensions\nDiï¬€Ã©rence fondamentale: permet d'exprimer la corrÃ©lation entre les caractÃ©ristiquesApprentissage: dÃ©terminer les paramÃ¨tres du modÃ¨leIntuition: on aura un modÃ¨le plus expressif que le prÃ©cÃ©dent"
  },
  {
    "page": 32,
    "text": "32Distribution normale multivariÃ©e - visualisation\nEspÃ©rance centrÃ©e en (0,0)MÃªme variance pour chaque caractÃ©ristiqueEspÃ©rance centrÃ©e en (0,0)Les caractÃ©ristiques x1 sont plus dispersÃ©esEspÃ©rance centrÃ©e en (-2,3)MÃªme varianceDiagonale de la matrice: correspond Ã  la variance de chaque caractÃ©ristiqueÎ¼=(00)\nÎ£=(1001)\nÎ¼=(00)\nÎ£=(5001)\nÎ¼=(âˆ’23)\nÎ£=(1001)\nCes distributions pouvaient dÃ©jÃ  Ãªtre modÃ©lisÃ©e avec notre premier modÃ¨le (covariances nulles)"
  },
  {
    "page": 33,
    "text": "33Distribution normale multivariÃ©e - visualisation\nLes deux caractÃ©ristiques ont  tendance Ã  augmenter ensembleExpressivitÃ© du modÃ¨le: ces situations ne pouvaient pas Ãªtre modÃ©lisÃ©es avec notre premier modÃ¨leCovariance positiveUne caractÃ©ristique a tendance Ã   diminuer lorsque l'autre augmenteCovariance nÃ©gativeSituation quelconqueÎ¼=(00)\nÎ£=(10.90.91)\nÎ¼=(00)\nÎ£=(1âˆ’0.5âˆ’0.51)\nÎ¼=(âˆ’12)\nÎ£=(0.6âˆ’0.7âˆ’0.71)\nGÃ©nÃ©ralitÃ©: sâ€™adapte Ã  nâ€™importe quel nombre de dimensions"
  },
  {
    "page": 34,
    "text": "\nMaximumÂ deÂ vraisemblance:Â maxÎ¼,Î£L(Î¼,Î£|x(1),â€¦,x(m))DÃ©tection d'anomalies - second modÃ¨le\n34Total:Â n+n+(n2âˆ’n)2Â paramÃ¨tresÂ paramÃ¨tresÂ Ã Â dÃ©terminerÂ (contreÂ 2nÂ dansÂ notreÂ modÃ¨leÂ prÃ©cÃ©dent)\nObjectif: trouver la meilleure loi normale multivariÃ©ePrincipe: prendre les paramÃ¨tres maximisant la fonction de vraisemblanceBonne nouvelle: il sâ€™agit encore dâ€™un rÃ©sultat statistique connuNotez bien que chaque donnÃ©e est un vecteur de dimension n x 1\nCombien de paramÃ¨tres doit-on dÃ©terminer pour n caractÃ©ristiques ?Vecteur de n paramÃ¨tresMatrice symÃ©trique de taille n x nVecteurÂ desÂ moyennes:Â Î¼=(âˆ’12)MatriceÂ variance/covariance:Â Î£=(0.6âˆ’0.7âˆ’0.71)Exemple: considÃ©rons des donnÃ©es Ã  deux caractÃ©ristiques\nAttention: ne pas oublier de considÃ©rer que la matrice variance/covariance est symÃ©trique\nÎ¼=1mmâˆ‘i=1x(i)\nÎ£=1mmâˆ‘i=1(x(i)âˆ’Î¼)(x(i)âˆ’Î¼)T"
  },
  {
    "page": 35,
    "text": "Algorithme de dÃ©tection d'anomalies - second modÃ¨le\n35Phase d'entraÃ®nement\nPhase d'Ã©valuationObjectif: dÃ©terminer les paramÃ¨tres de la loi normale multivariÃ©eOn utilise les donnÃ©es d'entraÃ®nement\nOn retourne tous les paramÃ¨tres (c-Ã -d, le modÃ¨le)Objectif: calculer la probabilitÃ© qu'une nouvelle donnÃ©e soit rÃ©guliÃ¨reOn utilise la loi normale multivariÃ©e dÃ©ï¬nieEn dessous du seuil: on considÃ¨re la donnÃ©e comme une anomalie La probabilitÃ© est dÃ©ï¬nie selon cette loi normale\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡Î¼,Î£Î¼=1mmâˆ‘i=1x(i)Î£=1mmâˆ‘i=1(x(i)âˆ’Î¼)(x(i)âˆ’Î¼)Tğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–³ğ—‹ğ–ºğ—‚ğ—‡ğ—‚ğ—‡ğ—€ğ–¬ğ—ğ—…ğ—ğ—‚(D):\nDÃ©ï¬nition du vecteur des espÃ©rancesDÃ©ï¬nition de la matrice variance/covariance\nÌ‚y=p(x(ğ—ğ–¾ğ—Œğ—)|Î¼,Î£)ğ—‚ğ–¿Ì‚y<Ïµ:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ–ºğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)ğ–¾ğ—…ğ—Œğ–¾:ğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡ğ—‹ğ–¾ğ—€ğ—ğ—…ğ–ºğ—‹ğ–½ğ–ºğ—ğ–ºğ–¿ğ—ˆğ—‹x(ğ—ğ–¾ğ—Œğ—)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))ğ– ğ—‡ğ—ˆğ—†ğ–ºğ—…ğ—’ğ–£ğ–¾ğ—ğ–¾ğ–¼ğ—ğ—‚ğ—ˆğ—‡ğ–¬ğ—ğ—…ğ—ğ—‚(x(ğ—ğ–¾ğ—Œğ—),Ïµ,Î¼,Î£):"
  },
  {
    "page": 36,
    "text": "Comparaison entre les deux modÃ¨les\n36\nQuelles sont les forces et faiblesses des deux modÃ¨les ?\nAvantage: croissance linÃ©aire des paramÃ¨tres avec le nombre de caractÃ©ristiquesInconvÃ©nient: manque d'expressivitÃ© pour considÃ©rer les corrÃ©lations Avantage: trÃ¨s peu coÃ»teux Ã  calculerDistribution normale univariÃ©e\nÌ‚y=nâˆj=1p(xj|Î¼j,Ïƒ2j)=nâˆj=11Ïƒ2Ï€exp(âˆ’(xâˆ’Î¼j)22Ïƒ2j)Î¼âˆˆâ„:espÃ©ranceÏƒ2âˆˆâ„:variance\n#paramÃ¨tres:2nDistribution normale multivariÃ©e\nÌ‚y=p(x|Î¼,Î£)=1(2Ï€)n/2Ã—ğšğšğš(Î£)1/2exp(âˆ’12(xâˆ’Î¼)TÎ£âˆ’1(xâˆ’Î¼))Î£âˆˆâ„nÃ—n:matriceÂ covarianceÎ¼âˆˆâ„n:vecteurÂ desÂ espÃ©rances\n#paramÃ¨tres:2n+(n2âˆ’n)2InconvÃ©nient: la croissance des paramÃ¨tres est maintenant quadratiqueAvantage: le modÃ¨le assez expressif pour capturer les corrÃ©lationsInconvÃ©nient: plus coÃ»teux Ã  calculer (inversion dâ€™une matrice)Diï¬ƒcultÃ©: requiert un grand nombre de donnÃ©es (au moins plus que n) pour avoir une matrice inversible"
  },
  {
    "page": 37,
    "text": "Diï¬€Ã©rences avec l'apprentissage supervisÃ©\n37\nQue pensez-vous de cette idÃ©e ?\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}\n(x(i),y(i))avecÂ y(i)=0â†’donnÃ©eÂ rÃ©guliÃ¨re(x(i),y(i))avecÂ y(i)=1â†’anomalieIdÃ©e: collecter un grand nombre de donnÃ©es concernant la tÃ¢che Ã  Ã©tudier\nApprentissage supervisÃ©: suppose un grand nombre dâ€™exemples de chaque classeDiï¬ƒcultÃ©: on a rarement un nombre suï¬ƒsant de donnÃ©es pour les anomaliesAnomalie: diï¬ƒcile Ã  caractÃ©riser (peu semblable Ã  tout ce qu'on a pu voir)MÃªme si on peut avoir beaucoup de donnÃ©es concernant les cas sains\nApprentissage non-supervisÃ©: pas sensible Ã  cette diï¬ƒcultÃ©\nConsÃ©quence: il peut Ãªtre intÃ©ressant de passer Ã  une mÃ©thode supervisÃ©eAu ï¬l du temps: on peut avoir assez de donnÃ©es pour caractÃ©riser une anomalieExemple: dÃ©tection de spams (qui sont devenus trÃ¨s frÃ©quents)Passage progressif Ã  de lâ€™apprentissage supervisÃ©"
  },
  {
    "page": 38,
    "text": "Table des matiÃ¨res\n38Apprentissage non-supervisÃ©1. DÃ©ï¬nition et applications de l'apprentissage non-supervisÃ© 2. Formalisation du problÃ¨me de regroupement (clustering) 3. Algorithme K-means 4. Formalisation du problÃ¨me de dÃ©tection d'anomalies 5. Notion de statistique: maximum de vraisemblance 6. ModÃ©lisation par des distributions normales univariÃ©es 7. ModÃ©lisation par une distribution normale multivariÃ©e\n"
  },
  {
    "page": 39,
    "text": "SynthÃ¨se des notions vues\n39Cours donnÃ©s Ã  PolytechniqueINF8111: Fouille de donnÃ©es (Daniel Aloise)LOG6308: SystÃ¨mes de recommandation (Michel Desmarais)\nApprentissage non-supervisÃ©DÃ©ï¬nition: apprentissage qui se fait sur base de donnÃ©es non-labellisÃ©esPrincipe: exploitation de la similaritÃ© entre les donnÃ©es pour l'apprentissage\nD:{x(1),x(2),â€¦,x(m)}ProblÃ¨me de regroupementObjectif: regrouper les donnÃ©es similaires en groupes (clusters) cohÃ©rentsMÃ©thode de rÃ©solution: algorithme K-meansPrincipe: recherche locale pour minimiser une fonction de coÃ»tAmÃ©liorations possibles: redÃ©marrages, choix des positions initiales, etc.\nProblÃ¨me de dÃ©tection d'anomalies\nObjectif: prÃ©dire la probabilitÃ© quâ€™une nouvelle donnÃ©e soit rÃ©guliÃ¨reModÃ©lisation: produit de lois normales univariÃ©es ou une loi normale mutivariÃ©ePrincipe: maximiser une fonction de vraisemblance"
  },
  {
    "page": 40,
    "text": "\nExemples de questions d'examen\n401. Eï¬€ectuer plusieurs itÃ©rations de l'algorithme k-means sur un jeu de donnÃ©e 2. Savoir dÃ©tecter si une donnÃ©e est une anomalie Ã  l'aide des modÃ¨les vusThÃ©oriePratique1. Donner deux applications et exemple d'application de l'apprentissage non-supervisÃ©s 2. Expliquer les garanties de convergence de l'algorithme k-means 3. Expliquer les avantages/inconvÃ©nients des deux modÃ¨les pour la dÃ©tection d'anomalie"
  },
  {
    "page": 41,
    "text": " \nINF8215 - Intelligence artiï¬cielleMÃ©thodes et algorithmesApprentissage non-supervisÃ©: FIN\nDALLE: Detecting an anomaly, by Picasso"
  }
]