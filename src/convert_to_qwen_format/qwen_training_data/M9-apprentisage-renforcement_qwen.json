[
  {
    "id": "6585d7c4-d7c0-4bb8-9bdd-578d3c094125",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_1.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 9: Apprentissage par renforcement"
      }
    ]
  },
  {
    "id": "0542cf0e-266b-432b-8af0-ac7ae8023e2e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_2.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "0129acc4-04b5-4cea-a28a-a2d9ccad5c14",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_3.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n3Apprentissage par renforcement1. DÃ©ï¬nition de l'apprentissage par renforcement 2. Illustration sur diï¬€Ã©rents problÃ¨mes 3. DÃ©ï¬nition et formalisation d'un processus de dÃ©cisions sÃ©quentielles 4. Formalisation d'un agent d'apprentissage par renforcement 5. Diï¬ƒcultÃ©s en apprentissage par renforcementCe module ne fait pas partie de la matiÃ¨re d'examen"
      }
    ]
  },
  {
    "id": "ca01c7af-a60d-4e1a-a6e2-124f4b5549a5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_4.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartCaractÃ©ristiques de l'apprentissage par renforcement\n4Apprentissage supervisÃ©L'apprentissage se faisait Ã  l'aide de donnÃ©es labellisÃ©esD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}L'objectif est d'arriver Ã  apprendre sur base de donnÃ©es dont on connait la vraie valeurApprentissage non-supervisÃ©Apprentissage qui se fait uniquement sur base de donnÃ©es non-labellisÃ©esD:{x(1),x(2),â€¦,x(m)}L'objectif est d'utiliser la similaritÃ© entre les donnÃ©es pour orienter l'apprentissageApprentissage par renforcementL'apprentissage de l'agent se fait sur base d'un processus d'essais-erreursRequiert un moyen d'Ã©valuer la qualitÃ© des essais que l'agent eï¬€ectueL'agent va collecter de nouvelles donnÃ©es au fur et Ã  mesure de ses essaisL'agent agit dans un environnement donnÃ©Situations oÃ¹ un agent doit trouver la meilleure sÃ©quence de dÃ©cisions pour remplir un objectifChaque action faite est susceptible de modiï¬er l'Ã©tat de l'environnement"
      }
    ]
  },
  {
    "id": "980e9a4f-16bf-4683-b158-ee11154fcaae",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_5.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartApplication en robotique\n5\nhttps://www.youtube.com/watch?v=n2gE7n11h1YLearning to Walk via Deep Reinforcement Learning. Tuomas Haarnoja et al. Robotics: Science and Systems (RSS). 2019."
      }
    ]
  },
  {
    "id": "2bfefb69-9d09-49b8-baab-2b297f910432",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_6.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAlphaGo: Match contre Lee Sedol\n6\nhttps://www.youtube.com/watch?v=WXuK6gekU1YNotez la surprise gÃ©nÃ©rale lors du mouvement d'AlphaGo"
      }
    ]
  },
  {
    "id": "04d2fabe-e9aa-430d-9a74-123484f54642",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_7.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartIllustration d'un problÃ¨me de dÃ©cisions sÃ©quentielles\n7\nPacman (PremiÃ¨re version du jeu date de 1980)ObjectifCollecter le plus de points avant de se faire mangerToutes les informations concernant le plateauPosition du Pacman, fantÃ´mes, bonus, etc.Etat (state)Tout ce qui est pertinent pour la prise de dÃ©cisionActionDÃ©cisions que le joueur peut faireRÃ©compense (reward)Une action va changer l'Ã©tat actuelUne action va engendrer un signal de rÃ©compenseSignal utilisÃ© pour orienter la prise de dÃ©cisionValeur nÃ©gative pour une mauvaise actionMouvement Ã  gauche, droite, haut, bas ou rien faire- 100 si on se dÃ©place sur un fantÃ´meValeur positive pour une bonne action+ 10 si on prend un bonusProcessus de dÃ©cisions sÃ©quentiellesEï¬€ectuer la meilleure sÃ©quence de dÃ©cisions aï¬n de rÃ©aliser un objectifExemple"
      }
    ]
  },
  {
    "id": "7cf668e7-3e39-4c65-80bc-86b4666dc054",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_8.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVue globale d'un processus de dÃ©cisions sÃ©quentielles\n8AgentEnvironmentActionStateRewardAgentL'intelligence prenant les dÃ©cisions (ou eï¬€ectuant les actions) aï¬n d'accomplir son objectifEnvironnementDÃ©ï¬nit la dynamique du monde (Ã©tats du systÃ¨me, les actions possibles, l'impact des actions, etc.)Englobe toutes les informations utiles et pouvant Ãªtre disponibles pour la prise de dÃ©cisionsLe monde dans lequel l'agent va interagir\nL'agent et l'environnement sont en continuelle interactionL'agent modiï¬e l'environnement par ses actionsL'environnement prÃ©sente des situations diï¬€Ã©rentes Ã  l'agent"
      }
    ]
  },
  {
    "id": "fe1b3cc2-7522-4ba7-a9e9-961b2235c431",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_9.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©compense dans un processus de dÃ©cisions sÃ©quentielles\n9InterprÃ©tation de la rÃ©compenseN'importe quel objectif Ã  atteindre peut Ãªtre dÃ©crit comme une maximisation des rÃ©compenses cumulÃ©es espÃ©rÃ©es de toutes les actions eï¬€ectuÃ©es.HypothÃ¨se de la rÃ©compense\nPeut sembler restrictif, mais est assez ï¬‚exible et applicable dans beaucoup de situationsSignal numÃ©rique qui indique Ã  quel point l'agent a bien agi pour un certain instant Element fondamental qui est utilisÃ© pour formaliser l'objectif Ã  atteindre\nHypothÃ¨se fondamentale en apprentissage par renforcement\nQue pensez vous de cette hypothÃ¨se ?L'objectif correspond ainsi Ã  maximiser le total des rÃ©compenses cumulÃ©es espÃ©rÃ©es\nCependant, il peut Ãªtre diï¬ƒcile de dÃ©ï¬nir certaines tÃ¢ches en terme de rÃ©compenses...On parle de rÃ©compense espÃ©rÃ©e, car certaines actions peuvent donner diï¬€Ã©rentes rÃ©compensesMaximisation des rÃ©compenses\nNote"
      }
    ]
  },
  {
    "id": "b75b6914-0245-4353-bd63-edd3e2444538",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_10.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemples de rÃ©compenses\n10\nJeu du casse-briquesLe score du jeu (fonction du nombre de briques cassÃ©es)\nPousse l'agent Ã  maximiser son score\nJeu d'Ã©chec+1 en cas de victoire0 en cas d'Ã©galitÃ©-1 en cas de dÃ©faite\nQue pensez vous de l'idÃ©e d'intÃ©grer des connaissances du jeu (heuristiques) dans la fonction de rÃ©compense ?\nPortefeuille d'investissement+x pour chaque dollar gagnÃ© par vos placements-x pour chaque dollar perdu par vos placements"
      }
    ]
  },
  {
    "id": "611559c1-927a-45f1-9de7-adfc002e470c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_11.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemples de rÃ©compenses (mauvaise conception)\n11\nCoastRunner 7\nhttps://openai.com/blog/faulty-reward-functions/Jeu de course avec des bateauxBesoin de passer par diï¬€Ã©rents checkpointsPlusieurs bonus peuvent Ãªtre collectÃ©s (turbo, etc.)Signal de rÃ©compenseRÃ©compense positive si on passe par un checkpointRÃ©compense positive si on prend un bonus \nQue pensez-vous de cette conception ?L'agent prÃ©fÃ¨re collecter les bonus plutÃ´t que ï¬nir la course\nOn rÃ©compense des sous-objectifs basÃ©s sur notre connaissance du jeu, et non l'objectif ï¬nal\nSnakeCollecter le maximum de pointsSignal de rÃ©compense+1 si on collecte un fruit\nQue pensez-vous de cette conception ?-1 pour chaque dÃ©placement (pour aller sur le fruit au plus vite)L'agent va Ãªtre pousser Ã  se suicider pour limiter les pertes"
      }
    ]
  },
  {
    "id": "3e0b8283-7a59-43fe-aec6-ccaf1d1abf9c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_12.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFormalisation d'une fonction objectif\n12Maximiser le total des rÃ©compenses cumulÃ©esObjectif\nQue pensez-vous de prendre, Ã  chaque Ã©tape, l'action  qui donne le plus de rÃ©compenses dans l'immÃ©diat ?Les actions faites peuvent avoir un impact Ã  long terme ExemplesUn investissement engendre des frais immÃ©diats, mais peut prendre des annÃ©es pour Ãªtre proï¬tableEï¬€ectuer une maintenance a un coÃ»t, mais peut empÃªcher un dysfonctionnement dans les prochains joursLes meilleures rÃ©compenses peuvent arriver trÃ¨s tard, et Ãªtre conditionnelles Ã  des actions eï¬€ectuÃ©s tÃ´tMaximiser chaque rÃ©compense immÃ©diate ne revient pas Ã  maximiser la rÃ©compense globaleSouvent, il est prÃ©fÃ©rable de sacriï¬er une rÃ©compense immÃ©diate, pour une meilleure au long termeRÃ©compense Ã  long terme\nAvez-vous des exemples concrets ?"
      }
    ]
  },
  {
    "id": "4002c3d8-e31a-4209-982c-d45b34462351",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_13.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartProcessus de dÃ©cision de Markov (MDP)\n13AgentEnvironmentActionStateRewardEnsembles essentielst=1,2,3,â€¦:instantsÂ temporelsÂ (timeÂ step)St:l'Ã©tatÂ deÂ l'environnementÂ auÂ tempsÂ tAt:l'actionÂ priseÂ parÂ l'agentÂ auÂ tempsÂ tp:SÃ—AÃ—Sâ†’[0,1]:laÂ fonctionÂ deÂ probabilitÃ©Â d'allerÂ d'unÂ Ã©tatÂ Ã Â unÂ autreÂ enÂ faisantÂ uneÂ certaineÂ actionA:l'ensembleÂ deÂ toutesÂ lesÂ actionsÂ possiblesS:l'ensembleÂ deÂ tousÂ lesÂ Ã©tatsÂ possibles\nr:SÃ—Aâ†’â„:laÂ fonctionÂ deÂ rÃ©compense,Â donnantÂ uneÂ valeurÂ Ã Â l'actionÂ faiteÂ Ã Â partirÂ d'unÂ certainÂ Ã©tatFormalisation d'un MDPHypothÃ¨se fondamentale d'un MDPChaque Ã©tat est markovien"
      }
    ]
  },
  {
    "id": "c29339e7-8b7b-4b38-b9ae-e5866af1e304",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_14.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartEtat markovien\n14â„™(St+1|St)=â„™(St+1|S1,â€¦,St)Intuitivement, l'Ã©tat actuel contient toutes les informations utiles du passÃ©Etat Markovien\nConnaissant uniquement la position de la voiture,  peut-on prÃ©dire la position ?Exemple: voiture en mouvementEtat amÃ©liorÃ©: position, vitesse, accÃ©lÃ©ration, etc.Un Ã©tat est markovien s'il contient toutes les informations utiles du passÃ©La probabilitÃ© d'aller Ã  un autre Ã©tat ne dÃ©pend que de notre Ã©tat prÃ©sent\nAndrey Markov  (1856-1922)Une fois que l'on a notre Ã©tat actuel, on peut jeter l'historique\nQue faire en cas d'Ã©tats non-markoviens ?Augmenter l'Ã©tat avec de nouvelles informations Etat incomplet: positionHypothÃ¨se rÃ©aliste, et simpliï¬ant Ã©normÃ©ment les algorithmes de dÃ©cisions"
      }
    ]
  },
  {
    "id": "ff016755-64d1-4d59-8da7-77544792686e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_15.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVariantes des MDPs\n15Processus de dÃ©cision de Markov stochastique\nProcessus de dÃ©cision de Markov partiellement observable (POMDP)Il en va de mÃªme pour la rÃ©compenseChaque action comporte de l'alÃ©atoire, et peut vous amener Ã  diï¬€Ã©rents Ã©tats\nL'agent n'a qu'une vision incomplÃ¨te de l'environnementVariante gÃ©nÃ©ralement considÃ©rÃ©e dans la formalisation standardL'agent doit construire sa propre reprÃ©sentation de son Ã©tat\nC'est pourquoi on parle de maximisation de la rÃ©compense cumulÃ©e espÃ©rÃ©e\nL'agent ne reÃ§oit qu'une observation de l'Ã©tat de l'environnement"
      }
    ]
  },
  {
    "id": "59b5300b-fd69-4754-b1fe-f570c3991bc5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_16.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nComposantes d'un agent d'apprentissage par renforcement\n16ModÃ¨le de l'environnement (model)\nPolitique de sÃ©lection d'actions (policy)Fonction de valeur (value function)La reprÃ©sentation que l'agent se fait de l'environnementLa reprÃ©sentation que l'agent se fait de la qualitÃ© de chaque Ã©tat et de chaque action \nLa dÃ©cision de l'action Ã  prendre Ã©tant donnÃ© notre Ã©tat\nPlacement  Ã  cet endroit++=\nhttps://cf.shopee.ph/ï¬le/ 673d6d6c4e658a40ea481efda94820e4_tn\nEst-ce qu'un Ãªtre humain peut Ãªtre modÃ©lisÃ© par ces trois composantes ?Chaque agent de RL est basÃ© sur une ou plusieurs de ces composantes\n+Placement en  haut/droite=+Placement en  haut/droite=..."
      }
    ]
  },
  {
    "id": "95165bf0-51f3-487e-ade1-39e49cf9a4df",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_17.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartModÃ¨le de l'environnement (model)\n17IntuitionReprÃ©sentation qu'a l'agent sur le fonctionnement de l'environnementp(sâ€² |s,a)=â„™[St=sâ€² |Stâˆ’1=s,Atâˆ’1=a]ProbabilitÃ© d'aller sur un Ã©tat suivant Ã  partir d'une action faite Ã  l'Ã©tat actuel\nr(s,a)=ğ”¼[Rt|St=s,At=a]Fonction de transitionp:SÃ—SÃ—Aâ†’[0,1]Fonction de rÃ©compenseEspÃ©rance de la rÃ©compense immÃ©diate si une action est faite Ã  partir de l'Ã©tat actuelr:SÃ—Aâ†’â„Notez que cette formalisation tient compte d'un processus de dÃ©cisions stochastiquesCommentairesReprend les informations sur l'eï¬€et des actions (modiï¬cation de l'Ã©tat, et rÃ©compenses collectÃ©es) \nEn gÃ©nÃ©ral, l'agent ne sait pas comment fonctionne l'environnement et doit le dÃ©couvrir par essais/erreurs"
      }
    ]
  },
  {
    "id": "ea8221ad-7f4c-4887-9da1-2252f0582426",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_18.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartPolitique de sÃ©lection d'action (policy)\n18DÃ©ï¬ni le comportement de l'agentEtant donnÃ© notre Ã©tat actuel, quelle action doit Ãªtre prisea=Ï€(s)Ï€(a|s):â„™[At=a|St=s]Ï€:Sâ†’AIntuitionPolitique dÃ©terministeEtant donnÃ© mon Ã©tat, mon action sera toujours la mÃªmePolitique stochastiqueEtant donnÃ© mon Ã©tat, le choix de mon action est sujet Ã  de l'alÃ©atoire contrÃ´lÃ© \nEn quelles situations cela peut-il Ãªtre bÃ©nÃ©ï¬que pour l'agent ?Notez la diï¬€Ã©rence avec un environnement stochastiqueIci, c'est notre agent qui a un comportement stochastiqueParfois, la stratÃ©gie optimale implique d'agir de maniÃ¨re alÃ©atoire"
      }
    ]
  },
  {
    "id": "d27036ff-0a0d-41ba-8aef-0f707ccaa745",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_19.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction de valeur (value function)\n19Fonction de valeur d'un Ã©tat (state-value function)ReprÃ©sentation interne que l'agent se fait de la qualitÃ© de chaque Ã©tatSomme espÃ©rÃ©e des rÃ©compenses obtenues Ã  partir d'un Ã©tat, si on suit une certaine politique de sÃ©lection\nVÏ€(s)=ğ”¼[Rt+Î³Rt+1+Î³2Rt+2+Î³3Rt+3+â€¦|St=s]Â (formalisationÂ gÃ©nÃ©rale)Discounting factorValeur comprise entre 0 et 1Î³âˆˆ[0,1]Permet de considÃ©rer le fait qu'il est prÃ©fÃ©rable d'avoir une rÃ©compense tÃ´t plutÃ´t que tardValeur de 0: l'agent ne va considÃ©rer que la rÃ©compense immÃ©diateValeur de 1: l'agent va considÃ©rer toutes les rÃ©compenses avec la mÃªme importanceVÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]\nLa valeur des rÃ©compenses futures dÃ©croit avec le tempsVÏ€(s)=ğ”¼[Rt|St=s]VÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]Fonction de valeur d'une action (action-value function)QÏ€(s,a)=ğ”¼[Rt+Î³Rt+1+Î³2Rt+2+Î³3Rt+3+â€¦|St=s,At=a]Â (formalisationÂ gÃ©nÃ©rale)On peut Ã©galement construire une fonction de la qualitÃ© d'une action Ã  partir d'un certain Ã©tat"
      }
    ]
  },
  {
    "id": "6d9a2edd-58df-44c6-8831-677563aa2eef",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_20.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemple: l'environnement du labyrinthe\n20Environnement du labyrintheS'Ã©chapper du labyrinthe en eï¬€ectuant le moins de dÃ©placements\nADActions: dÃ©placement d'une casep(sâ€² |s,L)=1Â ifÂ sâ€² Â isÂ theÂ leftÂ (L)Â cellÂ ofÂ s\nr(s,a)=âˆ’1âˆ€sâˆˆS,âˆ€aâˆˆAp(sâ€² |s,L)=0Â otherwisep(sâ€² |s,T)=1Â ifÂ sâ€² Â isÂ theÂ topÂ (T)Â cellÂ ofÂ sp(sâ€² |s,T)=0Â otherwisep(sâ€² |s,R)=1Â ifÂ sâ€² Â isÂ theÂ rightÂ (R)Â cellÂ ofÂ sp(sâ€² |s,R)=0Â otherwisep(sâ€² |s,B)=1Â ifÂ sâ€² Â isÂ theÂ bottomÂ (B)Â cellÂ ofÂ sp(sâ€² |s,B)=0Â otherwiseFonctions de probabilitÃ© (ou de transition)\nFonction de rÃ©compenseEtats: la case oÃ¹ l'agent se trouveA={left,Â right,Â top,Â bottom}S=ensembleÂ desÂ casesÂ duÂ labyrinthe"
      }
    ]
  },
  {
    "id": "64fd7985-779e-40e1-af6f-bdb606ef9998",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_21.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemple d'un agent d'apprentissage par renforcement\n21Fonction de valeur d'un Ã©tat (state-value function)\nAD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1VÏ€(s)=ğ”¼[Rt+Rt+1+Rt+2+Rt+3+â€¦|St=s]Î³=1Â (sansÂ discount)Politique de sÃ©lection ADPolitique dÃ©terministeValeur de -1 par cases pour atteindre l'arrivÃ©e depuis une certaine casePour chaque Ã©tat, quelle action doit-Ãªtre eï¬€ectuÃ©e ?a=Ï€(s)Â (dÃ©ï¬niÂ parÂ lesÂ ï¬‚Ã¨ches)ModÃ¨leL'agent a un accÃ¨s au modÃ¨le complet de l'environnementIl connait parfaitement ls fonctions de transition et de rÃ©compense\nQuelle est la qualitÃ© de chaque Ã©tat ?CommentairesNotez que l'agent considÃ©rÃ© est dÃ©jÃ  entraÃ®nÃ©On n'a pas encore abordÃ© la maniÃ¨re de faire l'apprentissage"
      }
    ]
  },
  {
    "id": "a1fa0a79-e9fb-42f7-852f-e95977a1fb32",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_22.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartCatÃ©gorisation d'agents : avec ou sans modÃ¨le\n22Agent utilisant un modÃ¨le (model-based agent)Contient dans sa reprÃ©sentation un modÃ¨le complet de comment l'environnement fonctionneUn modÃ¨le de l'environnement n'est pas disponible pour l'agent (ou pas utilisÃ©)Donne la possibilitÃ© d'utiliser des algorithmes de recherche (A* par exemple)\nComment crÃ©er un agent qui rÃ©sout ce genre de problÃ¨mes ?Plusieurs faÃ§ons et choix de conception possiblesUn premier critÃ¨re de distinction est l'utilisation/ou non d'un modÃ¨le complet de l'environnementLes fonctions de transition et de rÃ©compense sont entiÃ¨rement connuesAgent n'utilisant pas un modÃ¨le (Model-free agent)L'agent ne peut apprendre le fonctionnement de l'environnement que par ses expÃ©riencesComme vous pouvez vous y attendre, c'est une tÃ¢che trÃ¨s diï¬ƒcileRend diï¬ƒcile l'utilisation d'algorithmes de rechercheL'objectif est d'apprendre par des essais-erreurs une politique de sÃ©lection ou une fonction de valeur"
      }
    ]
  },
  {
    "id": "541e0a9e-38c3-4be6-aea4-8f18d42727e4",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_23.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartPlaniï¬cation et apprentissage\n23Deux techniques fondamentales pour rÃ©soudre des problÃ¨mes de dÃ©cision sÃ©quentielsApprentissage par renforcementPlaniï¬cation (planning) \nApproches hybridesSuppose que le modÃ¨le de l'environnement est connuL'agent utilise ce modÃ¨le pour amÃ©liorer sa politique de sÃ©lection\nUtilisation de l'apprentissage par renforcement pour apprendre un modÃ¨le de l'environnementRÃ©solution du problÃ¨me en utilisant des algorithmes de planiï¬cationPeut prendre par exemple la forme d'exploration dans un arbre d'Ã©tatsL'agent interagit avec l'environnement aï¬n d'amÃ©liorer sa politique de sÃ©lectionLe modÃ¨le de l'environnement est inconnuLa rÃ©solution est gouvernÃ©e par de l'apprentissage essais-erreurs\nLa rÃ©solution est gouvernÃ©e par un algorithme de recherche exploitant le mieux le modÃ¨leLa diï¬ƒcultÃ© est que le nombres de possibilitÃ©s est exponentiellement largeRÃ©solution d'un MDP"
      }
    ]
  },
  {
    "id": "ebd0e14f-14e7-4327-9dd5-3fd4fe7a463a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_24.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©solution par planiï¬cation\n24Algorithmes de rÃ©solutionProgrammation dynamique (policy iteration, value iteration)Algorithmes de recherche (A*, minimax, monte carlo tree search)Labyrinthe simpliï¬Ã©\nADADAD\nADADAD\nAD\nLa diï¬ƒcultÃ© vient du fait que le nombre d'Ã©tats est en gÃ©nÃ©ral trÃ¨s grand"
      }
    ]
  },
  {
    "id": "8d4dd63e-691b-4ec8-829d-dc06e7b90ec1",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_25.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©solution par apprentissage par renforcement\n25Algorithmes de rÃ©solutionAlgorithmes basÃ©s sur l'apprentissage d'une fonction de valeur (Q-learning, SARSA, etc.)Algorithmes basÃ©s sur l'apprentissage d'une politique de sÃ©lection (policy gradient, A3C, PPO, etc.)Labyrinthe simpliï¬Ã©\nLe labyrinthe est inconnu: l'agent doit le dÃ©couvrir par lui mÃªmeAgentEnvironmentActionStateRewardAD\nADRÃ©compense: -1"
      }
    ]
  },
  {
    "id": "27342d5e-5339-44a6-9d15-9f460749255b",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_26.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartCatÃ©gorisation d'agents : value-based et policy-based agents\n26Agent apprenant la fonction de valeur (value-based agent)L'agent se borne Ã  apprendre la meilleure fonction de valeur possibleAvec cette information, la policy devient impliciteL'objectif est d'avoir la meilleure estimation de la qualitÃ© de chaque Ã©tat\nQuelle est cette fonction de sÃ©lection implicite ?Chaque fois prendre l'action amenant Ã  l'Ã©tat qui a la meilleure qualitÃ©AD-13-12-13-11-11-10-9-8-7-12-6-7-8-13-5-6-5-4-3-7-2-1Agent apprenant la fonction de sÃ©lection (policy-based agent)\nADL'agent tente d'apprendre directement la meilleure politique de sÃ©lectionN'utilise pas de fonction de valeursAgent hybride (actor-critic)L'agent utilise Ã  la fois la fonction de valeur et de sÃ©lectionLa fonction de valeur n'est qu'une Ã©tape intermÃ©diaireCe qui nous intÃ©resse, au ï¬nal, c'est la politique de sÃ©lectionÏ€(a|s)=argmaxa(QÏ€(s,a))âˆ€sâˆˆS"
      }
    ]
  },
  {
    "id": "f546d3e3-e536-422d-ace3-74788529db1a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_27.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTaxonomie des agents de RL\n27Fonction de valeurValue-based agentsPolitique de sÃ©lectionPolicy-based agentsActor-critic\nModÃ¨le de l'environnementModel-based agentModel-free agent"
      }
    ]
  },
  {
    "id": "c30c3e89-c842-4dc4-912a-11dffeb8dd09",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_28.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartCompromis entre l'exploration et l'exploitation\n28\nIntuitionL'apprentissage par renforcement se fait par essais-erreursL'agent doit dÃ©couvrir une bonne politique Ã  partir d'expÃ©riencesNÃ©cessite d'eï¬€ectuer des actions amenant Ã  de bonnes rÃ©compensesMais aussi d'explorer des zones inconnues, pouvant Ãªtre prometteusesExplorationL'objectif est d'acquÃ©rir de l'information sur l'environnementExploitationCompromisL'objectif est d'exploiter l'information aï¬n de maximiser la rÃ©compenseUn bon agent intÃ¨gre un compromis entre ces deux aspectsExploitation pure: maximisation de la rÃ©compense sur base d'une vision incomplÃ¨te de l'environnementExploration pure: aucune maximisation de la rÃ©compense n'est faiteCrÃ©er un agent ayant un bon compromis est un dÃ©ï¬ majeur en apprentissage par renforcementhttp://ai.berkeley.edu/home.html"
      }
    ]
  },
  {
    "id": "3a40f284-435a-4436-ad4d-63ddbe9e9498",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_29.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemples de compromis\n29Choix d'un restaurantAller continuellement au mÃªme restaurant que l'on sait bonEssayer un nouveau restaurant qui vient d'ouvrirExtraction de matiÃ¨res prÃ©cieuses (cuivre, fer, or, etc.)Miner dans une zone oÃ¹ on a dÃ©jÃ  trouvÃ© des ï¬lonsMiner dans une nouvelle zoneStratÃ©gie pour des jeuxSuivre une stratÃ©gie qui fonctionne bienEssayer une autre stratÃ©gieCampagne de marketingUtiliser des publicitÃ©s que l'on sait eï¬ƒcacesEssayer des nouveaux prototypes de publicitÃ©s\nhttp://ai.berkeley.edu/home.html\nCompÃ©tition de RL sur Minecraft: https://minerl.io/\nChoix de ï¬lmsRegarder une grosse production hollywoodienneTenter un ï¬lm d'auteurs moins connus"
      }
    ]
  },
  {
    "id": "85df3915-ac38-4c6d-ae4f-319251086bf8",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_30.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartIllustration de la diï¬ƒcultÃ© pratique de l'exploration\n30\nDeepMind Blog https://deepmind.com/blog/article/deep-reinforcement-learning\nMontezuma's Revenge\nBreakout\nMontezuma's Revenge requiert une importante facultÃ© exploration pour Ãªtre rÃ©solu"
      }
    ]
  },
  {
    "id": "7fb4529a-039b-41b5-822e-a7ad272b9363",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_31.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartSynthÃ¨se des notions vues\n31Apprentissage par renforcementApprentissage qui se fait sur base d'un signal de rÃ©compenseL'objectif est de rÃ©soudre un processus de dÃ©cisions sÃ©quentielles\nCours donnÃ© Ã  PolytechniqueINF8953DE: Reinforcement learning (Sarath Chandar)\nAgentEnvironmentActionStateRewardLe cas standard est un processus de dÃ©cision de MarkovEnsemble d'Ã©tatsEnsemble d'actionsFonction de transitionFonction de rÃ©compense\nComposantes d'un agent ModÃ¨le de l'environnement: reprÃ©sentation que l'agent se fait de l'environnementFonction de valeur: reprÃ©sentation que l'agent se fait sur la qualitÃ© de chaque Ã©tatPolitique de sÃ©lection: action que l'agent va eï¬€ectuer, s'il se trouve dans un Ã©tat spÃ©ciï¬queAutres ressourcesDeepMind: Cours en lignehttps://deepmind.com/learning-resources/reinforcement-learning-series-2021Livre: Reinforcement learning: an introduction (Sutton et Barto, 2nd)"
      }
    ]
  },
  {
    "id": "5c4e283e-43c8-41aa-b73a-bfd29b4f8013",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M9-apprentisage-renforcement_page_32.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesApprentissage par renforcement: FIN"
      }
    ]
  }
]