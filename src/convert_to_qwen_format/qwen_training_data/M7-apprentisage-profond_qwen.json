[
  {
    "id": "59101b20-1c70-44cc-adc5-0d5a8d639d3e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_1.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiﬁcielleMéthodes et algorithmesModule 7: Réseaux de neurones et apprentissage profond"
      }
    ]
  },
  {
    "id": "0c42dbfb-508e-4d61-9261-efb2f0f6fa1a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_2.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartContenu du cours\n2Considérations pratiques et sociétalesModule 10: Utilisation en industrie, éthique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: Stratégies de recherche Module 2: Recherche en présence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisé Module 7: Réseaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisé Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "1c4a2559-abc0-4140-9c87-71897317b07d",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_3.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n3Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "8f5aeb3f-78a4-4ac9-937f-e4ce06962b77",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_4.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRappel de la régression logistique\n4z=w1x1+…+wnxn+bx1bwnw1xn..L(a,y)a=σ(z)Intérêt: permet de créer une frontière de décision linéaireEtape 1: on prend une combinaison linéaire des caractéristiquesEtape 2: on applique une sigmoide sur cette combinaison\nAlgorithme d’apprentissage: descente de gradient avec une passe en avant et en arrière\nHypothèse: une tendance linéaire entre les données peut être établieCe n’est malheureusement pas souvent le cas en pratiqueBesoin: on doit construire une fonction (hypothèse) plus expressive\nLes réseaux de neurones permettent de combler ce manque\nUne régression logistique construit d'une fonction prédisant une valeur entre 0 et 1 (module précédent)"
      }
    ]
  },
  {
    "id": "b3fce122-df51-46ed-8da2-341c64652636",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_5.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAperçu d'un réseau de neurones\n5z=w1x1+…+wnxn+bx1bwnw1xn...a=σ(z)\nxn...x1a Ensemble de neurones organisés en un réseau de plusieurs couchesRéseau de neurones\nSans le savoir, on a déjà construit nos premiers neurones !\nQu’est-ce qu’un neurone dans ce contexte ?Neurone actuel: combinaison linéaire de variables, à laquelle est appliquée une sigmoïde\nPar la suite: on verra comment créer d'autres types de neurones (non basé sur une sigmoïde)Représentation visuelle: les paramètres à apprendre (w1, ..., wn, b) sont souvent implicitesExemple de neurone\nRéseau: les neurones sont organisés en un réseau de plusieurs couches (layers)Intuition: chaque neurone apporte sa propre contribution pour la prédictionParamètres: chaque neurone a ses propres paramètres à apprendre (w et b)Conséquence: on a un modèle plus expressif qu’une régression logistique\nL’idée est d’apprendre une fonction diﬀérente avec chaque neurone, qui sont ensuite composées"
      }
    ]
  },
  {
    "id": "aa9096b9-2dff-4697-aa2f-d56b164452b1",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_6.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartMétaphore biologique\n6Transmission de l’information: sous forme de signal électrique, en passant de neurone en neuroneCerveau humain: organisé en un énorme réseau de neurones (86 millards de neurones)Attention: il s’agit simplement une métaphore, et non une correspondance exacte\nIl y a de nombreuses diﬀérences entre le fonctionnement d’un cerveau et ce qu’on va voirAttention: les inspirations naturelles (même existantes) sont plus de nature marketing que scientiﬁque\n\"C'est un abus de langage que de parler de neurones !  De la même façon qu'on parle d'aile pour un avion mais aussi pour un oiseau,  le neurone artiﬁciel est un modèle extrêmement simpliﬁé de la réalité biologique.\"https://www.sciencesetavenir.fr/high-tech/intelligence-artiﬁcielle/ selon-yann-lecun-l-intelligence-artiﬁcielle-a-20-ans-pour-faire-ses-preuves_120121\nYann LeCun"
      }
    ]
  },
  {
    "id": "d5373103-51d4-4aea-9d18-1eb7c536dcec",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_7.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nRéseau de neurones à deux couches: notations\n7Couche d’entrée: couche reprenant les caractéristiques de la donnéeCouche cachée: couche dédiée aux calculs intermédiairesConvention: la couche d’entrée est la couche 0Couche de sortie: couche dédiée à la prédiction ﬁnale Couche  d’entrée [0] (input layer)\nCouche  cachée [1] (hidden layer)\nCouche  de sortie [2] (output layer)La couche d’entrée n’a ainsi aucun paramètres à apprendre\nn[i]: nombre de neurones à la couche iw[i]j,b[i]j: paramètres du neurone j à la couche ixj: caractéristique j pour une donnée spéciﬁque \nNote: w[i]j est de dimension n[i−1]×1 (nombre de ﬂèches entrantes)aj: sortie du neurone j à la couche cachéêy: prédiction pour une donnée selon ses caractéristiques xz1=w[1]T1x+b[1]1a1=σ(z1)a1x2x1Intuition: un neurone est une régression logistique avec ses paramètres\nx1,x2\na1,a2,a3\n̂yNotation: on va étendre nos notations précédentes\na1=σ(w[1]T1x+b[1]1)\na2=σ(w[1]T2x+b[1]2)\na3=σ(w[1]T3x+b[1]3)"
      }
    ]
  },
  {
    "id": "86e34e69-b0d6-4fa6-9340-299192e3a7e5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_8.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRéseau de neurones: forme matricielleNotation matricielle: regrouper toutes les valeurs en vecteurs et matricesIntérêt: donne une expression compacte et plus eﬃcace à calculerx=(x1x2) vecteur des caractéristiques d'une instance spéciﬁqueW[1]=−w[1]T1−−w[1]T2−−w[1]T3−b[1]=b[1]1b[1]2b[1]3z[1]=z[1]1z[1]2z[1]3=−w[1]T1−−w[1]T2−−w[1]T3−(x1x2)+b[1]1b[1]2b[1]3=w[1]T1x+b[1]1w[1]T2x+b[1]2w[1]T3x+b[1]3 Etape de la combinaison linéaire: z[1]=W[1]x+b[1]\na[1]=a[1]1a[1]2a[1]3=σ(z[1]1)σ(z[1]2)σ(z[1]3)Etape de l'application de la sigmoide: a[1]=σ(z[1])Dimension des matricesx:n[0]×1=2×1W[1]:n[1]×n[0]=3×2b[1]:n[1]×1=3×1z[1]:n[1]×1=3×1a[1]:n[1]×1=3×18\na[i]j: sortie du neurone j à la couche i (suite de la notation précédente)z[i]j: calcul intermédiaire du neurone j à la couche iMatrice des poids  à la couche cachéeVecteur des biais  à la couche cachée"
      }
    ]
  },
  {
    "id": "f9fde7d1-b529-4803-ab2e-4c235001d87f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_9.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRéseau de neurones: équations fondamentales\nz[1]=W[1]x+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]̂y=σ(z[2])\nW[1]:n[1]×n[0]=3×2→6b[1]:n[1]×1=3×1→3b[2]:n[2]×1=1×1→1W[2]:n[2]×n[1]=1×3→3Total:13 paramètres devant être entrainés\nCombien de paramètres à apprendre a t-on dans ce réseau ?Expression similaire: notation a pour la couche d’entrée et de sortie\n9\nPrincipe: obtenir la prédiction à partir des caractéristiques en entrée\nz[1]=W[1]a[0]+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]a[2]=σ(z[2])Avantage: donne une expression plus générique\nPrincipe: calculer la taille de chaque matrice de paramètres, et sommer le toutNote: appliquer ces équations revient à faire la passe en avant dans un réseau de neurones\nCes équations sont les équations fondamentales des réseaux de neurones"
      }
    ]
  },
  {
    "id": "d49285a6-c55d-49a5-bb74-760840a05cfe",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_10.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRéseau de neurones : situation avec plusieurs données\nLimitation: ces équations ne considèrent qu’une seule donnée en entrée\nD:{(x(1),y(1)),(x(2),y(2)),…,(x(m),y(m))}\nz[1](i)=W[1]x(i)+b[1]a[1](i)=σ(z[1](i))z[2](i)=W[2]a[1](i)+b[2]a[2](i)=σ(z[2](i))∀i∈{1,…,m}̂y(i)=a[2](i)Objectif: appliquer ces équations sur chaque donnéeLa notation [i] réfère à une couche iLa notation (i) réfère à une donnée i\nErreur fréquente: confusion avec les indicesConseil: comprenez ce que chaque indice signiﬁe10\nz[1]=W[1]x+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]̂y=σ(z[2])\nz[1]=W[1]a[0]+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]a[2]=σ(z[2])Amélioration: adapter ces équations à un ensemble de donnéesNotation introduite dans le module précédent"
      }
    ]
  },
  {
    "id": "c5ee5b37-85bf-4b01-993d-2b02955d7027",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_11.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRéseau de neurones : calcul de la passe en avant itérative pour plusieurs données\nEst-ce que c'est correct ? \nEst-ce que c'est eﬃcace ? Oui !Non ! Il y a moyen de faire mieuxAlgorithme naïf: prédire itérativement le résultat pour chaque donnée\nAvez-vous une autre alternative ? \n11\nz[1](i)=W[1]x(i)+b[1]a[1](i)=σ(z[1](i))z[2](i)=W[2]a[1](i)+b[2]a[2](i)=σ(z[2](i))𝖿𝗈𝗋i𝗂𝗇1𝗍𝗈m:𝖿𝗈𝗋𝗐𝖺𝗋𝖽𝖯𝖺𝗌𝗌𝖨𝗍𝖾𝗋𝖺𝗍𝗂𝗏𝖾(D,W[1],b[1],…):\n𝗋𝖾𝗍𝗎𝗋𝗇a[2](1),…,a[2](m)Idée: on peut regrouper toutes les données dans une matrice|x(1)|,…,|x(m)|n[0]×1n[0]×1n[0]×m|…|x(1)…x(m)|…|=X#caractéristiques×#donnéesIntérêt: permet la conception d'algorithmes plus eﬃcaces, opérant sur des matricesIntérêt supplémentaire: cela est d'autant plus vrai lorsqu'on utilise des GPUsGraphical processor unit (GPU): processeur spécialisé pour réaliser des tâches similaires en parallèle\nForme matricielle: représentation des données en matrice à des ﬁns de performances"
      }
    ]
  },
  {
    "id": "b96a7bc7-8303-43fa-b46b-ea8592c8e725",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_12.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartForme matricielle pour plusieurs prédictions\nConséquence: les valeurs z, et a de chaque neurone deviennent une matriceZ[1]=−w[1]T1−−…−−w[1]Tn[1]−|…|x(1)…x(m)|…|+b[1]1…b[1]n[1]=|…|z[1](1)…z[1](m)|…|Forme matricielle: une colonne contient les informations spéciﬁques pour une instance\n=w[1]T1x(1)+b[1]1…w[1]T1x(m)+b[1]1………w[1]Tn[1]x(1)+b[1]n[1]…w[1]Tn[1]x(m)+b[1]n[1]A[1]=|…|σ(z[1](1))…σ(z[1](m))|…|=|…|a[1](1)…a[1](m)|…|Application de la sigmoide à chaque valeur (element-wise)Broadcasting de bX=|…|x(1)…x(m)|…|\nDimension:n[0]×mDimension:#caractéristiques×#données\n12\nDimension:n[1]×mDimension:#neurones×#données\nDimension:n[1]×mDimension:#neurones×#données\nCes opérations reviennent à calculer la passe en avant en même temps pour un ensemble de données"
      }
    ]
  },
  {
    "id": "93c75039-51f3-4526-a317-68c01d76e488",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_13.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartEquations fondamentales sous forme matricielle\nz[1]=W[1]x+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]a[2]=σ(z[2])̂y=a[2]Convention: on utilise des lettres majuscules pour les matricesApplication: problème de l'étudiant avec deux caractéristiques# heures d'étude:x1# heures de repos:x2Données: quatre données d’entraînementD:{(x(1),y(1)),(x(2),y(2)),(x(3),y(3)),(x(4),y(4))}Représentation: forme matricielleX=(x(1)1x(2)1x(3)1x(4)1x(1)2x(2)2x(3)2x(4)2)\nQue represente  x(3)2 ?Réponse: nombre d'heures de repos du troisième étudiantSortie de la première couche:A[1]=a[1](1)1a[1](2)1a[1](3)1a[1](4)1a[1](1)2a[1](2)2a[1](3)2a[1](4)2a[1](1)3a[1](2)3a[1](3)3a[1](4)3\nQue represente  a[1](2)3 ?Réponse:  activation du 3e neurone  de la couche 1 pour le  2e étudiantPrédiction ﬁnale: une valeur pour chaque donnéêY=(̂y(1)̂y(2)̂y(3)̂y(4))A[2]=(a[2](1)1a[2](2)1a[2](3)1a[2](4)1)13\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]Illustration: passage à une forme matricielle (équations de droite)"
      }
    ]
  },
  {
    "id": "b250a9d2-4fef-46a7-8927-6c17493ccff3",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_14.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n14Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "35ff605a-124e-4acb-9f13-84da0eb6bd1e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_15.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nApprentissage supervisé par un réseau de neurones\nJ(W[1],b[1],W[2],b[2])=1mm∑i=1L(̂y(i),y(i))b[1]:n[1]×1=3×1W[1]:n[1]×n[0]=3×2W[2]:n[2]×n[1]=1×3b[2]:n[2]×1=1×113 paramètres à déterminer\nConséquence: la fonction peut être minimisée par une descente de gradientBonne nouvelle: tout le réseau est diﬀérentiable si la fonction d'écart l'est égalementConséquence: on risque de converger dans un minimum local15\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]\nPrincipe: identique à une régression logistique, sauf que la fonction contient plus de paramètresObjectif: déterminer les paramètres qui minimisent une fonction de coût liée à la tâche de prédiction\nComment déterminer la valeur de ces paramètres ? Expression générique, qui ne diﬀère  que par la fonction d’écart choisieMauvaise nouvelle: la fonction de coût n’est plus convexe"
      }
    ]
  },
  {
    "id": "92d9cfbd-ac19-4538-8556-aded808abd3f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_16.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDescente de gradient dans un réseau de neurones\n16\n𝖼𝗈𝗆𝗉𝗎𝗍𝖾̂Y𝗋𝖾𝗉𝖾𝖺𝗍𝗎𝗇𝗍𝗂𝗅𝖼𝗈𝗇𝗏𝖾𝗋𝗀𝖾𝗇𝖼𝖾:𝖼𝗈𝗆𝗉𝗎𝗍𝖾∂J∂W[2],∂J∂b[2],∂J∂W[1],∂J∂b[1]W[2],b[2],W[1],b[1]=𝗂𝗇𝗂𝗍𝗂𝖺𝗅𝗂𝗓𝖾𝖱𝖺𝗇𝖽𝗈𝗆𝗅𝗒()W[2]=W[2]−α∂J∂W[2]b[2]=b[2]−α∂J∂b[2]W[1]=W[1]−α∂J∂W[1]b[1]=b[1]−α∂J∂b[1]𝗀𝗋𝖺𝖽𝗂𝖾𝗇𝗍𝖣𝖾𝗌𝖼𝖾𝗇𝗍(D,α):\n𝗋𝖾𝗍𝗎𝗋𝗇W[1],W[2],b[1],b[2]𝖼𝗈𝗆𝗉𝗎𝗍𝖾J(W[1],b[1],W[2],b[2])\nJ(W[1],b[1],W[2],b[2])=1mm∑i=1L(̂y(i),y(i))\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]\n∂J∂b[1]=1m(∂L(̂y(1),y(1))∂b[1]+…+∂L(̂y(m),y(m))∂b[1])∂J∂W[1]=1m(∂L(̂y(1),y(1))∂W[1]+…+∂L(̂y(m),y(m))∂W[1])∂J∂b[2]=1m(∂L(̂y(1),y(1))∂b[2]+…+∂L(̂y(m),y(m))∂b[2])∂J∂W[2]=1m(∂L(̂y(1),y(1))∂W[2]+…+∂L(̂y(m),y(m))∂W[2])Dérivation: www.efavdb.com/backpropagation-in-neural-networks\nCalcul du gradient: règle du chaînage sur un graphe de dépendances"
      }
    ]
  },
  {
    "id": "a6f52f38-ec3b-445a-940f-7f73ea7af7a0",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_17.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartConséquence de la non-convexité\n17\nQuelle est la conséquence majeure de la non-convexité de la fonction de coût ?Réalité: on risque de converger dans un minimum localAnalogie: utilisation de métaheuristiques dans une recherche localeConséquence: on doit maintenant faire attention à la façon dont on entraîne le réseauObjectif: faciliter l’apprentissage aﬁn de converger vers un bon minimum local\nQuelles sont les actions que nous pouvons prendre pour faciliter l’apprentissage ?(1) Remplacer la sigmoide par une autre fonction (tanh, ReLU, LeakyReLU, etc.)(2) Initialiser adéquatement les paramètres (bonne solution initiale)(3) Utiliser un autre algorithme que la descente de gradient (SGD, Adam, etc.)(4) Rajouter des opérations complémentaires dans le réseau (normalisation, BatchNorm)(5) Sélectionner adéquatement certains hyper-paramètres (taux d’apprentissage, etc.)\nIl y a encore continuellement des nouvelles idées qui sont introduites par la communauté scientiﬁque"
      }
    ]
  },
  {
    "id": "9c80a43f-0112-445b-ae69-8af14a50bdf5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_18.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n18Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "d720951a-16e2-4b25-98f7-d28b4377d270",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_19.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]Limitations de la sigmoide \n19Situation actuelle: une fonction sigmoide est appliquée à la sortie de chaque neuroneObjectif initial: obtenir une valeur entre 0 et 1 (prédiction d’une probabilité)Limitation: ce choix est non justiﬁé pour les neurones intermédiairesPertinence du choix: correct pour le neurone de la dernière couche\nA t-on d'autres alternatives ?\nPire encore: ce choix amène à des piètres performancesTransformation déﬁnissant la valeur de sortie un neurone après la combinaison des entréesFonction d’activation"
      }
    ]
  },
  {
    "id": "057b237e-758d-45eb-8b6a-fb95a3e2087c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_20.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation\n20Fonction sigmoide: fonction d'activation possible, mais il en existe plein d'autresContrainte 1: la fonction activation doit être non-linéaire (explication à venir)Tangente hyperbolique (tanh)Rectiﬁed linear unit (ReLU)Variantes du ReLU (Leaky ReLU, Exponential ReLU, etc.)Et plein d'autres... (Maxout, etc.)Lecture complémentaire: https://cs231n.github.io/neural-networks-1/#actfunContrainte 2: la fonction doit être principalement diﬀérentiable (calcul de la descente de gradient)\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])̂Y=A[2]Idée: considérer une autre fonction que la sigmoide comme fonction activationIl existe une multitude de fonctions d’activation dans la littérature\nRaison principale: chacune a ses forces et faiblesses ainsi que leur champ d’application"
      }
    ]
  },
  {
    "id": "7a747849-6674-419b-888b-3e9a2c5063e2",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_21.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation: la sigmoide\n21\ng(z)=11+e−zdg(z)dz=g(z)(1−g(z))Fonction sigmoideGradient de la sigmoide\nApplication: neurone de la dernière couche pour une classiﬁcation binaire (prédiction d'une probabilité)Fonction sigmoide: compresse n'importe quelle valeur dans un intervalle entre 0 et 1Diﬃculté: la fonction sature et le gradient quasi-nul pour des grosses valeurs (ralenti l'apprentissage)Diﬃculté: implique des opérations coûteuses (calcul d'exponentielles)Diﬃculté: Les valeurs de sortie sont toujours positivesConséquence: la descente de gradient aura tendance à zigzagger (ralenti l'apprentissage)\nW[1]=W[1]−α∂J∂W[1]\nRecommendation: utilisation non conseillée, sauf pour la sortie d'une classiﬁcation binaire"
      }
    ]
  },
  {
    "id": "47f25c4d-8d43-40e0-8d6a-ad6919cb580f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_22.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartValeurs positives de la sigmoideRegardons l'impact sur le neurone de la dernière couche\nz[2]=w[2]1a[1]1+w[2]2a[1]2+b[2]Avec a[1]1 et a[1]2 positifs (sortie d'une sigmoide)Etape 2: analysons les dérivées partielles de la fonction d'écart\n22\nz[1]=W[1]x+b[1]a[1]=σ(z[1])z[2]=W[2]a[1]+b[2]a[2]=σ(z[2])̂y=a[2]\nEtape 1: analysons les dérivées partielles de ce neurone pour les diﬀérents poids (w)A ce stade, on sait que ces 2 valeurs sont positives∂L(̂y,y)∂w[2]1=∂L(̂y,y)∂z[2]∂z[2]∂w[2]1=a[1]1∂L(̂y,y)∂z[2]∂L(̂y,y)∂w[2]2=∂L(̂y,y)∂z[2]∂z[2]∂w[2]2=a[1]2∂L(̂y,y)∂z[2]Valeur positiveMême valeurMise en situation: réseau à deux neurones cachés\nConséquence: donne un zigzag visuel dans la descente de gradient, ce qui ralenti l’apprentissageObservation: les deux dérivées partielles ont le même signeImplication: on a le même sens de descente pour ces deux paramètresw[2]1=w[2]1−α∂J∂w[2]1\nw[2]2=w[2]2−α∂J∂w[2]2\nMême signe\n∂z[2]∂w[2]1=a[1]1\n∂z[2]∂w[2]2=a[1]2"
      }
    ]
  },
  {
    "id": "ea47d255-8b26-4abe-8671-848dabc1347c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_23.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nFonction d'activation: tangente hyperbolique (tanh)\n23Tangente hyperboliqueGradient de tanh\nAvantage: les valeurs de la fonction sont centrées en 0 (mitige l'eﬀet de zigzag)Tangente hyperbolique: compresse n'importe quelle valeur dans un intervalle entre -1 et 1g(z)=tanh(z)dg(z)dz=1−tanh2(z)tanh(z)=ez−e−zez+e−z\nDiﬃculté: la fonction sature et le gradient quasi-nul pour des grosses valeurs (ralenti l'apprentissage)Diﬃculté: implique des opérations coûteuses (calcul d'exponentielles)\nRecommendation: fonction quasi-toujours préférable à la fonction sigmoide pour une couche cachée"
      }
    ]
  },
  {
    "id": "d8cc7289-6f45-422f-8fc7-133a6a21d141",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_24.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation: Rectiﬁed linear unit (ReLU)\n24Avantage: ne sature pas dans la région positive (convergence accélérée)ReLU: fonction linéaire par morceaux qui met toutes les valeurs négatives à 0\ng(z)=max(0,z)\nDeep sparse rectiﬁer neural network [Glorot et al. 2011]\nNotez l'échelle de l'axe des ordonnéesDérivée non déﬁnie en 0Pas un problème en pratiquedg(z)dz=8><>:0i fz<01i fz>0?ifz=0\nAvantage: opérations peu coûteusesCause: le gradient est nul dans la zone négative (rien ne sera appris pour ce neurone)Mitigation possible: prendre un taux d'apprentissage pas trop grandDiﬃculté: Les valeurs de sortie ne sont jamais négatives (eﬀet de zigzag)Diﬃculté: une descente trop forte peut amener une valeur dans la zone négative (neurone mort)\nRecommendation: très bon choix de fonction d’activation (recommandé pour vos premiers modèles)\nW[1]=W[1]−α∂J∂W[1]"
      }
    ]
  },
  {
    "id": "6818c1e5-6ad9-4ec8-9271-879890047ad4",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_25.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartNon-linéarité des fonctions d'activation\n25\nOn souhaite avoir une activation non-linéaire. Pourquoi est-ce important ? Exemple: considérons une activation linéaire pour les neurones cachésFonction identité: g(z)=zAnalyse: regardons l'impact dans un réseau dédié à une prédiction de probabilité\nZ[2]=W′ X+b′ ̂Y=σ(Z[2])Observation: le réseau s'eﬀondre en une combinaison linéaire des entrées XCause: la composition de deux fonctions linéaires est aussi linéaire\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]Dimension de W[2]W[1]:(1×3).(3×2)=1×2Dimension de W[2]b[1]+b[2]:(1×3).(3×1)+(1×1)=1×1\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=W[2](W[1]X+b[1])+b[2]A[2]=σ(Z[2])̂Y=A[2]\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=(W[2]W[1])X+(W[2]b[1]+b[2])A[2]=σ(Z[2])̂Y=A[2]\nSans non-linéarité, le modèle se réduit à régression logistique"
      }
    ]
  },
  {
    "id": "23d80638-3fd6-4265-ae46-d22e6b466bc5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_26.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivité des réseaux de neurones\n26Un réseau de neurones avec des fonctions d'activation non-linéaire peut théoriquement approximer avec une précision arbitraire n'importe quelle fonction réelleUniversalité des réseaux de neurones\nRésultat théorique important: justiﬁe le pouvoir expressif des réseaux de neurones (avec des non-linéarités)Terminologie: on dit également qu’un réseau de neurones est un approximateur universelNote: cela est également vrai avec une activation sigmoide et seulement une couche cachéeApproximation by superpositions of a sigmoidal function [Cybenko, 1989]Diﬃculté pratique: le réseau peut-être diﬃcile à entraîner (d’où les mécanismes supplémentaires)Diﬃculté pratique: ce résultat peut nécessiter un nombre immense de neurones"
      }
    ]
  },
  {
    "id": "939ffc91-3bae-4fd4-a5b6-8019a834b56a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_27.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nFonction d'activation de la dernière couche\n27\nPeut-on utiliser la même fonction d’activation pour tous les neurones ? Objectif de la dernière couche: déﬁnir le type de prédiction à obtenir\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])̂Y=A[2]En très grande majorité: oui, vous pouvez utiliser la même fonction partoutException: l’activation de la dernière couche a une importance particulière \nChoix adapté: fonction sigmoïdeTâche 2: classiﬁcation binaireObjectif: obtenir une probabilité de succès (valeur entre 0 et 1)Règle d’or: la dernière activation doit retourner une valeur cohérente avec votre tâche de prédiction\nFonction de perte liée: entropie croisée binaireTâche 1: régression\nChoix adapté: fonction identité (linéaire)Objectif: obtenir une valeur réelleFonction de perte liée: erreur quadratiqueNote: il s’agit d’un des rares cas ou vous devez utiliser une activation linéaire\ng[2](z)=σ(z)=11+e−z\ng[2](z)=z"
      }
    ]
  },
  {
    "id": "2905500d-3863-4b75-9a88-10449f6038cc",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_28.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation de la dernière couche\n28Tâche 3: classiﬁcation multi-classe\nObjectif: eﬀectuer une prédiction entre plusieurs classes d'objetsExtension naturelle de la classiﬁcation binaire\nComment eﬀectuer une prédiction multi-classe ? Etape initiale: supposons que l'on ait C classes de prédictionVraie valeur (y): correspond à une catégorie c∈CIdée: prédire une probabilité d’appartenance à chaque classe (de façon similaire au cas binaire)\nOiseauChienChaty=(100)Encodage possible: un vecteur de taille C×1 avec un 1 à la bonne classe et 0 sinon (one-hot encoding)Objectif: prédire la probabilité d'appartenance à chaque classePrédiction: correspond à un vecteur ̂y∈[0,1]C\n̂y=0.700.260.04Remarque: la somme des éléments de ce vecteur doit valoir 1 (ce sont des probabilités)\nComment obtenir un vecteur satisfaisant cette contrainte ?"
      }
    ]
  },
  {
    "id": "ce92b5f5-c801-43bf-bd96-1dcb7bee73e5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_29.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation softmax\n29Exemple: z=(542)→̂y=e5e5+e4+e2e4e5+e4+e2e2e5+e4+e2=0.700.260.04Intérêt: permet de réaliser des classiﬁcations avec plusieurs classes Fonction qui transforme un vecteur de C réels en entrée, en un vecteur de C probabilitésFonction softmax\n𝖲𝗈𝖿𝗍𝗆𝖺𝗑:ℝC→[0,1]C (avec la somme des outputs qui vaut à 1)̂yj=𝖲𝗈𝖿𝗍𝗆𝖺𝗑(zj)=ezj∑Cc=1ezc\nClassiﬁcation ﬁnale: eﬀectuer la prédiction en choisissant la classe ayant la plus haute probabilitéUtilisation: à la dernière couche d’un réseau de neurones pour une tâche de classiﬁcation multi-classeImpact sur le réseau: la dernière couche aura un nombre de neurones qui équivaut au nombre de classesIntuition: chaque neurone de la dernière couche va prédire une probabilité propre à la classe"
      }
    ]
  },
  {
    "id": "ac727b4e-f8d7-4999-942e-b636f2562f10",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_30.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\n̂y=0.700.260.04≈y=(100)Fonction d’écart pour une classiﬁcation multi-classe\n30\nComment caractériser la qualité d'une classiﬁcation multi-classe ?Encore une fois, cela revient à déﬁnir une fonction d’écart appropriée à cette tâcheClassiﬁcation binaire: utilisation de l’entropie croisée binaire Classiﬁcation multi-classe: utilisation de l’entropie croisée\nL(̂y,y)=−(ylog(̂y)+(1−y)log(1−̂y))Fonction d’écart communément utilisée pour des tâches de de classiﬁcation multi-classeErreur de l’entropie croisée (categorical cross entropy loss)\nL(̂y,y)=−C∑c=1yclog(̂yc) avec ̂yc∈[0,1] et C∑c=1̂yc=1C:le nombre de classes présentesyc:valeur du vecteur y qui vaut 1 si c correspond à la bonne classe (0 sinon)̂yc:valeur du vecteur ̂y qui correspond à la probabilité prédite de la classe cIntuition: exactement le même principe que le cas binairePrincipe: minimiser la fonction pousse (̂y→1) pour la bonne classe\nFonction de coût: écart moyen de toutes les prédictions"
      }
    ]
  },
  {
    "id": "4f7aa3ef-3940-4c77-93bc-fcc41287da75",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_31.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n31Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "36f4cad3-5f36-48b5-85da-468ff057098e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_32.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\n∂J∂W[1]=(uvuvuv)\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]Initialisation des paramètres: valeur identique\n32\nQue pensez-vous de cette idée ?\nContre-exemple: essayons avec une valeur initiale de 0 pour tous les paramètres\nW[1]=(000000)\nQue peut-on dire de la valeur de sortie de ces 3 neurones ?\nConclusion: on n’est pas plus expressif qu'un seul neurone dans la couche\nIdée 1: initialiser tous les paramètres à la même valeurRegardons ce qui se passe à la couche cachéeValeurs de sorties: identiques pour les 3 neuronesConséquence: rien ne diﬀérencie les neurones, ils calculent la même fonctionPasse en arrière: les trois gradients vont par conséquent avoir la même valeurValeurs du gradient liées au 3e neurone\nb[1]=(000)\na[1]=σ(W[1]x+b[1])\na[1]1=a[1]2=a[1]3"
      }
    ]
  },
  {
    "id": "54693971-f160-44ba-9478-5e5b5decd9ce",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_33.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartInitialisation des paramètres: valeur aléatoire quelconque\n33Intuition: permet de corriger la diﬃculté précédente car le gradient lié à chaque neurone sera diﬀérent\nEst-ce que n'importe quelle valeur est bonne ?Descente de gradient: demande de calculer le gradient d'une fonction sigmoide (version de base)\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]\nIdée 2: initialiser tous les paramètres à une valeur aléatoire quelconque\nQue se passe t-il si les paramètres ont une grande valeur (positive ou négative) ?\nTrès petite valeurObservation: on se retrouve sur une région assez plate de la sigmoide (pente très faible)Conséquence théorique: la valeur du gradient sera par très petiteConséquence pratique: l’entraînement sera lent\nW[1]=W[1]−α∂J∂W[1]"
      }
    ]
  },
  {
    "id": "5f8a312c-eaa1-49c5-b7d1-b319e2476a8a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_34.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartInitialisation des paramètres: valeur aléatoire proche de zéro\n34Intuition: permet de corriger la diﬃculté précédente en permettant des plus grands pas de gradient\nZ[1]=W[1]X+b[1]A[1]=σ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=σ(Z[2])̂Y=A[2]\nIdée 3: initialiser tous les paramètres à une valeur aléatoire proche de zéroL’objectif est d’être dans cette zoneNote: il n’y a aucun problème à initialiser les biais (b) à zéroApproche simple: initialiser les paramètres aléatoirement dans un intervalle entre -1 et 1Autres fonctions d’initialisation (hors matière)Cette initialisation est simple et donne de bons résultats en pratique pour de petits réseaux\nObjectif: aider à la stabilisation du gradient dans des réseaux à beaucoup de couches\nInitialisation de Xavier: W[l]∼𝒩(μ=0,σ2=1n[l−1]) (pour une activation tanh)Idée: changer la façon d’initialiser en fonction de la couche considérée\nInitialisation de He: W[l]∼𝒩(μ=0,σ2=2n[l−1]) (pour une activation ReLU)"
      }
    ]
  },
  {
    "id": "8bcc3fe2-b9a2-4295-87ea-45bac3ce96b6",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_35.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVisualisation de l’impact de l’initialisation\n35Lecture complémentaire: https://www.deeplearning.ai/ai-notes/initialization/index.htmlUnderstanding the diﬃculty of training deep feedforward neural networks [Glorot and Bengio, 2010]Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation [He et al., 2015]Architecture: réseau de neurones à 4 couches avec des activations de type tanhTâche: classiﬁcation de chiﬀres écrits à la main (même tâche que pour le devoir)\nAnalyse: impact de l’initialisation des paramètres sur les valeurs des activations à chaque couche\nLe signal globalement nulLe signal s’estompeLa signal reste stable"
      }
    ]
  },
  {
    "id": "f9172e4d-5c52-4c87-80d4-3895fd442d31",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_36.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nRécapitulatif: réseau de neurones\n36\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])̂Y=A[2]z1=w[1]T1x+b[1]1a1=σ(z1)a1x2x1\nPasse en avant: prédiction via les équations fondamentalesConcepts clefsNotation matricielle: expression compacte et plus eﬃcaceArchitecture: neurones structurés en plusieurs couchesEntraînement: descente de gradient et passe en arrièreFonction d’activation intermédiaire: non-linéaireDernière activation: dépend de la tâche de prédictionInitialisation des paramètres: aléatoire et proche de 0Diﬃculté: la fonction de coût n’est plus convexe\n∂J∂b[1]=1m(∂L(̂y(1),y(1))∂b[1]+…+∂L(̂y(m),y(m))∂b[1])∂J∂W[1]=1m(∂L(̂y(1),y(1))∂W[1]+…+∂L(̂y(m),y(m))∂W[1])∂J∂b[2]=1m(∂L(̂y(1),y(1))∂b[2]+…+∂L(̂y(m),y(m))∂b[2])∂J∂W[2]=1m(∂L(̂y(1),y(1))∂W[2]+…+∂L(̂y(m),y(m))∂W[2])\nJ(W[1],b[1],W[2],b[2])=1mm∑i=1L(̂y(i),y(i))"
      }
    ]
  },
  {
    "id": "c6943869-622c-43f6-9786-04597ee1b836",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_37.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVisualisation de la prédiction\n37\nhttps://playground.tensorﬂow.org\nCaractéristiques en entrée: coordonnée en x, coordonnée en yActivation: tangente hyperboliqueTaux d’apprentissage: 0.01Chaque neurone apprend une fonction diﬀérente Prédiction plus complexe\nTâche: classiﬁcation binaire de points 2D"
      }
    ]
  },
  {
    "id": "ec508b3b-b183-46ad-90da-25cfa34f820c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_38.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n38Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "56f43cd9-b2a5-44df-b42f-aa32bbcc96d1",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_39.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartLimitation de notre réseau de neurones\n39\nQuelles sont les limitations de notre réseau de neurones actuel ?\nLimitation: il peut manquer d'expressivité pour approximer des fonctions complexesFigure de droite: la séparation avec le même réseau devient diﬃcileConclusion: il faut une fonction de prédiction plus expressive !Figure de gauche: bonne séparation avec un réseau à deux couches\nPour cela, on va simplement augmenter le nombre de couches dans le réseau"
      }
    ]
  },
  {
    "id": "cf3cbfe6-8e3c-4529-a105-3f1135fa7b13",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_40.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartApprentissage profond (deep learning)\n40\nTerminologie: le terme profond fait référence au nombre de couches du réseauObjectif: construire des fonctions de prédiction plus expressives\nContenu du module: on a formalisé les réseaux de neurones de façon génériqueAvantage: l’extension à l’apprentissage profond va être TRES facileConceptuellement, il n'y a pas de profondeur limite (bien que des diﬃcultés apparaissent)Exemple: le réseau utilisé dans AlphaZero a environ 80 couches, celui de GPT-4 a 120 couchesApprentissage profond (deep learning): utiliser un réseau avec plus d’une couche cachée\nPour faire simple, on a déjà fait tout le nécessaire…"
      }
    ]
  },
  {
    "id": "c51730a0-0312-40e3-a314-a564df1becd3",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_41.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartNotations pour le deep learning\n41Notationsn[l]: nombre de neurones pour la couche lW[l],b[l]: paramètres de la couche lX: caractéristiques en entrée (format matriciel)L: nombre de couches (couche d'entrée non comprise)A[l]: sortie de la couche lZ[l]: valeurs avant l'activation de la couche l̂Y: sortie du réseau (prédiction)\nExercice: donnez les dimensions de chaque matrice pour 10 données d’entraînementExercice: donnez le nombre de paramètres à apprendre dans ce réseau"
      }
    ]
  },
  {
    "id": "0482803f-7497-4275-8b7f-0dccea88d2a6",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_42.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartEquations fondamentales de l'apprentissage profond\n42\nZ[l]=W[l]A[l−1]+b[l]A[l]=g[l](Z[l])̂Y=A[L]𝚏𝚘𝚛l∈{1,…,L}:A[0]=X\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])̂Y=A[2]\nSituation précédente: réseau de neurones à 2 couches\nSituation actuelle: réseau de neurones à L couches\nIl n’y a vraiment rien de nouveau conceptuellement par rapport à précédemment"
      }
    ]
  },
  {
    "id": "d5892307-cd53-44dc-bf7b-03555f92dfe5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_43.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDescente de gradient dans un réseau de neurones profond\n43\n𝖼𝗈𝗆𝗉𝗎𝗍𝖾̂Y𝗋𝖾𝗉𝖾𝖺𝗍𝗎𝗇𝗍𝗂𝗅𝖼𝗈𝗇𝗏𝖾𝗋𝗀𝖾𝗇𝖼𝖾:𝖼𝗈𝗆𝗉𝗎𝗍𝖾∂J∂W[L],∂J∂b[L],…,∂J∂W[1],∂J∂b[1]W[l],b[l]=𝗂𝗇𝗂𝗍𝗂𝖺𝗅𝗂𝗓𝖾𝖱𝖺𝗇𝖽𝗈𝗆𝗅𝗒()∀l∈{1,…,L}W[l]=W[l]−α∂J∂W[l]∀l∈{1,…,L}b[l]=b[l]−α∂J∂b[l]∀l∈{1,…,L}𝗀𝗋𝖺𝖽𝗂𝖾𝗇𝗍𝖣𝖾𝗌𝖼𝖾𝗇𝗍(D,α):\n𝗋𝖾𝗍𝗎𝗋𝗇W[1],b[1],…,W[L],b[L]𝖼𝗈𝗆𝗉𝗎𝗍𝖾J(W[1],b[1],…,W[L],b[L])\nJ(W[1],b[1],…,W[L],b[L])=1mm∑i=1L(̂y(i),y(i))Calcul des dérivées par un graphe de dépendance  et une application de la passe en arrière\n̂Y=A[L]\nZ[l]=W[l]A[l−1]+b[l]A[l]=g[l](Z[l])𝚏𝚘𝚛l∈{1,…,L}:A[0]=X\n∂J∂b[l]=1m(∂L(̂y(1),y(1))∂b[l]+…+∂L(̂y(m),y(m))∂b[l])∂J∂W[l]=1m(∂L(̂y(1),y(1))∂W[l]+…+∂L(̂y(m),y(m))∂W[l])𝚏𝚘𝚛l𝚒𝚗L𝚝𝚘1:Nombre de paramètresW[1]=n[1]×n[0]=5×4=20b[1]=n[1]×1=5×1=5W[2]=n[2]×n[1]=6×5=30W[3]=n[3]×n[2]=6×6=36W[4]=n[4]×n[3]=5×6=30W[5]=n[5]×n[4]=4×5=20W[6]=n[6]×n[5]=1×4=4b[2]=n[2]×1=6×1=6b[3]=n[3]×1=6×1=6b[4]=n[4]×1=5×1=5b[5]=n[5]×1=4×1=4b[6]=n[6]×1=1×1=1Total: 167 paramètres à apprendre"
      }
    ]
  },
  {
    "id": "52799909-d404-4c7c-8e63-a72c7ba6735a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_44.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivité de l'apprentissage profond\n44Intérêt principal: permet de construire des fonctions plus expressives\nSimple relation (linéaire)Relation  plus complexeRelation encore plus complexeObservation: les neurones apprennent des relations de plus en plus complexe avec le niveau de profondeurhttps://playground.tensorﬂow.org\nQu’en est-il des fonctions encore plus compliquées ?"
      }
    ]
  },
  {
    "id": "2dc31c92-58ac-4471-892b-a1d13e6a6946",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_45.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivité de l'apprentissage profond\n45Situation à résoudre: assez compliquée, et demande une bonne fonction d’approximationSolution: en augmentant la taille du réseau, on arrive à avoir des meilleures frontières de décision\nExpérimentez par vous même en essayant d'autres fonctions d'activation et taux d'apprentissagehttps://playground.tensorﬂow.orgNeurone mort avec une activation ReLUObservation: une activation ReLU a été utilisée, et un neurone est mort"
      }
    ]
  },
  {
    "id": "7231c6a7-b309-4386-bf9a-01044e2ab389",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_46.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n46Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "e7d3fe99-d40b-4138-bda5-5c68ff0514fd",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_47.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartIntroduction aux hyper-paramètres\n47\nComment déterminer l'architecture du réseau à utiliser ?Cette question amène à diﬀérencier paramètres et hyper-paramètresParamètres: valeurs qui sont apprises durant la descente de gradient\nW[l]: poids de la couche lb[l]: biais de la couche l\nExemple: nombre de couches, fonction d’activation, etc.Autre choix à faire: taux d’apprentissageIntuition: il s’agit des valeurs qui vont déterminer la fonction que vous construisezValeurs déﬁnies par l'utilisateur utilisées pour contrôler l'apprentissage et le type de modèleHyperparamètre\nPoint de vue de l’utilisateur: il n’a aucun contrôle sur la valeur des paramètres\nObjectif idéal: trouver les hyper-paramètres qui vont résulter au modèle le plus performantImportance: les valeurs choisies ont un impact sur la qualité de l’apprentissage et sur le modèle obtenu(1) Nombre de couches(2) Nombre de neurones par couche(3) Fonction d'activation utilisée(4) Taux d'apprentissageEt encore tout plein !"
      }
    ]
  },
  {
    "id": "2b1f0ac1-93de-40c9-a907-d22e311d948a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_48.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartSélection de la valeur des hyper-paramètres\n48\nDiﬃculté de la tâche: il y a énormément de combinaisons à testerTaux d'apprentissage: {0.0001,0.001,0.01,0.1}Nombre de couches: {1,2,3,4,5}Fonction d'activation: {ReLU,tanh,sigmoid}Mise en situation: exemple très petit avec quelques choix à faireTotal: 4×5×3=60 conﬁgurations à testerLien avec le module 3: trouver la meilleure conﬁguration est un problème d’optimisation combinatoireDiﬃculté supplémentaire: évaluer chaque conﬁguration est coûteux car cela demande d’entraîner un réseauConseils pratiquesEtape 1: ﬁxez les valeurs standards dans un premier temps (par exemple une activation ReLU)Etape 2: procédez itérativement et analysez les résultats de votre modèleExemple: une mauvaise précision sur les données d'entrainement peut être dû à un manque d'expressivitéAction possible: il serait pertinent d’essayer d’augmenter la profondeur du réseauAlgorithme de recherche d'hyper-paramètresRecherche exhaustive (grid search)Recherche aléatoire (random search)Recherche locale et métaheuristiquesOptimisation bayésienneHyperparameter search in machine learning [Claesen and De Moor, 2015]"
      }
    ]
  },
  {
    "id": "e31b7cb7-9aa7-4ff2-9c92-633baa57f501",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_49.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAutres conseils pratiques\n49Diﬃcultés et pièges habituels\n(1) Confondre les dimensions des matrices(2) Oublier d'initialiser les poids aléatoirement(3) Avoir un taux d'apprentissage trop grand (surtout avec ReLU)(4) Oublier d'ajouter une fonction d’activation non-linéaire(5) Avoir une fonction d'activation non adaptée au problème à la dernière couche(6) Utiliser une fonction de coût non adaptée au problèmeEvaluation rigoureuse et analyse de vos réseauxBonne pratique: diviser vos données en plusieurs ensembles (entraînement, validation, et de test)Notion de sous-apprentissage (underﬁtting): diﬃculté à donner de bons résultats sur des données vuesNotion de sur-apprentissage (overﬁtting): diﬃculté à donner de bons résultats sur des données non-vuesCes considérations seront à prendre en compte lors du devoir 3 et discutées lors de votre laboratoire\nJe vous conseille Keras pour débuter (très facile d’utilisation)Librairies pour l'apprentissage profondObjectif 1: faciliter la création de réseaux pour un utilisateurObjectif 2: automatiser l’entraînement d’un modèle"
      }
    ]
  },
  {
    "id": "d040c756-a7ab-4afa-a2ae-737bcf8c3d1f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_50.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nAutres architectures existantes\n50Il existe également d’autres architectures de réseau de neuronesIntérêt: avoir une architecture adaptée aux données en entrées\nGradient-based learning applied to document recognition [LeCun, Bengio et al., 1998]\nRéseau récurrent: spécialisé pour l'apprentissage sur des séquences de donnéesRéseau à convolution: spécialisé pour l'apprentissage sur des images\nLong short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997]Neural networks and physical systems with emergent collective computational abilities [Hopﬁeld, 1982]\nEt d’autres: réseau à convolution de graphe, architecture adversarielle, modèles génératifs, etc."
      }
    ]
  },
  {
    "id": "fc91ca7b-ff68-49d1-9d6b-c4402e453cfa",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_51.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDiﬃcultés de l'apprentissage profond et alternatives\n51\nQuelles sont les alternatives aux réseaux de neuronesBesoin de l’apprentissage profond: trois composants essentiels(1) Une méthode d’apprentissage (ce qu’on a vu dans ce module)(2) Un nombre conséquent de données labellisées(3) De la puissance de calculDans les deux cas, si un de ces ingrédients manque, le résultat (ou performances) en sera aﬀecté\nLes besoins (2) et (3) augmentent avec la complexité du réseauAnalogie: le triangle du feu donnant les trois besoins pour avoir un feu\nIl existe une multitude d’autres méthodes pour l’apprentissage superviséArbres de décisionForêts aléatoiresClassiﬁcateur bayésienSupport vector machine (SVM)Modèles probabilistesGradient boosting (XGBoost)Conclusion: en tout temps, il est pertinent de rester ouvert à d’autres méthodesConclusion: les success stories de l’apprentissage profond sont souvent liées à des ressources immenses"
      }
    ]
  },
  {
    "id": "6f155cfb-55da-4157-851a-64dda80a2f94",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_52.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemple récent de réseau profond: Dall-E 2\n52\nNombre de paramètres à apprendre: plus de 3.5 millards de paramètresNombre de données: environ 650 millions imagesTemps d’entraînement: information malheureusement très diﬃcile à trouver, mais très conséquent"
      }
    ]
  },
  {
    "id": "076a1ec1-6d90-4cb7-891b-95934301e141",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_53.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matières\n53Réseaux de neurones1. Concepts principaux des réseaux de neurones 2. Extension de la régression logistique 3. Notations standards des réseaux de neurones 4. Exécution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramètresApprentissage profond1. Limitation des petits réseaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un réseau de neurones 4. Introduction aux hyper-paramètres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "1330086c-5cdb-431a-978d-ec4ff2055eb5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_54.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nSynthèse des notions vues\n54Réseaux de neurones\nEntraînement: descente de gradient et passe en arrièreDiﬃculté: la fonction de coût n’est plus convexe\nDéﬁnition: ensemble de neurones organisés en un réseauIntérêt: chaque neurone a ses paramètres et contribue à la prédictionApproximateur universel: capacité d'approximer n'importe quelle fonctionBesoin: activations non-linéairesApprentissage profondDéﬁnition: réseau de neurones ayant plus d'une couche cachéeIntérêt: permet de réaliser des approximations plus précisesTrois ingrédients essentiels:\n(1) Une méthode d’apprentissage(2) Un nombre conséquent de données labellisées(3) De la puissance de calculConséquence: utilisation de mécanismes pour faciliter l’apprentissage"
      }
    ]
  },
  {
    "id": "f9448eda-7d92-4e7f-b227-888fd4a8d10d",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_55.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAller plus loin...\n55Cours donnés à PolytechniqueINF8460: Traitement automatique de la langue naturelle (Amal Zouaq)\nINF8245E: Machine learning (Sarath Chandar)\nINF8225: I.A.: techniques probabilistes et d'apprentissage (Chris pal)\nCours en ligne\nFastAI: Practical deep learning for codersCoursera: Machine learning (Andrew Ng)Coursera: Deep learning (Andrew Ng)https://course.fast.ai/https://www.coursera.org/learn/machine-learninghttps://www.coursera.org/specializations/deep-learning\nAutres ressourcesEnormément de ressources sur le web (blog, vidéo youtube, tutoriel, etc.)\nhttps://www.youtube.com/results?search_query=code+emporium\nhttps://distill.pub/"
      }
    ]
  },
  {
    "id": "81eb1162-375b-4afc-a56e-06a78f65fd94",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_56.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemples de questions d'examen\n561. Indiquer le nombre de paramètres à entraîner pour un réseau de neurones donné 2. Indiquer la taille des matrices dans une représentation matricielle 3. Donnez les équations de la forward pass pour un réseau de neurones donnéThéoriePratique1. Expliquer ce qu'est un réseau de neurones comparé à une régression logistique 2. Expliquer l'importance d'avoir une fonction d'activation non-linéaire 3. Expliquer l'intérêt d'initialiser aléatoirement les paramètres 4. Expliquer la diﬀérence entre paramètres et hyper-paramètres"
      }
    ]
  },
  {
    "id": "ae733706-00b1-4d48-9b0c-9aadbb51b650",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_57.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiﬁcielleMéthodes et algorithmesRéseaux de neurones et apprentissage profond: FIN\nDALLE: Neural network, digital art"
      }
    ]
  }
]