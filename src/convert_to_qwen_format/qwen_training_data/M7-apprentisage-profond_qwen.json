[
  {
    "id": "59101b20-1c70-44cc-adc5-0d5a8d639d3e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_1.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesModule 7: RÃ©seaux de neurones et apprentissage profond"
      }
    ]
  },
  {
    "id": "0c42dbfb-508e-4d61-9261-efb2f0f6fa1a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_2.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartContenu du cours\n2ConsidÃ©rations pratiques et sociÃ©talesModule 10: Utilisation en industrie, Ã©thique, et philosophie Raisonnement par recherche (essais-erreurs avec de l'intuition)Module 1: StratÃ©gies de recherche Module 2: Recherche en prÃ©sence d'adversaires Module 3: Recherche locale \nRaisonnement par apprentissageModule 6: Apprentissage supervisÃ© Module 7: RÃ©seaux de neurones et apprentissage profond Module 8: Apprentissage non-supervisÃ© Module 9: Apprentissage par renforcement \nRaisonnement logiqueModule 4: Programmation par contraintes Module 5: Agents logiques"
      }
    ]
  },
  {
    "id": "1c4a2559-abc0-4140-9c87-71897317b07d",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_3.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n3RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "8f5aeb3f-78a4-4ac9-937f-e4ce06962b77",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_4.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRappel de la rÃ©gression logistique\n4z=w1x1+â€¦+wnxn+bx1bwnw1xn..L(a,y)a=Ïƒ(z)IntÃ©rÃªt: permet de crÃ©er une frontiÃ¨re de dÃ©cision linÃ©aireEtape 1: on prend une combinaison linÃ©aire des caractÃ©ristiquesEtape 2: on applique une sigmoide sur cette combinaison\nAlgorithme dâ€™apprentissage: descente de gradient avec une passe en avant et en arriÃ¨re\nHypothÃ¨se: une tendance linÃ©aire entre les donnÃ©es peut Ãªtre Ã©tablieCe nâ€™est malheureusement pas souvent le cas en pratiqueBesoin: on doit construire une fonction (hypothÃ¨se) plus expressive\nLes rÃ©seaux de neurones permettent de combler ce manque\nUne rÃ©gression logistique construit d'une fonction prÃ©disant une valeur entre 0 et 1 (module prÃ©cÃ©dent)"
      }
    ]
  },
  {
    "id": "b3fce122-df51-46ed-8da2-341c64652636",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_5.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAperÃ§u d'un rÃ©seau de neurones\n5z=w1x1+â€¦+wnxn+bx1bwnw1xn...a=Ïƒ(z)\nxn...x1a Ensemble de neurones organisÃ©s en un rÃ©seau de plusieurs couchesRÃ©seau de neurones\nSans le savoir, on a dÃ©jÃ  construit nos premiers neurones !\nQuâ€™est-ce quâ€™un neurone dans ce contexte ?Neurone actuel: combinaison linÃ©aire de variables, Ã  laquelle est appliquÃ©e une sigmoÃ¯de\nPar la suite: on verra comment crÃ©er d'autres types de neurones (non basÃ© sur une sigmoÃ¯de)ReprÃ©sentation visuelle: les paramÃ¨tres Ã  apprendre (w1, ..., wn, b) sont souvent implicitesExemple de neurone\nRÃ©seau: les neurones sont organisÃ©s en un rÃ©seau de plusieurs couches (layers)Intuition: chaque neurone apporte sa propre contribution pour la prÃ©dictionParamÃ¨tres: chaque neurone a ses propres paramÃ¨tres Ã  apprendre (w et b)ConsÃ©quence: on a un modÃ¨le plus expressif quâ€™une rÃ©gression logistique\nLâ€™idÃ©e est dâ€™apprendre une fonction diï¬€Ã©rente avec chaque neurone, qui sont ensuite composÃ©es"
      }
    ]
  },
  {
    "id": "aa9096b9-2dff-4697-aa2f-d56b164452b1",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_6.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartMÃ©taphore biologique\n6Transmission de lâ€™information: sous forme de signal Ã©lectrique, en passant de neurone en neuroneCerveau humain: organisÃ© en un Ã©norme rÃ©seau de neurones (86 millards de neurones)Attention: il sâ€™agit simplement une mÃ©taphore, et non une correspondance exacte\nIl y a de nombreuses diï¬€Ã©rences entre le fonctionnement dâ€™un cerveau et ce quâ€™on va voirAttention: les inspirations naturelles (mÃªme existantes) sont plus de nature marketing que scientiï¬que\n\"C'est un abus de langage que de parler de neurones !  De la mÃªme faÃ§on qu'on parle d'aile pour un avion mais aussi pour un oiseau,  le neurone artiï¬ciel est un modÃ¨le extrÃªmement simpliï¬Ã© de la rÃ©alitÃ© biologique.\"https://www.sciencesetavenir.fr/high-tech/intelligence-artiï¬cielle/ selon-yann-lecun-l-intelligence-artiï¬cielle-a-20-ans-pour-faire-ses-preuves_120121\nYann LeCun"
      }
    ]
  },
  {
    "id": "d5373103-51d4-4aea-9d18-1eb7c536dcec",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_7.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nRÃ©seau de neurones Ã  deux couches: notations\n7Couche dâ€™entrÃ©e: couche reprenant les caractÃ©ristiques de la donnÃ©eCouche cachÃ©e: couche dÃ©diÃ©e aux calculs intermÃ©diairesConvention: la couche dâ€™entrÃ©e est la couche 0Couche de sortie: couche dÃ©diÃ©e Ã  la prÃ©diction ï¬nale Couche  dâ€™entrÃ©e [0] (input layer)\nCouche  cachÃ©e [1] (hidden layer)\nCouche  de sortie [2] (output layer)La couche dâ€™entrÃ©e nâ€™a ainsi aucun paramÃ¨tres Ã  apprendre\nn[i]:Â nombreÂ deÂ neuronesÂ Ã Â laÂ coucheÂ iw[i]j,b[i]j:Â paramÃ¨tresÂ duÂ neuroneÂ jÂ Ã Â laÂ coucheÂ ixj:Â caractÃ©ristiqueÂ jÂ pourÂ uneÂ donnÃ©eÂ spÃ©ciï¬queÂ \nNote:Â w[i]jÂ estÂ deÂ dimensionÂ n[iâˆ’1]Ã—1Â (nombreÂ deÂ ï¬‚Ã¨chesÂ entrantes)aj:Â sortieÂ duÂ neuroneÂ jÂ Ã Â laÂ coucheÂ cachÃ©eÌ‚y:Â prÃ©dictionÂ pourÂ uneÂ donnÃ©eÂ selonÂ sesÂ caractÃ©ristiquesÂ xz1=w[1]T1x+b[1]1a1=Ïƒ(z1)a1x2x1Intuition: un neurone est une rÃ©gression logistique avec ses paramÃ¨tres\nx1,x2\na1,a2,a3\nÌ‚yNotation: on va Ã©tendre nos notations prÃ©cÃ©dentes\na1=Ïƒ(w[1]T1x+b[1]1)\na2=Ïƒ(w[1]T2x+b[1]2)\na3=Ïƒ(w[1]T3x+b[1]3)"
      }
    ]
  },
  {
    "id": "86e34e69-b0d6-4fa6-9340-299192e3a7e5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_8.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©seau de neurones: forme matricielleNotation matricielle: regrouper toutes les valeurs en vecteurs et matricesIntÃ©rÃªt: donne une expression compacte et plus eï¬ƒcace Ã  calculerx=(x1x2)Â vecteurÂ desÂ caractÃ©ristiquesÂ d'uneÂ instanceÂ spÃ©ciï¬queW[1]=âˆ’w[1]T1âˆ’âˆ’w[1]T2âˆ’âˆ’w[1]T3âˆ’b[1]=b[1]1b[1]2b[1]3z[1]=z[1]1z[1]2z[1]3=âˆ’w[1]T1âˆ’âˆ’w[1]T2âˆ’âˆ’w[1]T3âˆ’(x1x2)+b[1]1b[1]2b[1]3=w[1]T1x+b[1]1w[1]T2x+b[1]2w[1]T3x+b[1]3Â EtapeÂ deÂ laÂ combinaisonÂ linÃ©aire:Â z[1]=W[1]x+b[1]\na[1]=a[1]1a[1]2a[1]3=Ïƒ(z[1]1)Ïƒ(z[1]2)Ïƒ(z[1]3)EtapeÂ deÂ l'applicationÂ deÂ laÂ sigmoide:Â a[1]=Ïƒ(z[1])Dimension des matricesx:n[0]Ã—1=2Ã—1W[1]:n[1]Ã—n[0]=3Ã—2b[1]:n[1]Ã—1=3Ã—1z[1]:n[1]Ã—1=3Ã—1a[1]:n[1]Ã—1=3Ã—18\na[i]j:Â sortieÂ duÂ neuroneÂ jÂ Ã Â laÂ coucheÂ iÂ (suiteÂ deÂ laÂ notationÂ prÃ©cÃ©dente)z[i]j:Â calculÂ intermÃ©diaireÂ duÂ neuroneÂ jÂ Ã Â laÂ coucheÂ iMatrice des poids  Ã  la couche cachÃ©eVecteur des biais  Ã  la couche cachÃ©e"
      }
    ]
  },
  {
    "id": "f9fde7d1-b529-4803-ab2e-4c235001d87f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_9.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©seau de neurones: Ã©quations fondamentales\nz[1]=W[1]x+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]Ì‚y=Ïƒ(z[2])\nW[1]:n[1]Ã—n[0]=3Ã—2â†’6b[1]:n[1]Ã—1=3Ã—1â†’3b[2]:n[2]Ã—1=1Ã—1â†’1W[2]:n[2]Ã—n[1]=1Ã—3â†’3Total:13Â paramÃ¨tresÂ devantÂ ÃªtreÂ entrainÃ©s\nCombien de paramÃ¨tres Ã  apprendre a t-on dans ce rÃ©seau ?Expression similaire: notation a pour la couche dâ€™entrÃ©e et de sortie\n9\nPrincipe: obtenir la prÃ©diction Ã  partir des caractÃ©ristiques en entrÃ©e\nz[1]=W[1]a[0]+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]a[2]=Ïƒ(z[2])Avantage: donne une expression plus gÃ©nÃ©rique\nPrincipe: calculer la taille de chaque matrice de paramÃ¨tres, et sommer le toutNote: appliquer ces Ã©quations revient Ã  faire la passe en avant dans un rÃ©seau de neurones\nCes Ã©quations sont les Ã©quations fondamentales des rÃ©seaux de neurones"
      }
    ]
  },
  {
    "id": "d49285a6-c55d-49a5-bb74-760840a05cfe",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_10.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©seau de neurones : situation avec plusieurs donnÃ©es\nLimitation: ces Ã©quations ne considÃ¨rent quâ€™une seule donnÃ©e en entrÃ©e\nD:{(x(1),y(1)),(x(2),y(2)),â€¦,(x(m),y(m))}\nz[1](i)=W[1]x(i)+b[1]a[1](i)=Ïƒ(z[1](i))z[2](i)=W[2]a[1](i)+b[2]a[2](i)=Ïƒ(z[2](i))âˆ€iâˆˆ{1,â€¦,m}Ì‚y(i)=a[2](i)Objectif: appliquer ces Ã©quations sur chaque donnÃ©eLa notation [i] rÃ©fÃ¨re Ã  une couche iLa notation (i) rÃ©fÃ¨re Ã  une donnÃ©e i\nErreur frÃ©quente: confusion avec les indicesConseil: comprenez ce que chaque indice signiï¬e10\nz[1]=W[1]x+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]Ì‚y=Ïƒ(z[2])\nz[1]=W[1]a[0]+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]a[2]=Ïƒ(z[2])AmÃ©lioration: adapter ces Ã©quations Ã  un ensemble de donnÃ©esNotation introduite dans le module prÃ©cÃ©dent"
      }
    ]
  },
  {
    "id": "c5ee5b37-85bf-4b01-993d-2b02955d7027",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_11.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartRÃ©seau de neurones : calcul de la passe en avant itÃ©rative pour plusieurs donnÃ©es\nEst-ce que c'est correct ? \nEst-ce que c'est eï¬ƒcace ? Oui !Non ! Il y a moyen de faire mieuxAlgorithme naÃ¯f: prÃ©dire itÃ©rativement le rÃ©sultat pour chaque donnÃ©e\nAvez-vous une autre alternative ? \n11\nz[1](i)=W[1]x(i)+b[1]a[1](i)=Ïƒ(z[1](i))z[2](i)=W[2]a[1](i)+b[2]a[2](i)=Ïƒ(z[2](i))ğ–¿ğ—ˆğ—‹iğ—‚ğ—‡1ğ—ğ—ˆm:ğ–¿ğ—ˆğ—‹ğ—ğ–ºğ—‹ğ–½ğ–¯ğ–ºğ—Œğ—Œğ–¨ğ—ğ–¾ğ—‹ğ–ºğ—ğ—‚ğ—ğ–¾(D,W[1],b[1],â€¦):\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡a[2](1),â€¦,a[2](m)IdÃ©e: on peut regrouper toutes les donnÃ©es dans une matrice|x(1)|,â€¦,|x(m)|n[0]Ã—1n[0]Ã—1n[0]Ã—m|â€¦|x(1)â€¦x(m)|â€¦|=X#caractÃ©ristiquesÃ—#donnÃ©esIntÃ©rÃªt: permet la conception d'algorithmes plus eï¬ƒcaces, opÃ©rant sur des matricesIntÃ©rÃªt supplÃ©mentaire: cela est d'autant plus vrai lorsqu'on utilise des GPUsGraphical processor unit (GPU): processeur spÃ©cialisÃ© pour rÃ©aliser des tÃ¢ches similaires en parallÃ¨le\nForme matricielle: reprÃ©sentation des donnÃ©es en matrice Ã  des ï¬ns de performances"
      }
    ]
  },
  {
    "id": "b96a7bc7-8303-43fa-b46b-ea8592c8e725",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_12.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartForme matricielle pour plusieurs prÃ©dictions\nConsÃ©quence: les valeurs z, et a de chaque neurone deviennent une matriceZ[1]=âˆ’w[1]T1âˆ’âˆ’â€¦âˆ’âˆ’w[1]Tn[1]âˆ’|â€¦|x(1)â€¦x(m)|â€¦|+b[1]1â€¦b[1]n[1]=|â€¦|z[1](1)â€¦z[1](m)|â€¦|Forme matricielle: une colonne contient les informations spÃ©ciï¬ques pour une instance\n=w[1]T1x(1)+b[1]1â€¦w[1]T1x(m)+b[1]1â€¦â€¦â€¦w[1]Tn[1]x(1)+b[1]n[1]â€¦w[1]Tn[1]x(m)+b[1]n[1]A[1]=|â€¦|Ïƒ(z[1](1))â€¦Ïƒ(z[1](m))|â€¦|=|â€¦|a[1](1)â€¦a[1](m)|â€¦|Application de la sigmoide Ã  chaque valeur (element-wise)Broadcasting de bX=|â€¦|x(1)â€¦x(m)|â€¦|\nDimension:n[0]Ã—mDimension:#caractÃ©ristiquesÃ—#donnÃ©es\n12\nDimension:n[1]Ã—mDimension:#neuronesÃ—#donnÃ©es\nDimension:n[1]Ã—mDimension:#neuronesÃ—#donnÃ©es\nCes opÃ©rations reviennent Ã  calculer la passe en avant en mÃªme temps pour un ensemble de donnÃ©es"
      }
    ]
  },
  {
    "id": "93c75039-51f3-4526-a317-68c01d76e488",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_13.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartEquations fondamentales sous forme matricielle\nz[1]=W[1]x+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]a[2]=Ïƒ(z[2])Ì‚y=a[2]Convention: on utilise des lettres majuscules pour les matricesApplication: problÃ¨me de l'Ã©tudiant avec deux caractÃ©ristiques#Â heuresÂ d'Ã©tude:x1#Â heuresÂ deÂ repos:x2DonnÃ©es: quatre donnÃ©es dâ€™entraÃ®nementD:{(x(1),y(1)),(x(2),y(2)),(x(3),y(3)),(x(4),y(4))}ReprÃ©sentation: forme matricielleX=(x(1)1x(2)1x(3)1x(4)1x(1)2x(2)2x(3)2x(4)2)\nQue represente  x(3)2Â ?RÃ©ponse: nombre d'heures de repos du troisiÃ¨me Ã©tudiantSortie de la premiÃ¨re couche:A[1]=a[1](1)1a[1](2)1a[1](3)1a[1](4)1a[1](1)2a[1](2)2a[1](3)2a[1](4)2a[1](1)3a[1](2)3a[1](3)3a[1](4)3\nQue represente  a[1](2)3Â ?RÃ©ponse:  activation du 3e neurone  de la couche 1 pour le  2e Ã©tudiantPrÃ©diction ï¬nale: une valeur pour chaque donnÃ©eÌ‚Y=(Ì‚y(1)Ì‚y(2)Ì‚y(3)Ì‚y(4))A[2]=(a[2](1)1a[2](2)1a[2](3)1a[2](4)1)13\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]Illustration: passage Ã  une forme matricielle (Ã©quations de droite)"
      }
    ]
  },
  {
    "id": "b250a9d2-4fef-46a7-8927-6c17493ccff3",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_14.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n14RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "35ff605a-124e-4acb-9f13-84da0eb6bd1e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_15.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nApprentissage supervisÃ© par un rÃ©seau de neurones\nJ(W[1],b[1],W[2],b[2])=1mmâˆ‘i=1L(Ì‚y(i),y(i))b[1]:n[1]Ã—1=3Ã—1W[1]:n[1]Ã—n[0]=3Ã—2W[2]:n[2]Ã—n[1]=1Ã—3b[2]:n[2]Ã—1=1Ã—113 paramÃ¨tres Ã  dÃ©terminer\nConsÃ©quence: la fonction peut Ãªtre minimisÃ©e par une descente de gradientBonne nouvelle: tout le rÃ©seau est diï¬€Ã©rentiable si la fonction d'Ã©cart l'est Ã©galementConsÃ©quence: on risque de converger dans un minimum local15\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nPrincipe: identique Ã  une rÃ©gression logistique, sauf que la fonction contient plus de paramÃ¨tresObjectif: dÃ©terminer les paramÃ¨tres qui minimisent une fonction de coÃ»t liÃ©e Ã  la tÃ¢che de prÃ©diction\nComment dÃ©terminer la valeur de ces paramÃ¨tres ? Expression gÃ©nÃ©rique, qui ne diï¬€Ã¨re  que par la fonction dâ€™Ã©cart choisieMauvaise nouvelle: la fonction de coÃ»t nâ€™est plus convexe"
      }
    ]
  },
  {
    "id": "92d9cfbd-ac19-4538-8556-aded808abd3f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_16.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDescente de gradient dans un rÃ©seau de neurones\n16\nğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾Ì‚Yğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾âˆ‚Jâˆ‚W[2],âˆ‚Jâˆ‚b[2],âˆ‚Jâˆ‚W[1],âˆ‚Jâˆ‚b[1]W[2],b[2],W[1],b[1]=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()W[2]=W[2]âˆ’Î±âˆ‚Jâˆ‚W[2]b[2]=b[2]âˆ’Î±âˆ‚Jâˆ‚b[2]W[1]=W[1]âˆ’Î±âˆ‚Jâˆ‚W[1]b[1]=b[1]âˆ’Î±âˆ‚Jâˆ‚b[1]ğ—€ğ—‹ğ–ºğ–½ğ—‚ğ–¾ğ—‡ğ—ğ–£ğ–¾ğ—Œğ–¼ğ–¾ğ—‡ğ—(D,Î±):\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡W[1],W[2],b[1],b[2]ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾J(W[1],b[1],W[2],b[2])\nJ(W[1],b[1],W[2],b[2])=1mmâˆ‘i=1L(Ì‚y(i),y(i))\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nâˆ‚Jâˆ‚b[1]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b[1]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b[1])âˆ‚Jâˆ‚W[1]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚W[1]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚W[1])âˆ‚Jâˆ‚b[2]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b[2]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b[2])âˆ‚Jâˆ‚W[2]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚W[2]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚W[2])DÃ©rivation: www.efavdb.com/backpropagation-in-neural-networks\nCalcul du gradient: rÃ¨gle du chaÃ®nage sur un graphe de dÃ©pendances"
      }
    ]
  },
  {
    "id": "a6f52f38-ec3b-445a-940f-7f73ea7af7a0",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_17.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartConsÃ©quence de la non-convexitÃ©\n17\nQuelle est la consÃ©quence majeure de la non-convexitÃ© de la fonction de coÃ»t ?RÃ©alitÃ©: on risque de converger dans un minimum localAnalogie: utilisation de mÃ©taheuristiques dans une recherche localeConsÃ©quence: on doit maintenant faire attention Ã  la faÃ§on dont on entraÃ®ne le rÃ©seauObjectif: faciliter lâ€™apprentissage aï¬n de converger vers un bon minimum local\nQuelles sont les actions que nous pouvons prendre pour faciliter lâ€™apprentissage ?(1) Remplacer la sigmoide par une autre fonction (tanh, ReLU, LeakyReLU, etc.)(2) Initialiser adÃ©quatement les paramÃ¨tres (bonne solution initiale)(3) Utiliser un autre algorithme que la descente de gradient (SGD, Adam, etc.)(4) Rajouter des opÃ©rations complÃ©mentaires dans le rÃ©seau (normalisation, BatchNorm)(5) SÃ©lectionner adÃ©quatement certains hyper-paramÃ¨tres (taux dâ€™apprentissage, etc.)\nIl y a encore continuellement des nouvelles idÃ©es qui sont introduites par la communautÃ© scientiï¬que"
      }
    ]
  },
  {
    "id": "9c80a43f-0112-445b-ae69-8af14a50bdf5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_18.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n18RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "d720951a-16e2-4b25-98f7-d28b4377d270",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_19.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]Limitations de la sigmoide \n19Situation actuelle: une fonction sigmoide est appliquÃ©e Ã  la sortie de chaque neuroneObjectif initial: obtenir une valeur entre 0 et 1 (prÃ©diction dâ€™une probabilitÃ©)Limitation: ce choix est non justiï¬Ã© pour les neurones intermÃ©diairesPertinence du choix: correct pour le neurone de la derniÃ¨re couche\nA t-on d'autres alternatives ?\nPire encore: ce choix amÃ¨ne Ã  des piÃ¨tres performancesTransformation dÃ©ï¬nissant la valeur de sortie un neurone aprÃ¨s la combinaison des entrÃ©esFonction dâ€™activation"
      }
    ]
  },
  {
    "id": "057b237e-758d-45eb-8b6a-fb95a3e2087c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_20.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation\n20Fonction sigmoide: fonction d'activation possible, mais il en existe plein d'autresContrainte 1: la fonction activation doit Ãªtre non-linÃ©aire (explication Ã  venir)Tangente hyperbolique (tanh)Rectiï¬ed linear unit (ReLU)Variantes du ReLU (Leaky ReLU, Exponential ReLU, etc.)Et plein d'autres... (Maxout, etc.)Lecture complÃ©mentaire: https://cs231n.github.io/neural-networks-1/#actfunContrainte 2: la fonction doit Ãªtre principalement diï¬€Ã©rentiable (calcul de la descente de gradient)\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])Ì‚Y=A[2]IdÃ©e: considÃ©rer une autre fonction que la sigmoide comme fonction activationIl existe une multitude de fonctions dâ€™activation dans la littÃ©rature\nRaison principale: chacune a ses forces et faiblesses ainsi que leur champ dâ€™application"
      }
    ]
  },
  {
    "id": "7a747849-6674-419b-888b-3e9a2c5063e2",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_21.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation: la sigmoide\n21\ng(z)=11+eâˆ’zdg(z)dz=g(z)(1âˆ’g(z))Fonction sigmoideGradient de la sigmoide\nApplication: neurone de la derniÃ¨re couche pour une classiï¬cation binaire (prÃ©diction d'une probabilitÃ©)Fonction sigmoide: compresse n'importe quelle valeur dans un intervalle entre 0 et 1Diï¬ƒcultÃ©: la fonction sature et le gradient quasi-nul pour des grosses valeurs (ralenti l'apprentissage)Diï¬ƒcultÃ©: implique des opÃ©rations coÃ»teuses (calcul d'exponentielles)Diï¬ƒcultÃ©: Les valeurs de sortie sont toujours positivesConsÃ©quence: la descente de gradient aura tendance Ã  zigzagger (ralenti l'apprentissage)\nW[1]=W[1]âˆ’Î±âˆ‚Jâˆ‚W[1]\nRecommendation: utilisation non conseillÃ©e, sauf pour la sortie d'une classiï¬cation binaire"
      }
    ]
  },
  {
    "id": "47f25c4d-8d43-40e0-8d6a-ad6919cb580f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_22.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartValeurs positives de la sigmoideRegardons l'impact sur le neurone de la derniÃ¨re couche\nz[2]=w[2]1a[1]1+w[2]2a[1]2+b[2]AvecÂ a[1]1Â etÂ a[1]2Â positifsÂ (sortieÂ d'uneÂ sigmoide)Etape 2: analysons les dÃ©rivÃ©es partielles de la fonction d'Ã©cart\n22\nz[1]=W[1]x+b[1]a[1]=Ïƒ(z[1])z[2]=W[2]a[1]+b[2]a[2]=Ïƒ(z[2])Ì‚y=a[2]\nEtape 1: analysons les dÃ©rivÃ©es partielles de ce neurone pour les diï¬€Ã©rents poids (w)A ce stade, on sait que ces 2 valeurs sont positivesâˆ‚L(Ì‚y,y)âˆ‚w[2]1=âˆ‚L(Ì‚y,y)âˆ‚z[2]âˆ‚z[2]âˆ‚w[2]1=a[1]1âˆ‚L(Ì‚y,y)âˆ‚z[2]âˆ‚L(Ì‚y,y)âˆ‚w[2]2=âˆ‚L(Ì‚y,y)âˆ‚z[2]âˆ‚z[2]âˆ‚w[2]2=a[1]2âˆ‚L(Ì‚y,y)âˆ‚z[2]Valeur positiveMÃªme valeurMise en situation: rÃ©seau Ã  deux neurones cachÃ©s\nConsÃ©quence: donne un zigzag visuel dans la descente de gradient, ce qui ralenti lâ€™apprentissageObservation: les deux dÃ©rivÃ©es partielles ont le mÃªme signeImplication: on a le mÃªme sens de descente pour ces deux paramÃ¨tresw[2]1=w[2]1âˆ’Î±âˆ‚Jâˆ‚w[2]1\nw[2]2=w[2]2âˆ’Î±âˆ‚Jâˆ‚w[2]2\nMÃªme signe\nâˆ‚z[2]âˆ‚w[2]1=a[1]1\nâˆ‚z[2]âˆ‚w[2]2=a[1]2"
      }
    ]
  },
  {
    "id": "ea47d255-8b26-4abe-8671-848dabc1347c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_23.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nFonction d'activation: tangente hyperbolique (tanh)\n23Tangente hyperboliqueGradient de tanh\nAvantage: les valeurs de la fonction sont centrÃ©es en 0 (mitige l'eï¬€et de zigzag)Tangente hyperbolique: compresse n'importe quelle valeur dans un intervalle entre -1 et 1g(z)=tanh(z)dg(z)dz=1âˆ’tanh2(z)tanh(z)=ezâˆ’eâˆ’zez+eâˆ’z\nDiï¬ƒcultÃ©: la fonction sature et le gradient quasi-nul pour des grosses valeurs (ralenti l'apprentissage)Diï¬ƒcultÃ©: implique des opÃ©rations coÃ»teuses (calcul d'exponentielles)\nRecommendation: fonction quasi-toujours prÃ©fÃ©rable Ã  la fonction sigmoide pour une couche cachÃ©e"
      }
    ]
  },
  {
    "id": "d8cc7289-6f45-422f-8fc7-133a6a21d141",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_24.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation: Rectiï¬ed linear unit (ReLU)\n24Avantage: ne sature pas dans la rÃ©gion positive (convergence accÃ©lÃ©rÃ©e)ReLU: fonction linÃ©aire par morceaux qui met toutes les valeurs nÃ©gatives Ã  0\ng(z)=max(0,z)\nDeep sparse rectiï¬er neural network [Glorot et al. 2011]\nNotez l'Ã©chelle de l'axe des ordonnÃ©esDÃ©rivÃ©e non dÃ©ï¬nie en 0Pas un problÃ¨me en pratiquedg(z)dz=8><>:0i fz<01i fz>0?ifz=0\nAvantage: opÃ©rations peu coÃ»teusesCause: le gradient est nul dans la zone nÃ©gative (rien ne sera appris pour ce neurone)Mitigation possible: prendre un taux d'apprentissage pas trop grandDiï¬ƒcultÃ©: Les valeurs de sortie ne sont jamais nÃ©gatives (eï¬€et de zigzag)Diï¬ƒcultÃ©: une descente trop forte peut amener une valeur dans la zone nÃ©gative (neurone mort)\nRecommendation: trÃ¨s bon choix de fonction dâ€™activation (recommandÃ© pour vos premiers modÃ¨les)\nW[1]=W[1]âˆ’Î±âˆ‚Jâˆ‚W[1]"
      }
    ]
  },
  {
    "id": "6818c1e5-6ad9-4ec8-9271-879890047ad4",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_25.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartNon-linÃ©aritÃ© des fonctions d'activation\n25\nOn souhaite avoir une activation non-linÃ©aire. Pourquoi est-ce important ? Exemple: considÃ©rons une activation linÃ©aire pour les neurones cachÃ©sFonctionÂ identitÃ©:Â g(z)=zAnalyse: regardons l'impact dans un rÃ©seau dÃ©diÃ© Ã  une prÃ©diction de probabilitÃ©\nZ[2]=Wâ€² X+bâ€² Ì‚Y=Ïƒ(Z[2])Observation: le rÃ©seau s'eï¬€ondre en une combinaison linÃ©aire des entrÃ©es XCause: la composition de deux fonctions linÃ©aires est aussi linÃ©aire\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]DimensionÂ deÂ W[2]W[1]:(1Ã—3).(3Ã—2)=1Ã—2DimensionÂ deÂ W[2]b[1]+b[2]:(1Ã—3).(3Ã—1)+(1Ã—1)=1Ã—1\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=W[2](W[1]X+b[1])+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nZ[1]=W[1]X+b[1]A[1]=Z[1]Z[2]=(W[2]W[1])X+(W[2]b[1]+b[2])A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nSans non-linÃ©aritÃ©, le modÃ¨le se rÃ©duit Ã  rÃ©gression logistique"
      }
    ]
  },
  {
    "id": "23d80638-3fd6-4265-ae46-d22e6b466bc5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_26.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivitÃ© des rÃ©seaux de neurones\n26Un rÃ©seau de neurones avec des fonctions d'activation non-linÃ©aire peut thÃ©oriquement approximer avec une prÃ©cision arbitraire n'importe quelle fonction rÃ©elleUniversalitÃ© des rÃ©seaux de neurones\nRÃ©sultat thÃ©orique important: justiï¬e le pouvoir expressif des rÃ©seaux de neurones (avec des non-linÃ©aritÃ©s)Terminologie: on dit Ã©galement quâ€™un rÃ©seau de neurones est un approximateur universelNote: cela est Ã©galement vrai avec une activation sigmoide et seulement une couche cachÃ©eApproximation by superpositions of a sigmoidal function [Cybenko, 1989]Diï¬ƒcultÃ© pratique: le rÃ©seau peut-Ãªtre diï¬ƒcile Ã  entraÃ®ner (dâ€™oÃ¹ les mÃ©canismes supplÃ©mentaires)Diï¬ƒcultÃ© pratique: ce rÃ©sultat peut nÃ©cessiter un nombre immense de neurones"
      }
    ]
  },
  {
    "id": "939ffc91-3bae-4fd4-a5b6-8019a834b56a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_27.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nFonction d'activation de la derniÃ¨re couche\n27\nPeut-on utiliser la mÃªme fonction dâ€™activation pour tous les neurones ? Objectif de la derniÃ¨re couche: dÃ©ï¬nir le type de prÃ©diction Ã  obtenir\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])Ì‚Y=A[2]En trÃ¨s grande majoritÃ©: oui, vous pouvez utiliser la mÃªme fonction partoutException: lâ€™activation de la derniÃ¨re couche a une importance particuliÃ¨re \nChoix adaptÃ©: fonction sigmoÃ¯deTÃ¢che 2: classiï¬cation binaireObjectif: obtenir une probabilitÃ© de succÃ¨s (valeur entre 0 et 1)RÃ¨gle dâ€™or: la derniÃ¨re activation doit retourner une valeur cohÃ©rente avec votre tÃ¢che de prÃ©diction\nFonction de perte liÃ©e: entropie croisÃ©e binaireTÃ¢che 1: rÃ©gression\nChoix adaptÃ©: fonction identitÃ© (linÃ©aire)Objectif: obtenir une valeur rÃ©elleFonction de perte liÃ©e: erreur quadratiqueNote: il sâ€™agit dâ€™un des rares cas ou vous devez utiliser une activation linÃ©aire\ng[2](z)=Ïƒ(z)=11+eâˆ’z\ng[2](z)=z"
      }
    ]
  },
  {
    "id": "2905500d-3863-4b75-9a88-10449f6038cc",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_28.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation de la derniÃ¨re couche\n28TÃ¢che 3: classiï¬cation multi-classe\nObjectif: eï¬€ectuer une prÃ©diction entre plusieurs classes d'objetsExtension naturelle de la classiï¬cation binaire\nComment eï¬€ectuer une prÃ©diction multi-classe ? EtapeÂ initiale:Â supposonsÂ queÂ l'onÂ aitÂ CÂ classesÂ deÂ prÃ©dictionVraieÂ valeurÂ (y):Â correspondÂ Ã Â uneÂ catÃ©gorieÂ câˆˆCIdÃ©e: prÃ©dire une probabilitÃ© dâ€™appartenance Ã  chaque classe (de faÃ§on similaire au cas binaire)\nOiseauChienChaty=(100)EncodageÂ possible:Â unÂ vecteurÂ deÂ tailleÂ CÃ—1Â avecÂ unÂ 1Â Ã Â laÂ bonneÂ classeÂ etÂ 0Â sinonÂ (one-hotÂ encoding)Objectif: prÃ©dire la probabilitÃ© d'appartenance Ã  chaque classePrÃ©diction:Â correspondÂ Ã Â unÂ vecteurÂ Ì‚yâˆˆ[0,1]C\nÌ‚y=0.700.260.04Remarque: la somme des Ã©lÃ©ments de ce vecteur doit valoir 1 (ce sont des probabilitÃ©s)\nComment obtenir un vecteur satisfaisant cette contrainte ?"
      }
    ]
  },
  {
    "id": "ce92b5f5-c801-43bf-bd96-1dcb7bee73e5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_29.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartFonction d'activation softmax\n29Exemple:Â z=(542)â†’Ì‚y=e5e5+e4+e2e4e5+e4+e2e2e5+e4+e2=0.700.260.04IntÃ©rÃªt: permet de rÃ©aliser des classiï¬cations avec plusieurs classes Fonction qui transforme un vecteur de C rÃ©els en entrÃ©e, en un vecteur de C probabilitÃ©sFonction softmax\nğ–²ğ—ˆğ–¿ğ—ğ—†ğ–ºğ—‘:â„Câ†’[0,1]CÂ (avecÂ laÂ sommeÂ desÂ outputsÂ quiÂ vautÂ Ã Â 1)Ì‚yj=ğ–²ğ—ˆğ–¿ğ—ğ—†ğ–ºğ—‘(zj)=ezjâˆ‘Cc=1ezc\nClassiï¬cation ï¬nale: eï¬€ectuer la prÃ©diction en choisissant la classe ayant la plus haute probabilitÃ©Utilisation: Ã  la derniÃ¨re couche dâ€™un rÃ©seau de neurones pour une tÃ¢che de classiï¬cation multi-classeImpact sur le rÃ©seau: la derniÃ¨re couche aura un nombre de neurones qui Ã©quivaut au nombre de classesIntuition: chaque neurone de la derniÃ¨re couche va prÃ©dire une probabilitÃ© propre Ã  la classe"
      }
    ]
  },
  {
    "id": "ac727b4e-f8d7-4999-942e-b636f2562f10",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_30.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nÌ‚y=0.700.260.04â‰ˆy=(100)Fonction dâ€™Ã©cart pour une classiï¬cation multi-classe\n30\nComment caractÃ©riser la qualitÃ© d'une classiï¬cation multi-classe ?Encore une fois, cela revient Ã  dÃ©ï¬nir une fonction dâ€™Ã©cart appropriÃ©e Ã  cette tÃ¢cheClassiï¬cation binaire: utilisation de lâ€™entropie croisÃ©e binaire Classiï¬cation multi-classe: utilisation de lâ€™entropie croisÃ©e\nL(Ì‚y,y)=âˆ’(ylog(Ì‚y)+(1âˆ’y)log(1âˆ’Ì‚y))Fonction dâ€™Ã©cart communÃ©ment utilisÃ©e pour des tÃ¢ches de de classiï¬cation multi-classeErreur de lâ€™entropie croisÃ©e (categorical cross entropy loss)\nL(Ì‚y,y)=âˆ’Câˆ‘c=1yclog(Ì‚yc)Â avecÂ Ì‚ycâˆˆ[0,1]Â etÂ Câˆ‘c=1Ì‚yc=1C:leÂ nombreÂ deÂ classesÂ prÃ©sentesyc:valeurÂ duÂ vecteurÂ yÂ quiÂ vautÂ 1Â siÂ cÂ correspondÂ Ã Â laÂ bonneÂ classeÂ (0Â sinon)Ì‚yc:valeurÂ duÂ vecteurÂ Ì‚yÂ quiÂ correspondÂ Ã Â laÂ probabilitÃ©Â prÃ©diteÂ deÂ laÂ classeÂ cIntuition: exactement le mÃªme principe que le cas binairePrincipe:Â minimiserÂ laÂ fonctionÂ pousseÂ (Ì‚yâ†’1)Â pourÂ laÂ bonneÂ classe\nFonction de coÃ»t: Ã©cart moyen de toutes les prÃ©dictions"
      }
    ]
  },
  {
    "id": "4f7aa3ef-3940-4c77-93bc-fcc41287da75",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_31.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n31RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "36f4cad3-5f36-48b5-85da-468ff057098e",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_32.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nâˆ‚Jâˆ‚W[1]=(uvuvuv)\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]Initialisation des paramÃ¨tres: valeur identique\n32\nQue pensez-vous de cette idÃ©e ?\nContre-exemple: essayons avec une valeur initiale de 0 pour tous les paramÃ¨tres\nW[1]=(000000)\nQue peut-on dire de la valeur de sortie de ces 3 neurones ?\nConclusion: on nâ€™est pas plus expressif qu'un seul neurone dans la couche\nIdÃ©e 1: initialiser tous les paramÃ¨tres Ã  la mÃªme valeurRegardons ce qui se passe Ã  la couche cachÃ©eValeurs de sorties: identiques pour les 3 neuronesConsÃ©quence: rien ne diï¬€Ã©rencie les neurones, ils calculent la mÃªme fonctionPasse en arriÃ¨re: les trois gradients vont par consÃ©quent avoir la mÃªme valeurValeurs du gradient liÃ©es au 3e neurone\nb[1]=(000)\na[1]=Ïƒ(W[1]x+b[1])\na[1]1=a[1]2=a[1]3"
      }
    ]
  },
  {
    "id": "54693971-f160-44ba-9478-5e5b5decd9ce",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_33.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartInitialisation des paramÃ¨tres: valeur alÃ©atoire quelconque\n33Intuition: permet de corriger la diï¬ƒcultÃ© prÃ©cÃ©dente car le gradient liÃ© Ã  chaque neurone sera diï¬€Ã©rent\nEst-ce que n'importe quelle valeur est bonne ?Descente de gradient: demande de calculer le gradient d'une fonction sigmoide (version de base)\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nIdÃ©e 2: initialiser tous les paramÃ¨tres Ã  une valeur alÃ©atoire quelconque\nQue se passe t-il si les paramÃ¨tres ont une grande valeur (positive ou nÃ©gative) ?\nTrÃ¨s petite valeurObservation: on se retrouve sur une rÃ©gion assez plate de la sigmoide (pente trÃ¨s faible)ConsÃ©quence thÃ©orique: la valeur du gradient sera par trÃ¨s petiteConsÃ©quence pratique: lâ€™entraÃ®nement sera lent\nW[1]=W[1]âˆ’Î±âˆ‚Jâˆ‚W[1]"
      }
    ]
  },
  {
    "id": "5f8a312c-eaa1-49c5-b7d1-b319e2476a8a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_34.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartInitialisation des paramÃ¨tres: valeur alÃ©atoire proche de zÃ©ro\n34Intuition: permet de corriger la diï¬ƒcultÃ© prÃ©cÃ©dente en permettant des plus grands pas de gradient\nZ[1]=W[1]X+b[1]A[1]=Ïƒ(Z[1])Z[2]=W[2]A[1]+b[2]A[2]=Ïƒ(Z[2])Ì‚Y=A[2]\nIdÃ©e 3: initialiser tous les paramÃ¨tres Ã  une valeur alÃ©atoire proche de zÃ©roLâ€™objectif est dâ€™Ãªtre dans cette zoneNote: il nâ€™y a aucun problÃ¨me Ã  initialiser les biais (b) Ã  zÃ©roApproche simple: initialiser les paramÃ¨tres alÃ©atoirement dans un intervalle entre -1 et 1Autres fonctions dâ€™initialisation (hors matiÃ¨re)Cette initialisation est simple et donne de bons rÃ©sultats en pratique pour de petits rÃ©seaux\nObjectif: aider Ã  la stabilisation du gradient dans des rÃ©seaux Ã  beaucoup de couches\nInitialisationÂ deÂ Xavier:Â W[l]âˆ¼ğ’©(Î¼=0,Ïƒ2=1n[lâˆ’1])Â (pourÂ uneÂ activationÂ tanh)IdÃ©e: changer la faÃ§on dâ€™initialiser en fonction de la couche considÃ©rÃ©e\nInitialisationÂ deÂ He:Â W[l]âˆ¼ğ’©(Î¼=0,Ïƒ2=2n[lâˆ’1])Â (pourÂ uneÂ activationÂ ReLU)"
      }
    ]
  },
  {
    "id": "8bcc3fe2-b9a2-4295-87ea-45bac3ce96b6",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_35.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVisualisation de lâ€™impact de lâ€™initialisation\n35Lecture complÃ©mentaire: https://www.deeplearning.ai/ai-notes/initialization/index.htmlUnderstanding the diï¬ƒculty of training deep feedforward neural networks [Glorot and Bengio, 2010]Delving deep into rectiï¬ers: Surpassing human-level performance on imagenet classiï¬cation [He et al., 2015]Architecture: rÃ©seau de neurones Ã  4 couches avec des activations de type tanhTÃ¢che: classiï¬cation de chiï¬€res Ã©crits Ã  la main (mÃªme tÃ¢che que pour le devoir)\nAnalyse: impact de lâ€™initialisation des paramÃ¨tres sur les valeurs des activations Ã  chaque couche\nLe signal globalement nulLe signal sâ€™estompeLa signal reste stable"
      }
    ]
  },
  {
    "id": "f9172e4d-5c52-4c87-80d4-3895fd442d31",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_36.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nRÃ©capitulatif: rÃ©seau de neurones\n36\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])Ì‚Y=A[2]z1=w[1]T1x+b[1]1a1=Ïƒ(z1)a1x2x1\nPasse en avant: prÃ©diction via les Ã©quations fondamentalesConcepts clefsNotation matricielle: expression compacte et plus eï¬ƒcaceArchitecture: neurones structurÃ©s en plusieurs couchesEntraÃ®nement: descente de gradient et passe en arriÃ¨reFonction dâ€™activation intermÃ©diaire: non-linÃ©aireDerniÃ¨re activation: dÃ©pend de la tÃ¢che de prÃ©dictionInitialisation des paramÃ¨tres: alÃ©atoire et proche de 0Diï¬ƒcultÃ©: la fonction de coÃ»t nâ€™est plus convexe\nâˆ‚Jâˆ‚b[1]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b[1]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b[1])âˆ‚Jâˆ‚W[1]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚W[1]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚W[1])âˆ‚Jâˆ‚b[2]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b[2]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b[2])âˆ‚Jâˆ‚W[2]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚W[2]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚W[2])\nJ(W[1],b[1],W[2],b[2])=1mmâˆ‘i=1L(Ì‚y(i),y(i))"
      }
    ]
  },
  {
    "id": "c6943869-622c-43f6-9786-04597ee1b836",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_37.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartVisualisation de la prÃ©diction\n37\nhttps://playground.tensorï¬‚ow.org\nCaractÃ©ristiques en entrÃ©e: coordonnÃ©e en x, coordonnÃ©e en yActivation: tangente hyperboliqueTaux dâ€™apprentissage: 0.01Chaque neurone apprend une fonction diï¬€Ã©rente PrÃ©diction plus complexe\nTÃ¢che: classiï¬cation binaire de points 2D"
      }
    ]
  },
  {
    "id": "ec508b3b-b183-46ad-90da-25cfa34f820c",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_38.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n38RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "56f43cd9-b2a5-44df-b42f-aa32bbcc96d1",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_39.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartLimitation de notre rÃ©seau de neurones\n39\nQuelles sont les limitations de notre rÃ©seau de neurones actuel ?\nLimitation: il peut manquer d'expressivitÃ© pour approximer des fonctions complexesFigure de droite: la sÃ©paration avec le mÃªme rÃ©seau devient diï¬ƒcileConclusion: il faut une fonction de prÃ©diction plus expressive !Figure de gauche: bonne sÃ©paration avec un rÃ©seau Ã  deux couches\nPour cela, on va simplement augmenter le nombre de couches dans le rÃ©seau"
      }
    ]
  },
  {
    "id": "cf3cbfe6-8e3c-4529-a105-3f1135fa7b13",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_40.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartApprentissage profond (deep learning)\n40\nTerminologie: le terme profond fait rÃ©fÃ©rence au nombre de couches du rÃ©seauObjectif: construire des fonctions de prÃ©diction plus expressives\nContenu du module: on a formalisÃ© les rÃ©seaux de neurones de faÃ§on gÃ©nÃ©riqueAvantage: lâ€™extension Ã  lâ€™apprentissage profond va Ãªtre TRES facileConceptuellement, il n'y a pas de profondeur limite (bien que des diï¬ƒcultÃ©s apparaissent)Exemple: le rÃ©seau utilisÃ© dans AlphaZero a environ 80 couches, celui de GPT-4 a 120 couchesApprentissage profond (deep learning): utiliser un rÃ©seau avec plus dâ€™une couche cachÃ©e\nPour faire simple, on a dÃ©jÃ  fait tout le nÃ©cessaireâ€¦"
      }
    ]
  },
  {
    "id": "c51730a0-0312-40e3-a314-a564df1becd3",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_41.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartNotations pour le deep learning\n41Notationsn[l]:Â nombreÂ deÂ neuronesÂ pourÂ laÂ coucheÂ lW[l],b[l]:Â paramÃ¨tresÂ deÂ laÂ coucheÂ lX:Â caractÃ©ristiquesÂ enÂ entrÃ©eÂ (formatÂ matriciel)L:Â nombreÂ deÂ couchesÂ (coucheÂ d'entrÃ©eÂ nonÂ comprise)A[l]:Â sortieÂ deÂ laÂ coucheÂ lZ[l]:Â valeursÂ avantÂ l'activationÂ deÂ laÂ coucheÂ lÌ‚Y:Â sortieÂ duÂ rÃ©seauÂ (prÃ©diction)\nExercice: donnez les dimensions de chaque matrice pour 10 donnÃ©es dâ€™entraÃ®nementExercice: donnez le nombre de paramÃ¨tres Ã  apprendre dans ce rÃ©seau"
      }
    ]
  },
  {
    "id": "0482803f-7497-4275-8b7f-0dccea88d2a6",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_42.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartEquations fondamentales de l'apprentissage profond\n42\nZ[l]=W[l]A[lâˆ’1]+b[l]A[l]=g[l](Z[l])Ì‚Y=A[L]ğšğš˜ğš›lâˆˆ{1,â€¦,L}:A[0]=X\nZ[1]=W[1]X+b[1]A[1]=g[1](Z[1])Z[2]=W[2]A[1]+b[2]A[2]=g[2](Z[2])Ì‚Y=A[2]\nSituation prÃ©cÃ©dente: rÃ©seau de neurones Ã  2 couches\nSituation actuelle: rÃ©seau de neurones Ã  L couches\nIl nâ€™y a vraiment rien de nouveau conceptuellement par rapport Ã  prÃ©cÃ©demment"
      }
    ]
  },
  {
    "id": "d5892307-cd53-44dc-bf7b-03555f92dfe5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_43.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDescente de gradient dans un rÃ©seau de neurones profond\n43\nğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾Ì‚Yğ—‹ğ–¾ğ—‰ğ–¾ğ–ºğ—ğ—ğ—‡ğ—ğ—‚ğ—…ğ–¼ğ—ˆğ—‡ğ—ğ–¾ğ—‹ğ—€ğ–¾ğ—‡ğ–¼ğ–¾:ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾âˆ‚Jâˆ‚W[L],âˆ‚Jâˆ‚b[L],â€¦,âˆ‚Jâˆ‚W[1],âˆ‚Jâˆ‚b[1]W[l],b[l]=ğ—‚ğ—‡ğ—‚ğ—ğ—‚ğ–ºğ—…ğ—‚ğ—“ğ–¾ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†ğ—…ğ—’()âˆ€lâˆˆ{1,â€¦,L}W[l]=W[l]âˆ’Î±âˆ‚Jâˆ‚W[l]âˆ€lâˆˆ{1,â€¦,L}b[l]=b[l]âˆ’Î±âˆ‚Jâˆ‚b[l]âˆ€lâˆˆ{1,â€¦,L}ğ—€ğ—‹ğ–ºğ–½ğ—‚ğ–¾ğ—‡ğ—ğ–£ğ–¾ğ—Œğ–¼ğ–¾ğ—‡ğ—(D,Î±):\nğ—‹ğ–¾ğ—ğ—ğ—‹ğ—‡W[1],b[1],â€¦,W[L],b[L]ğ–¼ğ—ˆğ—†ğ—‰ğ—ğ—ğ–¾J(W[1],b[1],â€¦,W[L],b[L])\nJ(W[1],b[1],â€¦,W[L],b[L])=1mmâˆ‘i=1L(Ì‚y(i),y(i))Calcul des dÃ©rivÃ©es par un graphe de dÃ©pendance  et une application de la passe en arriÃ¨re\nÌ‚Y=A[L]\nZ[l]=W[l]A[lâˆ’1]+b[l]A[l]=g[l](Z[l])ğšğš˜ğš›lâˆˆ{1,â€¦,L}:A[0]=X\nâˆ‚Jâˆ‚b[l]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚b[l]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚b[l])âˆ‚Jâˆ‚W[l]=1m(âˆ‚L(Ì‚y(1),y(1))âˆ‚W[l]+â€¦+âˆ‚L(Ì‚y(m),y(m))âˆ‚W[l])ğšğš˜ğš›lğš’ğš—Lğšğš˜1:Nombre de paramÃ¨tresW[1]=n[1]Ã—n[0]=5Ã—4=20b[1]=n[1]Ã—1=5Ã—1=5W[2]=n[2]Ã—n[1]=6Ã—5=30W[3]=n[3]Ã—n[2]=6Ã—6=36W[4]=n[4]Ã—n[3]=5Ã—6=30W[5]=n[5]Ã—n[4]=4Ã—5=20W[6]=n[6]Ã—n[5]=1Ã—4=4b[2]=n[2]Ã—1=6Ã—1=6b[3]=n[3]Ã—1=6Ã—1=6b[4]=n[4]Ã—1=5Ã—1=5b[5]=n[5]Ã—1=4Ã—1=4b[6]=n[6]Ã—1=1Ã—1=1Total: 167 paramÃ¨tres Ã  apprendre"
      }
    ]
  },
  {
    "id": "52799909-d404-4c7c-8e63-a72c7ba6735a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_44.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivitÃ© de l'apprentissage profond\n44IntÃ©rÃªt principal: permet de construire des fonctions plus expressives\nSimple relation (linÃ©aire)Relation  plus complexeRelation encore plus complexeObservation: les neurones apprennent des relations de plus en plus complexe avec le niveau de profondeurhttps://playground.tensorï¬‚ow.org\nQuâ€™en est-il des fonctions encore plus compliquÃ©es ?"
      }
    ]
  },
  {
    "id": "2dc31c92-58ac-4471-892b-a1d13e6a6946",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_45.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExpressivitÃ© de l'apprentissage profond\n45Situation Ã  rÃ©soudre: assez compliquÃ©e, et demande une bonne fonction dâ€™approximationSolution: en augmentant la taille du rÃ©seau, on arrive Ã  avoir des meilleures frontiÃ¨res de dÃ©cision\nExpÃ©rimentez par vous mÃªme en essayant d'autres fonctions d'activation et taux d'apprentissagehttps://playground.tensorï¬‚ow.orgNeurone mort avec une activation ReLUObservation: une activation ReLU a Ã©tÃ© utilisÃ©e, et un neurone est mort"
      }
    ]
  },
  {
    "id": "7231c6a7-b309-4386-bf9a-01044e2ab389",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_46.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n46RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "e7d3fe99-d40b-4138-bda5-5c68ff0514fd",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_47.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartIntroduction aux hyper-paramÃ¨tres\n47\nComment dÃ©terminer l'architecture du rÃ©seau Ã  utiliser ?Cette question amÃ¨ne Ã  diï¬€Ã©rencier paramÃ¨tres et hyper-paramÃ¨tresParamÃ¨tres: valeurs qui sont apprises durant la descente de gradient\nW[l]:Â poidsÂ deÂ laÂ coucheÂ lb[l]:Â biaisÂ deÂ laÂ coucheÂ l\nExemple: nombre de couches, fonction dâ€™activation, etc.Autre choix Ã  faire: taux dâ€™apprentissageIntuition: il sâ€™agit des valeurs qui vont dÃ©terminer la fonction que vous construisezValeurs dÃ©ï¬nies par l'utilisateur utilisÃ©es pour contrÃ´ler l'apprentissage et le type de modÃ¨leHyperparamÃ¨tre\nPoint de vue de lâ€™utilisateur: il nâ€™a aucun contrÃ´le sur la valeur des paramÃ¨tres\nObjectif idÃ©al: trouver les hyper-paramÃ¨tres qui vont rÃ©sulter au modÃ¨le le plus performantImportance: les valeurs choisies ont un impact sur la qualitÃ© de lâ€™apprentissage et sur le modÃ¨le obtenu(1) Nombre de couches(2) Nombre de neurones par couche(3) Fonction d'activation utilisÃ©e(4) Taux d'apprentissageEt encore tout plein !"
      }
    ]
  },
  {
    "id": "2b1f0ac1-93de-40c9-a907-d22e311d948a",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_48.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartSÃ©lection de la valeur des hyper-paramÃ¨tres\n48\nDiï¬ƒcultÃ© de la tÃ¢che: il y a Ã©normÃ©ment de combinaisons Ã  testerTauxÂ d'apprentissage:Â {0.0001,0.001,0.01,0.1}NombreÂ deÂ couches:Â {1,2,3,4,5}FonctionÂ d'activation:Â {ReLU,tanh,sigmoid}Mise en situation: exemple trÃ¨s petit avec quelques choix Ã  faireTotal:Â 4Ã—5Ã—3=60Â conï¬gurationsÂ Ã Â testerLien avec le module 3: trouver la meilleure conï¬guration est un problÃ¨me dâ€™optimisation combinatoireDiï¬ƒcultÃ© supplÃ©mentaire: Ã©valuer chaque conï¬guration est coÃ»teux car cela demande dâ€™entraÃ®ner un rÃ©seauConseils pratiquesEtape 1: ï¬xez les valeurs standards dans un premier temps (par exemple une activation ReLU)Etape 2: procÃ©dez itÃ©rativement et analysez les rÃ©sultats de votre modÃ¨leExemple: une mauvaise prÃ©cision sur les donnÃ©es d'entrainement peut Ãªtre dÃ» Ã  un manque d'expressivitÃ©Action possible: il serait pertinent dâ€™essayer dâ€™augmenter la profondeur du rÃ©seauAlgorithme de recherche d'hyper-paramÃ¨tresRecherche exhaustive (grid search)Recherche alÃ©atoire (random search)Recherche locale et mÃ©taheuristiquesOptimisation bayÃ©sienneHyperparameter search in machine learning [Claesen and De Moor, 2015]"
      }
    ]
  },
  {
    "id": "e31b7cb7-9aa7-4ff2-9c92-633baa57f501",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_49.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAutres conseils pratiques\n49Diï¬ƒcultÃ©s et piÃ¨ges habituels\n(1) Confondre les dimensions des matrices(2) Oublier d'initialiser les poids alÃ©atoirement(3) Avoir un taux d'apprentissage trop grand (surtout avec ReLU)(4) Oublier d'ajouter une fonction dâ€™activation non-linÃ©aire(5) Avoir une fonction d'activation non adaptÃ©e au problÃ¨me Ã  la derniÃ¨re couche(6) Utiliser une fonction de coÃ»t non adaptÃ©e au problÃ¨meEvaluation rigoureuse et analyse de vos rÃ©seauxBonne pratique: diviser vos donnÃ©es en plusieurs ensembles (entraÃ®nement, validation, et de test)Notion de sous-apprentissage (underï¬tting): diï¬ƒcultÃ© Ã  donner de bons rÃ©sultats sur des donnÃ©es vuesNotion de sur-apprentissage (overï¬tting): diï¬ƒcultÃ© Ã  donner de bons rÃ©sultats sur des donnÃ©es non-vuesCes considÃ©rations seront Ã  prendre en compte lors du devoir 3 et discutÃ©es lors de votre laboratoire\nJe vous conseille Keras pour dÃ©buter (trÃ¨s facile dâ€™utilisation)Librairies pour l'apprentissage profondObjectif 1: faciliter la crÃ©ation de rÃ©seaux pour un utilisateurObjectif 2: automatiser lâ€™entraÃ®nement dâ€™un modÃ¨le"
      }
    ]
  },
  {
    "id": "d040c756-a7ab-4afa-a2ae-737bcf8c3d1f",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_50.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nAutres architectures existantes\n50Il existe Ã©galement dâ€™autres architectures de rÃ©seau de neuronesIntÃ©rÃªt: avoir une architecture adaptÃ©e aux donnÃ©es en entrÃ©es\nGradient-based learning applied to document recognition [LeCun, Bengio et al., 1998]\nRÃ©seau rÃ©current: spÃ©cialisÃ© pour l'apprentissage sur des sÃ©quences de donnÃ©esRÃ©seau Ã  convolution: spÃ©cialisÃ© pour l'apprentissage sur des images\nLong short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997]Neural networks and physical systems with emergent collective computational abilities [Hopï¬eld, 1982]\nEt dâ€™autres: rÃ©seau Ã  convolution de graphe, architecture adversarielle, modÃ¨les gÃ©nÃ©ratifs, etc."
      }
    ]
  },
  {
    "id": "fc91ca7b-ff68-49d1-9d6b-c4402e453cfa",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_51.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartDiï¬ƒcultÃ©s de l'apprentissage profond et alternatives\n51\nQuelles sont les alternatives aux rÃ©seaux de neuronesBesoin de lâ€™apprentissage profond: trois composants essentiels(1) Une mÃ©thode dâ€™apprentissage (ce quâ€™on a vu dans ce module)(2) Un nombre consÃ©quent de donnÃ©es labellisÃ©es(3) De la puissance de calculDans les deux cas, si un de ces ingrÃ©dients manque, le rÃ©sultat (ou performances) en sera aï¬€ectÃ©\nLes besoins (2) et (3) augmentent avec la complexitÃ© du rÃ©seauAnalogie: le triangle du feu donnant les trois besoins pour avoir un feu\nIl existe une multitude dâ€™autres mÃ©thodes pour lâ€™apprentissage supervisÃ©Arbres de dÃ©cisionForÃªts alÃ©atoiresClassiï¬cateur bayÃ©sienSupport vector machine (SVM)ModÃ¨les probabilistesGradient boosting (XGBoost)Conclusion: en tout temps, il est pertinent de rester ouvert Ã  dâ€™autres mÃ©thodesConclusion: les success stories de lâ€™apprentissage profond sont souvent liÃ©es Ã  des ressources immenses"
      }
    ]
  },
  {
    "id": "6f155cfb-55da-4157-851a-64dda80a2f94",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_52.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemple rÃ©cent de rÃ©seau profond: Dall-E 2\n52\nNombre de paramÃ¨tres Ã  apprendre: plus de 3.5 millards de paramÃ¨tresNombre de donnÃ©es: environ 650 millions imagesTemps dâ€™entraÃ®nement: information malheureusement trÃ¨s diï¬ƒcile Ã  trouver, mais trÃ¨s consÃ©quent"
      }
    ]
  },
  {
    "id": "076a1ec1-6d90-4cb7-891b-95934301e141",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_53.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartTable des matiÃ¨res\n53RÃ©seaux de neurones1. Concepts principaux des rÃ©seaux de neurones 2. Extension de la rÃ©gression logistique 3. Notations standards des rÃ©seaux de neurones 4. ExÃ©cution de la backpropagation du gradient 5. Principes des fonctions d'activation 6. Initialisation des paramÃ¨tresApprentissage profond1. Limitation des petits rÃ©seaux de neurones 2. Principes de l'apprentissage profond 3. Profondeur d'un rÃ©seau de neurones 4. Introduction aux hyper-paramÃ¨tres 5. Conseils pratiques\nhttps://www.asimovinstitute.org/author/fjodorvanveen/"
      }
    ]
  },
  {
    "id": "1330086c-5cdb-431a-978d-ec4ff2055eb5",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_54.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin Cappart\nSynthÃ¨se des notions vues\n54RÃ©seaux de neurones\nEntraÃ®nement: descente de gradient et passe en arriÃ¨reDiï¬ƒcultÃ©: la fonction de coÃ»t nâ€™est plus convexe\nDÃ©ï¬nition: ensemble de neurones organisÃ©s en un rÃ©seauIntÃ©rÃªt: chaque neurone a ses paramÃ¨tres et contribue Ã  la prÃ©dictionApproximateur universel: capacitÃ© d'approximer n'importe quelle fonctionBesoin: activations non-linÃ©airesApprentissage profondDÃ©ï¬nition: rÃ©seau de neurones ayant plus d'une couche cachÃ©eIntÃ©rÃªt: permet de rÃ©aliser des approximations plus prÃ©cisesTrois ingrÃ©dients essentiels:\n(1) Une mÃ©thode dâ€™apprentissage(2) Un nombre consÃ©quent de donnÃ©es labellisÃ©es(3) De la puissance de calculConsÃ©quence: utilisation de mÃ©canismes pour faciliter lâ€™apprentissage"
      }
    ]
  },
  {
    "id": "f9448eda-7d92-4e7f-b227-888fd4a8d10d",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_55.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartAller plus loin...\n55Cours donnÃ©s Ã  PolytechniqueINF8460: Traitement automatique de la langue naturelle (Amal Zouaq)\nINF8245E: Machine learning (Sarath Chandar)\nINF8225: I.A.: techniques probabilistes et d'apprentissage (Chris pal)\nCours en ligne\nFastAI: Practical deep learning for codersCoursera: Machine learning (Andrew Ng)Coursera: Deep learning (Andrew Ng)https://course.fast.ai/https://www.coursera.org/learn/machine-learninghttps://www.coursera.org/specializations/deep-learning\nAutres ressourcesEnormÃ©ment de ressources sur le web (blog, vidÃ©o youtube, tutoriel, etc.)\nhttps://www.youtube.com/results?search_query=code+emporium\nhttps://distill.pub/"
      }
    ]
  },
  {
    "id": "81eb1162-375b-4afc-a56e-06a78f65fd94",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_56.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartExemples de questions d'examen\n561. Indiquer le nombre de paramÃ¨tres Ã  entraÃ®ner pour un rÃ©seau de neurones donnÃ© 2. Indiquer la taille des matrices dans une reprÃ©sentation matricielle 3. Donnez les Ã©quations de la forward pass pour un rÃ©seau de neurones donnÃ©ThÃ©oriePratique1. Expliquer ce qu'est un rÃ©seau de neurones comparÃ© Ã  une rÃ©gression logistique 2. Expliquer l'importance d'avoir une fonction d'activation non-linÃ©aire 3. Expliquer l'intÃ©rÃªt d'initialiser alÃ©atoirement les paramÃ¨tres 4. Expliquer la diï¬€Ã©rence entre paramÃ¨tres et hyper-paramÃ¨tres"
      }
    ]
  },
  {
    "id": "ae733706-00b1-4d48-9b0c-9aadbb51b650",
    "conversations": [
      {
        "role": "user",
        "content": [
          {
            "image": "src\\convert_to_llava_format\\qwen_training_data\\images\\M7-apprentisage-profond_page_57.png"
          },
          {
            "text": "Explain the content of this page from the AI course material."
          }
        ]
      },
      {
        "role": "assistant",
        "content": "Quentin CappartINF8175 - Intelligence artiï¬cielleMÃ©thodes et algorithmesRÃ©seaux de neurones et apprentissage profond: FIN\nDALLE: Neural network, digital art"
      }
    ]
  }
]